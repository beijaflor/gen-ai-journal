## Apple MLXを利用したiPhone/iPad用ローカルAIクライアント「Locally AI」がMacに対応。

https://applech2.com/archives/20251110-locally-ai-support-apple-silicon-mac.html

Apple MLXを活用したiPhone/iPad向けローカルAIクライアント「Locally AI」がMacに対応し、LlamaやGemmaなどのAIモデルをローカルで実行可能になった。

**Content Type**: ⚙️ Tools
**Language**: ja

**Scores**: Signal:4/5 | Depth:3/5 | Unique:3/5 | Practical:4/5 | Anti-Hype:4/5
**Main Journal**: 71/100 | **Annex Potential**: 70/100 | **Overall**: 72/100

**Topics**: [[ローカルAI, Apple Silicon, AIモデル, 開発ツール, プライバシー]]

Apple MLXを活用したローカルAIクライアント「Locally AI」が、これまでのiPhone/iPadサポートに加え、新たにMacにも対応しました。マッチングサイトTinderなどを運営する米Match GroupのiOSエンジニアであるAdrien Grondin氏によって開発されたこのアプリは、Apple Siliconに最適化されたApple MLXフレームワークを利用し、Llama、Gemma、Qwen、SmolLM、DeepSeekといった様々なAIモデルをHugging Faceからワンクリックでダウンロードし、デバイス上で完全にローカルで実行できるのが大きな特徴です。

Webアプリケーションエンジニアの視点から見ると、この機能拡張は開発ワークフローに複数の重要な意味をもたらします。第一に、クラウドベースのサービスに依存せずMac上で直接AIモデルを実行できるため、データプライバシーとセキュリティが大幅に向上します。「Locally AI」は「ログイン不要、データ収集なし」を掲げており、機密性の高いデータを扱うアプリケーションの開発や、オフライン環境でのAI機能のテストにおいて、これは決定的な利点となります。

第二に、Apple Siliconに最適化されたMLXの採用は、パフォーマンス面でのメリットを意味します。Mシリーズチップ搭載のMacユーザーは、高速かつ効率的なローカルAIモデルの推論を期待でき、大規模なモデルでもスムーズな試行やテストが可能になります。AI機能のプロトタイピングや、既存のアプリケーションへのAI統合を検討しているエンジニアにとって、開発サイクルを短縮し、反復的な調整を容易にするでしょう。

最新のv1.35/1.38では、Qwen 3 VL 2BやIBM Granite-4.0-H-1B、350Mに加え、iOS/iPadOS 26でサポートされた「Apple Foundation Model」にも対応し、利用できるモデルの幅を広げています。現在、Mac版では日本語入力に一部不具合があるものの、無料でApp Storeから入手可能であり、ローカルAI開発の新たな選択肢として注目されます。このツールは、AI駆動型機能の実装において、クラウドコストの削減、プライバシー保護、そして開発者の手に直接パフォーマンスと柔軟性をもたらす点で、Webアプリケーション開発に携わるエンジニアにとって実践的な価値を提供します。