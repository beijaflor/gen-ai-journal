## RAGを30倍速くするMetaの新技術「REFRAG」

https://zenn.dev/knowledgesense/articles/68089e123a636b

Metaの研究者らが開発した新技術「REFRAG」は、RAGの関連文書をベクトル形式でLLMに注入することで、回答生成速度を最大30倍高速化し、コンテキストサイズを拡張します。

**Content Type**: Research & Analysis

**Scores**: Signal:4/5 | Depth:4/5 | Unique:5/5 | Practical:4/5 | Anti-Hype:4/5
**Main Journal**: 84/100 | **Annex Potential**: 86/100 | **Overall**: 84/100

**Topics**: [[RAG高速化, LLM入力最適化, ベクトル埋め込み, コンテキスト拡張, 生成AI]]

RAGにおけるLLMの応答速度低下は、長い入力プロンプトが原因で発生し、特にエンタープライズ用途では大きな課題です。この課題に対し、Metaの「Superintelligence Labs」が提案する新技術「REFRAG」が画期的な解決策を提示しています。REFRAGは、RAGで検索した関連文書をテキストのままLLMに渡す代わりに、事前に学習した変換器（エンコーダ）でベクトル形式に圧縮し、そのベクトルをLLMに注入します。これにより、LLMがテキストを内部でベクトル化する時間と計算コストを大幅に削減し、最初のトークン生成までの時間を最大30.85倍高速化します。

この手法の特筆すべき点は、回答精度を損なうことなく高速化を実現し、従来の高速化手法と比較しても優位性があることです。また、LLMが一度に扱えるコンテキストサイズを実質的に16倍に拡張可能となり、複雑な情報処理が必要なエンタープライズRAGシステムにとって、その実用的な価値は計り知れません。REFRAGでは、関連文書を16文字のチャンクに分割しベクトル変換するプロセスに加え、強化学習を用いて一部の文書はテキストのままLLMに渡す判断を行うことで、精度と速度の最適なバランスを追求しています。

現状では、専用のエンコーダ学習やLLMのファインチューニングに大量のGPUリソースが必要ですが、将来的にはこの技術がRAGの標準となる可能性が高いと筆者は予測しています。Webアプリケーション開発者にとって、RAGシステムのパフォーマンスとスケーラビリティは極めて重要であり、REFRAGのような入力最適化技術は、ユーザー体験を向上させ、より高度なAIアシスタントを構築する上で不可欠な進歩となるでしょう。今後は、入力文字数と回答速度・精度のトレードオフを解決する多様なアプローチがRAG開発の鍵となります。