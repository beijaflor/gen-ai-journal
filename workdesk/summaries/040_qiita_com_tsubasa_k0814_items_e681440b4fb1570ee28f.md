## 【生成AI】ハルシネーションはなぜ起こるの? 【OpenAIの論文から解説】

https://qiita.com/tsubasa_k0814/items/e681440b4fb1570ee28f

大規模言語モデルがハルシネーションを起こす統計的推論メカニズムとその対策を、OpenAIの論文に基づき解説する。

**Content Type**: Research & Analysis
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:3/5 | Practical:4/5 | Anti-Hype:4/5
**Main Journal**: 76/100 | **Annex Potential**: 74/100 | **Overall**: 76/100

**Topics**: [[ハルシネーション, 大規模言語モデル, 事前学習, 評価システム, 生成AIの信頼性]]

本記事は、大規模言語モデル（LLM）がもっともらしいが事実と異なる情報を生成する「ハルシネーション」のメカニズムについて、OpenAIの論文「Why Language Models Hallucinate」を基に解説しています。Webアプリケーションエンジニアにとって、この現象の根本原因を理解することは、生成AIをシステムに組み込む際の信頼性確保やリスク管理において極めて重要です。

著者はハルシネーションの主な原因として二つの点を挙げています。一つ目は、AIの「事前学習」の目的が「次に来る単語を最もらしく予測すること」に最適化されているため、モデルが「知識を持っているように見せる」ことに注力し、事実の正確性を直接判断しない点です。これにより、モデルは文脈上最も自然な単語列を出力しますが、それが必ずしも事実と一致するわけではありません。例えば、個人的な誕生日情報のように学習データに少ない情報は「知らない」のではなく、それらしい推測を生成する傾向があります。

二つ目の原因は、過去の「評価システム」の問題です。かつてのシステムでは、モデルが「分からない」と回答すると0点となる一方で、適当に答えて偶然正解すれば点数が得られるため、不確かな情報でも推測して回答するインセンティブが働いていたと指摘されています。ただし、最近のモデルではこのエラー率が大幅に改善されているとのことです。

ハルシネーションを完全に排除することは困難ですが、対策として「分からない」と答えた場合に高い評価を与えるような評価システムの改善や、NotebookLMのように情報源を限定してAIが参照する環境（RAGに相当）の利用が有効であると述べられています。Webアプリケーション開発において生成AIを活用する際には、その特性を深く理解し、AIの出力が本当に正しい情報であるかを検証する「使い方」を工夫することが、ハルシネーションによるリスクを低減し、信頼性の高いアプリケーションを構築するための鍵となります。