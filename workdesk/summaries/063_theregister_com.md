## Copilotのハルシネーション問題でウェスト・ミッドランズ警察の本部長が引責辞任

https://www.theregister.com/2026/01/19/copper_chief_cops_it_after/

**Original Title**: West Midlands copper chief cops it after Copilot copped out

AIが生成した架空の情報を根拠に公的な意思決定を行った結果、警察組織のトップが辞任に追い込まれるという実社会での重大なリスクを露呈した。

**Content Type**: 📰 News & Announcements
**Language**: en

**Scores**: Signal:5/5 | Depth:2/5 | Unique:4/5 | Practical:3/5 | Anti-Hype:5/5
**Main Journal**: 83/100 | **Annex Potential**: 81/100 | **Overall**: 76/100

**Topics**: [[Microsoft Copilot, ハルシネーション, AIガバナンス, リスク管理, ファクトチェック]]

英ウェスト・ミッドランズ警察の本部長が、**Microsoft Copilot**が生成した架空の情報（**ハルシネーション**）に基づいて業務上の決定を下した責任を取り、辞任した。問題の発端は、サッカーの試合における警備判断の際、Copilotが「存在しない過去の試合での混乱」を捏造したことにある。警察側はこの虚偽情報を根拠にファンの入場禁止を決定したが、後の調査で情報の誤りが露呈。本部長は当初議会で「AIは使用していない」と証言していたが、後にCopilotによる誤情報であったことを認め、謝罪と辞任に至った。

この事例は、**LLM（大規模言語モデル）**の出力を無批判に業務フローへ組み込むことの致命的なリスクを示している。エンジニアは、単なる検索補助としての利用であっても、**グラウンディング**（根拠付け）や**ファクトチェック**のプロセスが欠如すれば、法的な責任問題に直結することを認識すべきだ。意思決定支援システムを構築する開発者や、組織内でのAI導入を推進する担当者は、AIの限界を周知するとともに、人間の介在（Human-in-the-loop）を必須とするワークフロー設計を徹底する必要がある。