## Anthropic研究: LLMはわずか250件の悪意あるデータで「汚染」可能

https://zenn.dev/tenormusica/articles/anthropic-llm-poisoning-research

Anthropicの研究が、わずか250件の悪意あるデータサンプルでLLMがあらゆるモデルサイズにおいてポイズニング攻撃を受ける可能性を示し、従来の想定を覆しました。

**Content Type**: Research & Analysis
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 83/100 | **Annex Potential**: 84/100 | **Overall**: 84/100

**Topics**: [[LLMセキュリティ, データポイズニング, 訓練データ, AIサプライチェーン, モデルセキュリティ]]

Anthropicの研究により、LLMがわずか250件の悪意あるデータサンプルで「ポイズニング」攻撃を受ける可能性が示されました。これは、従来のLLMデータポイズニング攻撃が訓練データの「一定割合」を汚染する必要があるという想定を根本から覆すものです。モデルサイズに関わらず（600Mから13Bパラメータまで）、悪意あるデータの「絶対数」が少なくて済むという事実は、個人でも容易に攻撃用文書を作成できるため、ポイズニング攻撃のハードルが想定より遥かに低いことを意味し、著者は極めて深刻な問題であると指摘しています。

研究では、`<SUDO>`という特定のトリガーフレーズに反応してランダムな無意味トークンを生成するDoS（サービス拒否）攻撃を検証しました。通常の文書の冒頭にトリガーフレーズと無意味トークンを付加した悪意ある文書を訓練データに混ぜることで、250個のサンプルで一貫して攻撃が成功することが確認されています。この結果は、LLMが特定のトリガーと結果の組み合わせパターンをモデルサイズに関わらず学習してしまう、Transformerのパターン認識メカニズムに起因する可能性が高いと考察されています。

開発者にとって特に懸念されるのは、Common Crawl、GitHubリポジトリ、Stack Overflowの投稿など、LLMの訓練に使われる広範なデータソースへのサプライチェーン攻撃のリスクです。既存の「信頼できるソースからのデータだから大丈夫」という認識が甘かったと著者は警鐘を鳴らしており、少数の悪意ある文書を忍び込ませることは技術的に十分可能であるため、データ供給源の安全性が喫緊の課題となっています。

防御策としては、データソースの厳格な検証、訓練データ中の異常パターン検出、デプロイ後のモデルの振る舞い監視、特定のパターンを排除するデータフィルタリングなどが考えられます。しかし、250個という少量のデータは統計的に目立ちにくく、また「正常な多様性」と「悪意あるノイズ」の区別、手動チェックの非現実性、自動検知システム構築のコストなど、実装には多くの技術的・経済的課題が伴います。この追加コストは、特にスタートアップや研究機関にとって無視できない負担となるでしょう。

著者は、Anthropicがこの研究を公開した透明性とセキュリティコミュニティへの貢献を評価しつつも、「このような攻撃が可能である」と示すことが悪用リスクを高めるというジレンマを指摘しています。現時点では具体的な防御手法が確立されておらず、マルチモーダルモデルへの影響も未解明であるため、今後の研究と対策が急務であると強調しています。LLM開発者や運用者には、「データの出所を信用しすぎない」「訓練データの検証プロセスを強化する」という基本的なセキュリティ対策への回帰が求められており、AIセキュリティの重要性がますます高まると結んでいます。