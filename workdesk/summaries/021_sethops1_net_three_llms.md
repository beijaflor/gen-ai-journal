## asking three LLMs a simple question

https://sethops1.net/post/asking-three-llms-a-simple-question/

著者は、簡単な事実確認において主要LLMが矛盾した不正確な回答を生成し、その情報源としての信頼性に疑問を呈することを明らかにした。

**Content Type**: AI Hype
**Scores**: Signal:4/5 | Depth:2/5 | Unique:3/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 77/100 | **Annex Potential**: 79/100 | **Overall**: 72/100

**Topics**: [[LLMの信頼性, 情報検索, 幻覚, AIの限界, ツール評価]]

この記事は、Cisco C1101-4Pルーターの発売時期というごくシンプルな質問を、主要LLMであるChatGPT、Gemini、GPT-OSS:20bの3つに投げかけた際の具体的な結果を報告しています。LLMがインターネット検索や手動調査の強力な代替となり得ると喧伝される中で、その実態は衝撃的でした。期待に反し、各LLMはそれぞれ「2019年後半」「2018年初頭」「1999年秋」と、互いに矛盾し、かつ実際の情報とは異なる回答を提示したのです。これは、LLMが単純な事実確認においても信頼性の低い情報を生成する可能性を示唆する、明確な反証事例と言えるでしょう。

Webアプリケーションエンジニアにとって、この結果が持つ意味は非常に大きい。日々の開発業務でAIを活用する際、LLMが提供するコードスニペットや技術情報、あるいは一般的な知識について、その出力結果を盲信する危険性を改めて浮き彫りにしています。特に、システムの設計方針、ライブラリの選定、セキュリティに関する情報など、プロジェクトの根幹に関わる重要な判断において、LLMが生成する「もっともらしい嘘（ハルシネーション）」を見過ごせば、深刻な問題に発展しかねません。この事例は、AIツールの能力を過大評価せず、その限界を正確に理解した上で賢く利用することの重要性を強調しています。LLMは確かに強力なアシスタントですが、提供される情報には常に批判的な視点を持ち、最終的なファクトチェックと検証は人間のエンジニアが責任を持って行うべきであるという、揺るぎない教訓を私たちに与えています。