## 「なぜAIは嘘をつくのか？」OpenAIが論文を公開

https://pc.watch.impress.co.jp/docs/news/2045655.html

OpenAIがハルシネーションの根本原因を特定し、事前学習とベンチマーク設計に問題があると発表、モデルが「分からない」と答える重要性を強調しました。

**Content Type**: 📰 News & Announcements

**Scores**: Signal:4/5 | Depth:3/5 | Unique:3/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 80/100 | **Annex Potential**: 78/100 | **Overall**: 76/100

**Topics**: [[LLMハルシネーション, 事前学習メカニズム, ベンチマーク評価, モデルの不確実性, AI信頼性]]

OpenAIは、言語モデルが生成する「ハルシネーション」（もっともらしいが誤った回答）の根本原因に関する最新の研究結果を発表しました。ウェブアプリケーション開発者として、この研究はAIの出力をどう信頼し、サービスに組み込むべきかを考える上で非常に重要です。

同社はハルシネーションの主な原因として二つの点を指摘しています。一つは、言語モデルの「事前学習」メカニズムです。モデルは膨大なテキストデータから次の単語を予測することで学習しますが、このプロセスには情報の正誤を判断するラベルがなく、単語の連なりにおける「文脈的な妥当性」を学習します。そのため、論文タイトルや人の誕生日といった出現頻度の低い任意の情報は予測が難しく、文脈上は自然でも事実とは異なる情報を生成してしまうのです。モデルは真実を知っているわけではなく、それらしく「語っている」に過ぎないという本質的な課題がここにあります。

もう一つは、AIモデルの性能を評価する「ベンチマークテスト」の設計です。現在の多くのテストでは、モデルが「分からない」と正直に答えることは評価されず、推測で回答して偶然正解すると高いスコアが得られる傾向にあります。この結果、モデルは不確かな情報でも推測で答えることを優先し、ハルシネーションを引き起こすインセンティブが生まれてしまいます。これは、開発者が信頼できるAIシステムを構築する上で、評価のインセンティブ設計がモデルの振る舞いに直接影響を与えることを示しています。

OpenAIは、ハルシネーションを抑制するために、モデルが「分からない」と正直に答えることを可能にし、推測による回答を抑制するよう既存のベンチマークを再設計する必要があると提言しています。これは、AIを組み込んだプロダクトの信頼性を高める上で非常に実践的な知見です。我々開発者は、モデルの「正直さ」を評価基準に含めることで、より予測可能で安全なAI体験をユーザーに提供できるようになるでしょう。