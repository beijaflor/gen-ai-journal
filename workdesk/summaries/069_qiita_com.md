## ギャルと学ぶContent filtering system #Azure

https://qiita.com/haruty/items/1250324eba7861d8f186

Azure OpenAI ServiceのContent Filterが「原宿」などの固有名詞を個人情報として誤検知する事象を題材に、フィルターの仕組みと具体的な回避策を解説する。

**Content Type**: 📖 Tutorial & Guide
**Language**: ja

**Scores**: Signal:4/5 | Depth:3/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 83/100 | **Annex Potential**: 81/100 | **Overall**: 80/100

**Topics**: [[Azure OpenAI Service, Content Filter, Responsible AI, PII, Content Safety Studio]]

Azure OpenAI Serviceを利用したプロダクト開発において、意図せずContent Filterが発動し、サービスが停止するリスクとその実務的な対策を解説している。著者は「原宿でAIとオケできるサービス」の構築中、「原宿」という単語が住所情報（PII）として検知され、APIが400エラー（ResponsibleAIPolicyViolation）を返してAIが無言になるというトラブルに遭遇した。この実体験に基づき、本記事ではContent Filterの仕組みとデバッグ手法が体系的にまとめられている。

まず、Content Filterは「入力（ユーザー指示）」と「出力（モデル応答）」の二段階で動作し、ヘイト、性的表現、暴力、自傷、個人情報の流出、さらには著作権保護されたテキストやコードの出力を監視・ブロックする。著者は、フィルターが「なぜブロックされたか」の具体的な単語や文脈を返さない仕様（攻撃者による回避パターンの学習を防ぐための防御設計）が、開発時のデバッグを極めて困難にしていると指摘する。

これに対する技術的なアプローチとして、著者は以下の3点を提示している。第一に、検証ツール「Content Safety Studio」の活用だ。APIレスポンスだけでは不明な「どのカテゴリに、どの程度の強さ（Severity：Safe/Low/Medium/High）で反応したか」を可視化することで、勘に頼らない分析を推奨している。第二に、プロンプトチューニングによる表現の緩和だ。固有名詞を「都内の有名な若者向けエリア」と言い換えたり、フィクションであることを明示する「ガード文」を挿入したりすることで、文脈上の安全性を高める手法を具体的に紹介している。

第三に、最終手段としてのフィルターレベルの調整（または特定項目のOFF）である。著者のケースでは、PIIの住所チェックをオフにすることが現実的な解となったが、その代償として真の住所情報が露出するリスクも伴う。筆者は、サービスの性質、説明責任、代替となるアプリ側でのチェック機構をセットで検討すべきであると、責任あるAI運用の重要性を強調している。エンジニアがLLMの「安全装置」による予期せぬ挙動をどう制御し、利便性と安全性のトレードオフをどう管理すべきか、示唆に富むガイドとなっている。