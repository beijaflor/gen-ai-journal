## LLMの中身を覗いてみたら、Transformerは「回路」を形成していた

https://zenn.dev/50s_zerotohero/articles/a6189c891fbd71

LLM内部で特定のタスクを処理するために形成される「回路」の実態を、メカニズム解釈可能性の視点から明らかにする。

**Content Type**: 🔬 Research & Analysis
**Language**: ja

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:3/5 | Anti-Hype:5/5
**Main Journal**: 84/100 | **Annex Potential**: 85/100 | **Overall**: 84/100

**Topics**: [[Mechanistic Interpretability, TransformerLens, Attention Head, Logit Lens, Residual Stream]]

LLMを「次に来るトークンを確率的に予測するだけの統計的オウム」と捉える見方に対し、著者は「メカニズム解釈可能性（Mechanistic Interpretability）」の視点から、モデル内部に論理的な処理構造が存在することを論じている。具体的には、IOI（間接目的語特定）というタスクを用い、GPT-2内部で情報がどのように処理・蓄積されているかを可視化・分析している。

解析の要となるのは、各トークンの意味を運ぶ「残差ストリーム（residual stream）」と、出力直前のスコアである「ロジット（logits）」の観察だ。著者は、特定の名前（MaryかJohnか）を予測する際のロジット差をレイヤーごとに追跡することで、モデルが最初から答えを知っているのではなく、レイヤーを重ねるごとに判断を固めていく過程を明らかにしている。特に、特定の中間レイヤーにおいて判断が急激に形成される様子は、LLMが単なる統計処理ではなく、段階的な推論回路を持っていることを示唆している。

さらに、著者は「アクティベーション・パッチング」という手法を用いた、特定のアテンションヘッド（Attention Head）の役割特定についても解説している。これは、特定のヘッドの出力を正常な状態と異常な状態で入れ替えることで、そのヘッドが推論に与える因果的な影響を調べる「脳外科手術」のような手法だ。これにより、「既出トークンの追跡」「構文の把握」「候補の除外と推奨」といった特定の機能を担う独立したヘッドが連携し、一つの「回路」として機能している実態が浮き彫りになった。

ウェブアプリケーションエンジニアにとって、この知見は「LLMのブラックボックス化」という懸念に対する強力な処方箋となる。プロンプトエンジニアリングなどの表面的なハックだけでなく、内部の回路構造を理解することで、なぜモデルが特定の出力を生成したのか、あるいはなぜ失敗したのかを論理的に追跡できる可能性を示しているからだ。著者は、TransformerLensなどの解析ツールを活用し、LLMを「中から見る」視点を持つことが、より信頼性の高いAIシステム構築への鍵になると主張している。