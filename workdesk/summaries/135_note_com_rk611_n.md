## 社長が、自腹のRTX2080で国産生成AIをゼロから作ってみた話

https://note.com/rk611/n/n4dfffbbed408

paiza社長が自前のゲーミングPCと著作権切れデータで最小限の国産LLM事前学習を試み、その実現の途方もない困難さと既存AIの背景にある膨大な労力を身をもって示した。

**Content Type**: 📖 Tutorial & Guide

**Scores**: Signal:4/5 | Depth:3/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 77/100 | **Annex Potential**: 78/100 | **Overall**: 80/100

**Topics**: [[LLM事前学習, 自作LLM, 生成AI開発コスト, 環境構築, 青空文庫データセット]]

多くのウェブアプリケーションエンジニアがChatGPTのような生成AIを日常的に活用する中、その基盤となるLLMをゼロから構築する困難さはほとんど知られていません。paizaの片山社長は、自前のRTX 2080 Ti搭載PCと青空文庫データセットを使い、GPT-2 small相当（1.2億パラメータ）の日本語LLM事前学習を試みました。この挑戦は、LLM開発コストと労力の現実を浮き彫りにします。

わずか1.5時間の学習時間にもかかわらず、WSL2、Docker、PyTorch CUDA版といった環境構築にはChatGPTの支援が不可欠でした。さらに重要なのは、事前学習のみのモデルが「地獄育ちの絶壁」のような意味不明な文章しか生成できなかった事実です。これは、私たちが使う高性能LLMが、事前学習後にRLHF（強化学習による人間フィードバック）やファインチューニングといった、はるかに膨大な時間とコストを要する追加学習を経て初めて実用的な能力を獲得していることを明確に示しています。

この経験は、エンジニアが基盤モデル構築の途方もないコスト（GPT-4級で数百億円、スカイツリー建設に匹敵）と、データ収集・クリーニング、トークナイザー設計、モデル学習、チューニングといった各工程の専門性と労力を「手触り感」をもって理解する貴重な機会です。既存LLMを利用する際も、その裏側にある技術的深淵と開発者の途方もない努力を認識することで、より賢明な利用方法や適切な期待値設定が可能になります。自社でLLMを開発・運用する可能性を探る企業にとって、この「やってみた」報告は、リソース計画と実現可能性評価において極めて重要な洞察を提供します。