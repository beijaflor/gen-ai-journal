## NVIDIAのLLM、Nemotron 3 Nanoは賢いけどコーディングには向かないかも。Mamba 2の特性が悪く出てる？

https://nowokay.hatenablog.com/entry/2025/12/16/042030

NVIDIAの軽量LLM「Nemotron 3 Nano」の検証を通じ、Mamba 2アーキテクチャがコーディング時のコンテキスト保持に与える負の影響と、その実用限界を指摘する。

**Content Type**: Research & Analysis
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:4/5 | Practical:3/5 | Anti-Hype:4/5
**Main Journal**: 77/100 | **Annex Potential**: 78/100 | **Overall**: 76/100

**Topics**: [[Nemotron 3 Nano, Mamba 2, LLM Architecture, Coding Assistant, NVIDIA]]

NVIDIAが新たに公開した軽量LLM「Nemotron 3 Nano」（30B-A3B MoE）について、著名な開発者である筆者が実機での検証結果を報告している。本モデルは計算効率を極限まで高めるため、従来のTransformerアーキテクチャの大部分をState Space Model（状態空間モデル）の一種である「Mamba 2」に置き換えており、NVFP4形式での学習により3Bというアクティブパラメータ数ながら高い知能を実現している点が特徴である。

筆者は、一般的な日本語の生成能力、日本に関する知識、論理的思考（四則演算の文章題）および要約性能をテストしており、それらについては既存の同クラスモデル（Ministral 3やGLM 4.6Vなど）と比較しても非常に優秀であると評価している。特に、思考プロセス（Thinking）を挟むことで複雑な論理パズルにも正解を出せる点は、このサイズ感のモデルとしては特筆すべき性能だとしている。

しかし、エンジニアにとって最も重要な「コーディング」の検証においては、明確な弱点が露呈した。ブロック崩しのコード生成を試みたところ、一度生成したはずのクラス群を1つのファイルにまとめる過程で、変数名が書き換わったり、`public`修飾子が脱落したりといった、ソースコードの整合性を欠く「うろ覚え」のような挙動が多発した。また、実行時例外が発生した際のエラー報告に対しても、コードの現状を把握できていない的外れな修正案を提示するなど、対話が深まるにつれて文脈の把握能力が低下する傾向が見られた。

この現象について、筆者はMamba 2のアーキテクチャ特性に注目している。Mamba 2は計算量が入力長に対して線形（O(n)）であるため、Transformer（O(n²)）よりも長いコンテキストを扱えるメリットがある。一方で、そのメカニズムは「過去の情報を圧縮して保持する」ものであるため、時間が経過するにつれて詳細な情報の再現性が失われやすい。筆者によれば、要約や翻訳、あるいは多少の忘却が許される一般的な対話ではこの特性は問題にならないが、一文字のミスも許されないコーディングにおいては致命的な欠陥になり得ると主張している。

結論として、Nemotron 3 Nanoは「非常に賢い軽量モデル」ではあるものの、厳密な整合性が要求されるコーディングエージェントのエンジンとしては現時点では推奨できない。エンジニアは、推論効率の高い新しいアーキテクチャを採用したモデルを選定する際、単なるベンチマークスコアだけでなく、このような「コンテキスト保持の性質」が自身の用途に適合しているかを慎重に見極める必要があるという、重要な教訓を提示している。