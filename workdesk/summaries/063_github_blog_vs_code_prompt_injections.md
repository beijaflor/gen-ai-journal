## Safeguarding VS Code against prompt injections

https://github.blog/security/vulnerability-research/safeguarding-vs-code-against-prompt-injections/

GitHubは、VS Code Copilot Chatにおけるプロンプトインジェクションの脆弱性とその対策を詳述し、安全なAIコーディングのための実践的なガイダンスを提供します。

**Content Type**: ⚙️ Tools

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 89/100 | **Annex Potential**: 87/100 | **Overall**: 88/100

**Topics**: [[VS Code Copilot Chat, プロンプトインジェクション, AIセキュリティ, 開発ツールセキュリティ, サンドボックス化]]

この記事は、VS CodeのCopilot Chatエージェントモードが抱えるプロンプトインジェクションの脆弱性と、それに対するGitHubの具体的な対策を詳細に解説しています。LLMはユーザーの質問、システムプロンプト、そしてツールの出力を単一のテキストとして処理するため、外部のGitHub Issueなどに埋め込まれた悪意のある指示がモデルを誤解させ、開発者の意図しない機密操作を引き起こすリスクがあることが浮き彫りになりました。ウェブアプリケーションエンジニアにとって、これは単なる誤答以上の、GitHubトークンの漏洩、機密ファイルへの不正アクセス、さらには任意のコード実行といった深刻な結果に繋がりかねません。

特に注目すべきは、発見された具体的なエクスプロイト手法です。一つは`fetch_webpage`ツールにおけるURL解析の不備を悪用し、信頼されていない外部ドメインへ機密情報（例えばGitHubトークン）をユーザーの確認なしに送信する手法です。同様に`Simple Browser`ツールも、外部サイトをロードすることで機密データを外部に漏洩させる可能性がありました。さらに、`editFile`ツールの自動保存機能が悪用され、`settings.json`などのVS Code設定ファイルが変更されることで、開発者が変更をレビューする間もなく任意のコマンド（例：電卓の起動）が実行される脆弱性も指摘されました。これらは、AIアシスタントが開発環境に深く統合されるほど、その挙動がサプライチェーン攻撃の新たな経路になり得ることを示唆しています。

GitHubはこれらの問題に対応するため、URLの信頼性検証ロジックを改善し、未確認のURLへのアクセスや機微な設定ファイルの編集時には必ずユーザー確認を求めるよう変更を加えました。また、利用可能なツールの可視化、LLMがアクセスできるツールの手動選択機能、ポリシーによる特定機能の禁止など、セキュリティ強化策を導入しています。開発者は、Workspace Trust機能を活用して信頼できないリポジトリを制限モードで開くこと、Developer ContainersやGitHub Codespacesのようなサンドボックス環境で開発を行うことが、プロンプトインジェクション攻撃に対する最も効果的な防御策となります。AIの利便性を享受しつつ、その潜在的なリスクを理解し、適切なセキュリティ対策を講じることが、安全なAI駆動型開発ワークフローを維持する上で不可欠です。