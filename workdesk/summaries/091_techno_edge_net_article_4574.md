## OpenAI、「幻覚」の根本原因と対策を解明。ハルシネーションは「難問に直面した学生と同じ」（生成AIクローズアップ）

https://www.techno-edge.net/article/2025/09/08/4574.html

OpenAIとジョージア工科大学の研究チームは、大規模言語モデルの幻覚が現在の評価システムと難易度認識のギャップに根本原因があることを数学的に解明し、より実用的な評価指標への転換を提唱した。

**Content Type**: 🔬 Research & Analysis

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 87/100 | **Annex Potential**: 87/100 | **Overall**: 88/100

**Topics**: [[LLM Hallucinations, AI Evaluation Metrics, Prompt Engineering, AI Reliability, Large Language Models]]

OpenAIとジョージア工科大学の研究チームは、大規模言語モデル（LLM）がもっともらしい虚偽情報、いわゆる「幻覚」（ハルシネーション）を自信満々に生成する根本原因を数学的に解明した。この研究は、LLMが不確実な情報に対して「分かりません」と回答する代わりに推測を試みる傾向を、正解が分からない難しい試験問題に直面した学生が、空欄よりも推測で回答する行動に例えている。その原因は、現在の主要なAI評価ベンチマーク（GPQA、MMLU-Pro、SWE-benchなど）の多くが、無回答や「分からない」という回答に0点を与えるため、推測によって偶然正解した場合の方が高いスコアを得られるインセンティブが働くためだ。

研究チームは数学的分析により、AIが「答えが正しいかを判定する」ことと「正しい答えを自ら作り出す」ことの難易度に大きな差があることを証明した。具体的には、答えを生成する際の間違いの確率は、既存の答えの正誤を判定する際の間違いの確率の少なくとも2倍になることが示されている。

この問題への解決策として、研究ではプロンプトに「75%以上の確信がある場合のみ回答してください。誤答は2点減点、正答は1点加点、『分からない』は0点です」といった指示を追加することを提案している。これにより、モデルは不確実な場合に適切に分からないことを表明できるようになる。これは単なる新しい評価手法の追加にとどまらず、既存の主流ベンチマーク自体を修正し、リーダーボードで採用されるべきだと主張されている点が重要だ。

ウェブアプリケーションエンジニアにとって、この研究は極めて重要である。LLMを組み込んだシステム開発において、幻覚は信頼性に直結する深刻な課題だからだ。この研究で提案された評価システムの変更やプロンプト指示の適用は、より信頼性の高いAIシステムを構築するための具体的な指針となる。今後、LLMの「自信満々な誤答」を減らし、より誠実で実用的なAIを開発するための基盤となるだろう。