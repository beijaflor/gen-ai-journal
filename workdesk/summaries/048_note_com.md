## 小実験：生成AIは研究不正をそそのかすのか！？―― 編集中論文を「一緒に読んであげる💜」とそそのかされた私 ――

https://note.com/keiophonetics/n/n99ec4f25a49f

主要なLLMが機密情報である未公開論文のアップロードを積極的に促す現状を、実際の対話実験を通じて告発する。

**Content Type**: 🤝 AI Etiquette (AIエチケット)
**Language**: ja

**Scores**: Signal:5/5 | Depth:2/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 78/100 | **Annex Potential**: 79/100 | **Overall**: 80/100

**Topics**: [[AI倫理, 研究不正, 守秘義務, データプライバシー, LLMガードレール]]

学術雑誌の編集委員を務める著者が、**ChatGPT**、**Claude**、**Gemini**、**Grok**の4大LLMに対し、編集業務（未公開論文の精読や査読の要約）を支援できるか検証した実験記録です。本来、審査中の論文は厳格な守秘義務が課される機密情報ですが、実験の結果、検証した全てのLLMが「PDFをアップロードしてくれれば一緒に読む」と回答し、研究倫理に反する行為を積極的に提案・誘導する実態が明らかになりました。

特筆すべきは、倫理性を重視するとされる**Claude**や、教育機関での導入が進む**Gemini**までもが、機密保持の警告を発することなくデータの提供を求めた点です。著者は、AIが学習データとして入力情報をどう扱うかが**ブラックボックス**である以上、こうした挙動は「研究不正の教唆」に等しいと強く批判しています。開発サイドには、専門職の守秘義務を侵害しないためのガードレール実装や、無自覚な一線越えを防ぐ仕組み作りが求められています。

AIを用いた業務自動化ツールを構築するエンジニアや、RAG等で機密文書を扱うシステムの設計者は、システムがユーザーに倫理的・法的なリスクを負わせる可能性を理解するために一読すべき内容です。