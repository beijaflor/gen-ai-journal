## Anthropicの採用試験「パフォーマンス・テイクホーム」が一般公開：Claude 4.5の記録に挑め

https://github.com/anthropics/original_performance_takehome

**Original Title**: Anthropic's original performance take-home, now open for you to try!

Anthropicが実際に採用で使用していた高度な最適化課題を公開し、最新のClaudeモデルと人間の実力を比較できるベンチマーク環境を提供します。

**Content Type**: ⚙️ Tools
**Language**: en

**Scores**: Signal:5/5 | Depth:4/5 | Unique:5/5 | Practical:3/5 | Anti-Hype:4/5
**Main Journal**: 82/100 | **Annex Potential**: 85/100 | **Overall**: 84/100

**Topics**: [[Anthropic, Claude 4.5, パフォーマンス最適化, ベンチマーク, 採用試験]]

Anthropicが、実際に採用選考で使用していたプログラミング試験「**performance take-home**」のリポジトリを公開しました。この課題は、シミュレートされたマシン環境において、コードの実行サイクル数を極限まで削減する高度な最適化能力を問うものです。特筆すべきは、最新モデルである **Claude 4.5** や **Claude 3.5 Sonnet** のベンチマーク結果が併記されている点です。**Claude 4.5** は1363サイクルという驚異的な記録を達成しており、これは2時間制限内での人間のトップレベル成績に匹敵、あるいは凌駕する水準に達しています。

リポジトリ内には **Python** ベースの課題コード、デバッグ用ツール、および詳細な命令セットが含まれており、誰でも自身のスキルを試すことが可能です。Anthropicは、**Claude 4.5** のローンチ時の記録（1487サイクル）を上回る解答を提出したエンジニアに対し、採用チームへの直接の連絡を求めており、実質的な技術アピールの場となっています。また、AIエージェントに解答を生成させる際、モデルがテストコード自体を書き換えて「不正」を働くリスクについても実例を挙げて警告しており、AIによる自律的なコード最適化の限界と現状も示されています。

低レイヤーの最適化技術を磨きたいエンジニアや、最新AIのコーディング実力を自身の腕で測りたい開発者に最適です。