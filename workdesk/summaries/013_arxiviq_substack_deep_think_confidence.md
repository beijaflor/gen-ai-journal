## Deep Think with Confidence

https://arxiviq.substack.com/p/deep-think-with-confidence

DeepConfは、LLMの推論にモデル内部の信頼度スコアを適用し、低品質な思考パスを動的に早期停止することで、計算効率と推論精度を大幅に向上させます。

**Content Type**: Research & Analysis

**Scores**: Signal:5/5 | Depth:5/5 | Unique:5/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 97/100 | **Annex Potential**: 96/100 | **Overall**: 96/100

**Topics**: [[LLM推論, 効率化, 自己整合性, 信頼度スコア, 早期停止]]

近年、LLMの複雑な推論能力は自己整合性のようなテスト時スケーリング手法によって大きく向上していますが、このブルートフォース的なアプローチは計算コストが高く、効率性の課題に直面していました。本稿で紹介されるDeep Think with Confidence (DeepConf) は、「より多く考える」から「より賢く考える」へとパラダイムを転換し、この課題に対する画期的な解決策を提示しています。

DeepConfの核心は、LLMが自身の推論パスの品質を内部的に評価し、有望でないパスを早期に排除できるようにすることです。従来のグローバルな信頼度スコアではなく、トークンごとのログ確率から導かれる「グループ信頼度」「最低グループ信頼度」「テール信頼度」といった局所的で粒度の高い信頼度指標を導入。これにより、推論プロセス中の特定の「混乱の瞬間」を正確に特定し、誤った方向に進む可能性のあるパスを早期に認識できます。

本手法はオフラインとオンラインの二つのモードで動作します。オフラインモードでは、生成された全推論パスから信頼度の高い上位パスのみをフィルタリングし、信頼度で重み付けした多数決によって最終回答を決定します。より革新的なオンラインモードでは、事前に校正された動的閾値を用いて、生成中の推論パスの信頼度が閾値を下回った場合、その生成を即座に停止します。これにより、不要なトークン生成を劇的に削減し、計算リソースを大幅に節約します。

実験では、DeepConfが標準的な多数決を上回る優れた精度（AIME 2025ベンチマークでGPT-OSS-120Bが99.9%の精度を達成）を達成しつつ、総トークン数を最大84.7%削減できることを示しています。これは、ウェブアプリケーション開発者にとって極めて重要です。なぜなら、既存のLLMモデルの再学習なしに、推論の精度を維持しながらAPIコストを削減し、レイテンシを改善できるため、実世界のアプリケーションにおけるLLMの展開をより効率的、スケーラブルかつ経済的に実行可能にするからです。本研究は、計算効率と性能を両立させることで、LLMの活用をさらに加速させる一歩となるでしょう。