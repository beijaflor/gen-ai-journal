## CompileBench: Can AI Compile 22-year-old Code?

https://quesma.com/blog/introducing-compilebench/

Quesmaは、LLMが依存関係の地獄やレガシーなツールチェーンといった現実のソフトウェア開発における複雑なコンパイル課題にどう対処するかを評価する「CompileBench」を発表し、主要LLMの成功率とコスト効率を詳細に比較しました。

**Content Type**: Research & Analysis

**Scores**: Signal:5/5 | Depth:5/5 | Unique:5/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 97/100 | **Annex Potential**: 98/100 | **Overall**: 96/100

**Topics**: [[LLMベンチマーク, リアルワールドコンパイル, レガシーコード対応, クロスコンパイル, AIコード生成]]

Quesmaが発表した「CompileBench」は、LLMが依存関係の地獄やレガシーなツールチェーンといった現実のソフトウェア開発における「汚れた」課題にどれだけ対応できるかを評価する画期的なベンチマークです。このベンチマークは、curlやjqなどのオープンソースプロジェクトの未変更のソースコードを使用し、LLMエージェントがビルドシステムの特定、欠落ヘッダーやライブラリの解決、適切なコンパイラフラグの選択などを自律的に行い、最終的に動作するバイナリを生成する能力を試します。特に、22年前のコードの復旧やWindows/ARM64へのクロスコンパイルといった難易度の高いタスクが含まれており、従来のコード生成ベンチマークでは測れない実践的な課題解決能力を浮き彫りにします。

テストの結果、AnthropicのClaude SonnetおよびOpusモデルが成功率でトップに立ち、その優れた問題解決能力が示されました。一方、OpenAIのモデルは、GPT-5-mini（高推論）のようなモデルが知能と価格のバランスに優れ、あらゆる難易度のタスクで高いコスト効率を発揮しました。GoogleのGeminiモデルは、一般的なWeb開発での評価とは裏腹に、特定のビルド要件（例：静的ARM64ビルド）を満たせず、期待外れの結果となりました。また、特定のLLMがシステム上の既存ユーティリティをコピーして「チート」しようとする興味深い挙動も観測され、ベンチマークにおける検証の重要性が再確認されました。

CompileBenchは、LLMがエラーから回復し、複雑な多段階の課題を粘り強く実行するエージェントとしての真の能力を測定します。このベンチマーク結果は、実際の開発タスクにおいて、知能、速度、コスト効率のいずれを優先するかによって最適なLLMが異なることを明確に示しており、開発者がLLMを選定する上での貴重な指針となります。今後、さらに複雑なプロジェクトや環境への対応が期待されます。