## AI fabricates 21 out of 23 citations lawyer sanctioned reported to state bar [pdf]

https://news.ycombinator.com/item?id=45236927

弁護士が生成AIツールに作成させた23件の法的引用のうち21件が捏造であったため制裁を受け、AIの信頼性における重大な課題が浮き彫りになりました。

**Content Type**: 📰 News & Announcements

**Scores**: Signal:4/5 | Depth:2/5 | Unique:3/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 78/100 | **Annex Potential**: 76/100 | **Overall**: 72/100

**Topics**: [[AIの幻覚, LLMの信頼性, リーガルテック, AI倫理, 開発者の責任]]

この記事は、生成AIの「幻覚（ハルシネーション）」が現実世界で深刻な結果を招いた具体的な事例を報じています。弁護士がChatGPT、Claude、Gemini、GrokといったAIツールに作成させた23件の法的引用のうち、21件が捏造されたものであったため、州弁護士会から制裁を受ける事態となりました。弁護士はAIの幻覚問題について知らなかったと説明していますが、その主張は受け入れられませんでした。

ウェブアプリケーションエンジニアにとって、この事例は極めて重要です。なぜなら、AIの信頼性が単なる理論的な問題ではなく、サービスの信用失墜や法的責任、さらにはユーザーへの直接的な損害につながることを明確に示しているからです。AIを活用した機能を開発する際、特に正確性が求められる分野（例: コード生成、データ分析、コンテンツ作成）では、AIの出力に対する厳格な検証プロセスと人間によるレビューが不可欠です。コメント欄では、検索機能付きモデルでも依然として幻覚が発生するとの指摘もあり、RAG（検索拡張生成）などの技術を導入したとしても、完全な解決策ではないことを示唆しています。開発者は、AIの限界を理解し、その不確実性を考慮したシステム設計と、リスクを軽減するための具体的な対策を講じる責任があります。AIをプロダクトに組み込む際には、その「なぜ」を深く掘り下げ、信頼性の確保に最大限の注意を払うべきでしょう。