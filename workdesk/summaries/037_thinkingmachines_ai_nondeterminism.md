## Defeating Nondeterminism in LLM Inference

https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/

Thinking Machines Labは、LLM推論の非決定性の根源がバッチ不変性の欠如にあることを突き止め、RMSNorm、行列乗算、Attentionにおけるバッチ不変カーネルの実装戦略を通じて、真に再現可能な推論結果の達成を可能にする方法を詳述しています。

**Content Type**: Research & Analysis

**Scores**: Signal:5/5 | Depth:5/5 | Unique:5/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 97/100 | **Annex Potential**: 98/100 | **Overall**: 96/100

**Topics**: [[LLM Inference, Reproducibility, Batch Invariance, GPU Kernels, On-Policy RL]]

LLM推論において、たとえ温度設定を0にしても結果が非決定的になる問題は、多くの開発者がLLMアプリケーションの信頼性とデバッグに直面する課題でした。本稿は、この非決定性の一般的な原因として挙げられる「並列処理と浮動小数点演算の非結合性」が主要な犯人ではないことを論破し、その真の根源が推論カーネルの「バッチ不変性」の欠如にあることを明らかにしています。

この問題は、推論サーバーにかかる負荷（バッチサイズ）が各リクエストの計算結果に微細な影響を与え、ユーザーの視点から見るとLLMの出力が非決定的になることに起因します。これは、GPUだけでなくCPUやTPUベースの推論エンドポイントでも発生します。

ウェブアプリケーションエンジニアにとって重要なのは、この非決定性がLLMを組み込んだプロダクトの予測可能性を損ない、デバッグを極めて困難にすることです。特に、LLMを強化学習（RL）のポリシーとして利用する場合、推論と学習の数値的な不一致は「オンポリシーRL」を事実上「オフポリシーRL」に変質させ、学習の安定性と効率を著しく低下させます。

本稿は、このバッチ不変性を達成するための具体的なカーネル実装戦略を提供します。RMSNorm、行列乗算、AttentionといったLLMの主要な計算ブロックにおいて、データ並列化や固定サイズ分割（Fixed Split-KV Strategy）などの手法を用いることで、バッチサイズに依存しない一貫した計算順序を保証できることを示しています。これにより、わずかな性能低下と引き換えに、ビット単位で再現可能な推論結果を得られます。

vLLMのFlexAttentionバックエンドを利用した実験では、非決定的な環境で80種類ものユニークな補完結果が得られたのに対し、バッチ不変カーネルを用いることで1000回の補完が全て同一になることが実証されました。この技術は、LLMを活用するシステムの堅牢性を高め、デバッグを容易にし、特に強化学習におけるモデルの学習効率と安定性を飛躍的に向上させるための重要な基礎となります。