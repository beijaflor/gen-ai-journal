## The maths you need to start understanding LLMs

https://www.gilesthomas.com/2025/09/maths-for-llms

大規模言語モデル（LLM）の推論プロセスを理解するために、ベクトルや行列、ドット積などの高校数学レベルの基礎概念が、高次元空間での意味表現や変換にいかに応用されるかを解説する。

**Content Type**: Tutorial & Guide

**Scores**: Signal:4/5 | Depth:4/5 | Unique:3/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 78/100 | **Annex Potential**: 76/100 | **Overall**: 80/100

**Topics**: [[LLMの基礎数学, ベクトル埋め込み, 行列演算, ソフトマックス関数, 高次元空間]]

この記事は、ウェブアプリケーションエンジニアが大規模言語モデル（LLM）の内部、特に「推論」フェーズの仕組みを深く理解するための数学的基礎を解説します。LLMが複雑なブラックボックスのように感じられることが多い中、筆者は高校数学レベルの知識（ベクトル、行列、ドット積など）で、その中核的な動作原理を十分に捉えられると強調します。

ウェブアプリケーションエンジニアにとって重要なポイントは以下の通りです。まず、LLMはトークンの予測確率や概念そのものを、高次元空間におけるベクトル（「vocab space」や「埋め込み」）として表現します。これにより、類似する概念が空間内で近くに配置され、意味的な関連性を数値的に扱えます。次に、ソフトマックス関数は、LLMの「生の」出力（ロジット）を、次のトークンの選択に利用できる正規化された確率分布へと変換します。また、ドット積は二つのベクトルの方向の近さを測ることで、埋め込み空間における概念間の類似性を効率的に評価する手法として機能します。最後に、行列の乗算は、高次元空間間でデータを変換（射影）する役割を果たし、ニューラルネットワークの線形層は本質的にこのプロジェクションを通じて入力データを異なる次元数の空間にマッピングしていると説明します。

これらの基礎知識は、LLMがなぜ特定の出力を生成するのか、なぜ埋め込みがセマンティック検索に役立つのかといった疑問に対し、数学的な裏付けをもって直感的に理解する上で不可欠です。LLMを単なるツールとして使うだけでなく、その内部構造と限界を把握することで、より洗練されたプロンプトエンジニアリング、モデル選択の最適化、さらには将来的なカスタムモデル開発へと繋がる洞察を与え、高度なAI駆動型アプリケーション設計の基盤となるでしょう。