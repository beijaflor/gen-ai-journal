## 「手作り RAG システム」で RAG の仕組みを学び直す

https://zenn.dev/google_cloud_jp/articles/e699bda0a298d6

マネージドサービスに頼らずRAGシステムをゼロから構築することで、現代的なロングコンテキストLLMを活かした高精度な検索・生成ロジックの本質を解き明かす。

**Content Type**: 📖 Tutorial & Guide
**Language**: ja

**Scores**: Signal:5/5 | Depth:4/5 | Unique:3/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 88/100 | **Annex Potential**: 82/100 | **Overall**: 84/100

**Topics**: [[RAG, Vertex AI, Gemini, Vector Search, LLM Architecture]]

Google Cloudのエンジニアである著者が、Vertex AI Searchなどのマネージドサービスをあえて使わず、PythonとGemini APIを用いてRAG（検索拡張生成）システムを自作するプロセスを詳解している。著者は、便利なツールが裏側で行っている処理を分解して実装することで、RAGの設計思想と高品質な出力を得るための工夫を深く理解できると説く。

従来のRAGはLLMのコンテキスト制限により、ドキュメントを数百文字程度の小さな「チャンク」に分割せざるを得ず、文脈の欠落が大きな課題だった。しかし、Gemini 2.5 Flashのような100万トークンを超えるロングコンテキストモデルの登場により、ドキュメントを分割せずそのままプロンプトへ入力する手法が可能になった。著者は、この「現代的なRAG」において検索精度を最大化するための戦略として、ドキュメントを直接ベクトル化するのではなく、LLMで事前に要約や検索キーワードを抽出してから埋め込みモデル（Embedding）に渡す手法を紹介している。これにより、ユーザーの曖昧な問いとドキュメントの核となる情報がマッチしやすくなる。

また、実用的な工夫として、埋め込みモデルの「タスクタイプ（RETRIEVAL_DOCUMENT / RETRIEVAL_QUERY）」の使い分けや、ユーザープロファイル・履歴を活用したパーソナライズ回答の生成手順をコード付きで解説している。単なる動作確認に留まらず、本番環境を見据えたベクトルエンジンの導入、Cloud DLPによる個人情報保護、Gen AI Evaluation Serviceによる回答品質（忠実性・関連性）の評価といった、エンジニアが実務で直面するスケーラビリティやセキュリティの考慮点まで網羅されている。RAGの「中身」をブラックボックス化させず、自律的にコントロールしたいエンジニアにとって、実装の解像度を高める極めて実践的なガイドとなっている。