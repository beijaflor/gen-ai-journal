## 「不確実なAI」の必要性：なぜチャットボットは「わからない」と言うべきなのか

https://uxdesign.cc/the-case-for-the-uncertain-ai-why-chatbots-should-say-im-not-sure-8d8b4d2bab89

**Original Title**: The case for the uncertain AI: Why chatbots should say “I’m not sure”

提唱する：AIの不確実性をあえて開示し、ユーザーとの信頼を築く「知的な謙虚さ」をUI/UXに組み込む必要性。

**Content Type**: 🤝 AI Etiquette (AIエチケット)
**Language**: en

**Scores**: Signal:4/5 | Depth:3/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:4/5
**Main Journal**: 76/100 | **Annex Potential**: 76/100 | **Overall**: 76/100

**Topics**: [[AI UX, RLHF, Hallucination, Intellectual Humility, Interface Design]]

昨今のAIチャットボットが、根拠のない情報を極めて高い自信で回答してしまう「**自信満々の誤り（Overconfidence Trap）**」という深刻なUX上の課題を論じています。その背景として、**RLHF（人間からのフィードバックによる強化学習）**が、AIに対して「正確さ」よりも「ユーザーを満足させる回答」を優先するよう学習させてしまう点や、推論に使用される**トークン**のコスト制限が深い思考を妨げている現状を指摘しています。

著者は、AIに人間のような「知的な謙虚さ」を実装し、不確実な際には「確信がない」と正直に伝える設計の必要性を提唱しています。具体的には、回答の根拠を明示する**Attribution（ソースの引用）**機能の強化や、モデルの確信度を視覚的に表現するUI/UXデザインの導入により、ユーザーとの信頼関係を再構築すべきだと主張しています。単なる性能向上だけでなく、AIの限界を適切に伝える「透明性」が次世代のAI開発における鍵となります。

AIエージェントや対話型インターフェースを開発しており、ハルシネーションへの対処やユーザーの期待値コントロールに課題を感じているエンジニアやプロダクトデザイナーにとって、極めて示唆に富む内容です。