## DeepseekR1-0528の挑戦（学習できない！）

https://qiita.com/HarachiFu/items/c8148bad462dc5909bc3

松尾研LLMコンペ参加チームは、巨大なMoEモデルDeepSeekR1-0528の学習に失敗した経験を共有し、FSDPのMoE非対応が原因であることを突き止め、DeepSpeedを解決策として提案します。

**Content Type**: Research & Analysis

**Scores**: Signal:4/5 | Depth:5/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 89/100 | **Annex Potential**: 89/100 | **Overall**: 88/100

**Topics**: [[LLMファインチューニング, MoEモデル, 分散学習, DeepSeekR1-0528, DeepSpeed]]

松尾研LLMコンペに参加したチームは、DeepSeek社が開発した6710億パラメータの巨大なMoEモデル「DeepSeekR1-0528」のファインチューニングに挑戦しましたが、最終的に学習に失敗しました。この失敗は、最先端のMoEモデルを実運用レベルで扱う上での具体的な課題と、適切な分散学習フレームワークの選択がいかに重要であるかを明確に示しています。

DeepSeekR1-0528は、MoEアーキテクチャと推論用のFP8形式で公開されており、まず学習可能なBF16形式への変換が必須です。さらに、その極めて巨大なサイズ（671B）とMoE構造（エキスパート層の独自命名）から、多数のGPUと専用の分散並列処理フレームワークが不可欠となります。チームが直面した最大の失敗要因は、分散学習ライブラリとしてFSDP (Fully Sharded Data Parallel) を採用したことでした。FSDPは、MoE層の特殊性を適切に認識できず、エキスパートの重みを非効率に分割してしまう上に、MoEモデルに特有のAll-to-All通信を破壊し、致命的なボトルネックを発生させました。具体的なエラーログには、「Attention weights should be of size (1, 128, 1024, 2048), but is torch.Size([1, 128, 1024, 1024])」という、テンソルサイズ不整合を示す重要な情報が記録されています。

この挑戦と失敗から得られた教訓は、MoEモデルのような革新的な大規模LLMの学習には、そのモデル構造の特殊性を深く理解し、それに最適化された分散学習フレームワーク（具体的にはDeepSpeed）の選定が不可欠であるということです。DeepSpeedは、ZeROメモリ削減技術に加え、MoE特有のAll-to-All通信を最適化する機能を持つため、この分野のデファクトスタンダードと見なされています。ウェブアプリケーションエンジニアが将来的にAIエージェントや高度なコード生成機能の実装を目指す上で、このような巨大モデルのファインチューニングは重要な選択肢となりえます。しかし、そのためにはモデルのアーキテクチャ、メモリ要件、そしてそれを支える分散学習技術に関する深い知見が求められることを、今回の事例は強く示唆しています。単に最新モデルを導入するだけでなく、その「動かし方」まで見通す視点が、プロダクトの成功に直結するでしょう。