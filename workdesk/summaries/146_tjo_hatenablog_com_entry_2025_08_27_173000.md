## 「推論する生成AI」は事前学習されていない課題を正しく推論することができない（共変量シフトに弱い）

https://tjo.hatenablog.com/entry/2025/08/27/173000

厳密な実験は、Chain-of-Thought (CoT) を用いる生成AIの推論能力が、事前学習データへのパターンマッチングに過ぎず、共変量シフトに極めて脆弱であることを解明する。

**Content Type**: 🔬 Research & Analysis

**Scores**: Signal:4/5 | Depth:4/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 83/100 | **Annex Potential**: 84/100 | **Overall**: 84/100

**Topics**: [[LLM推論能力, Chain-of-Thought (CoT), 共変量シフト, 事前学習データと汎化性能, AI評価手法]]

渋谷駅前で働くデータサイエンティストのブログは、Chain-of-Thought (CoT) を用いた生成AIの「推論」能力の限界に関する最新論文を詳細に紹介した。本研究は、CoT推論LLMが真の論理的推論ではなく、事前学習データへの洗練されたパターンマッチングに過ぎず、未知の問題に対する汎化性能が極めて低いことを厳密な実験で示唆している。

論文では、CoT推論性能に影響を与える普遍的な3つの次元（課題の種類、テキスト・トークンの長さ、課題のフォーマット）を自由に操作可能な人工データセット「DataAlchemy」を考案。これにより、LLMをゼロから事前学習させ、事前学習データとテストデータの分布の乖離度（共変量シフト）がCoT推論パフォーマンスにどう影響するかを網羅的に測定した。結果、データ分布が乖離すればするほど、LLMの推論パフォーマンスが著しく低下することが判明。新規の課題ルール、未知のアルファベット、異なる文字列の長さ、推論ステップ数の変化、ノイズの混入など、あらゆる共変量シフトの条件下で性能が劣化し、最終的な回答が誤ることが明らかになった。

この知見は、LLMを含む深層学習が「丸暗記」に近いレベルで学習し、そのパターン外の状況に極めて弱いという既存の認識を補強するものだ。私たちウェブアプリケーションエンジニアにとって、これはAIを活用したコーディングやエージェント開発において極めて重要な示唆を与える。AIに複雑なロジック生成やデバッグを依頼する際、モデルが学習データで経験したことのないような特殊なケースや、わずかに異なる形式の入力に対しては、期待通りの「推論」を行わず、誤った結果を生成するリスクが高いことを意味する。

したがって、LLMの能力を過信せず、真に新しい問題解決には人間による深い介入や検証が不可欠であると理解すべきだ。プロンプト設計や評価データセットの構築においては、モデルが直面し得る共変量シフトを考慮し、より堅牢なシステム構築を目指す必要がある。LLMのモデルサイズや温度を変更してもこの問題は解決しないため、評価時には事前学習データから十分に乖離したテストデータを準備することが強く提言されている。