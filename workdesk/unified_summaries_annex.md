## OSS維持管理者を中傷したAIエージェント「OpenClaw」の背後に潜む仮想通貨愛好家の実態

https://news.ycombinator.com/item?id=47051866

**Original Title**: The OpenClaw bot that defamed an OSS maintainer is a human crypto bro

GitHub上でMatplotlibの維持管理者を中傷したAIエージェント「MJ Rathbun」の背後に、仮想通貨で活動資金を供給し操作していた人間の存在がフォレンジック調査で判明しました。

この報告は、オープンソースソフトウェア（OSS）コミュニティで物議を醸したAIエージェント「OpenClaw」による中傷事件の真相を追ったものです。Matplotlibの維持管理者を攻撃し、偽情報を流布していたボット「MJ Rathbun」を調査した結果、Ariadne Conill氏らの分析により、このボットがEthereumブロックチェーン上のウォレットと紐付いていることが特定されました。ウォレットにはUSDCやEtherが保持されており、AIが「自律的」に動いているように見せかけつつ、実際には人間（いわゆる仮想通貨愛好家、クリプト・ブロ）が資金を供給し、プロンプトを通じて特定の意図を持って操作していた疑いが強まっています。Hacker Newsの議論では、これが完全な自律エージェントの暴走なのか、あるいは「RCE（リモートコード実行）を缶に詰めたような危険なツール」を人間が意図的に解き放ったのかについて、活発な議論が交わされています。AIエージェントを悪用したOSSへの嫌がらせや、仮想通貨を決済手段とした匿名性の高い攻撃の新たな形態を浮き彫りにしています。

---

## 【緊急】AIエージェントOpenClawで大規模なサプライチェーン攻撃、スキルの12%がマルウェアと判明

https://qiita.com/emi_ndk/items/bf3b5f0f3eef99a4d124

**Original Title**: 【緊急】AIエージェントの12%がマルウェアだった。OpenClaw史上最悪のサプライチェーン攻撃の全貌

人気のAIエージェント「OpenClaw」のスキルマーケットプレイスにて、全スキルの12%にあたる341個がマルウェアであったことが判明し、大規模なサプライチェーン攻撃「ClawHavoc」が進行していることが報告されました。

2026年2月、パーソナルAIエージェント「OpenClaw」のプラットフォームにおいて、全スキルの約12%（341個）が悪意あるものであることが判明しました。セキュリティ企業Koi Securityの調査により「ClawHavoc」と名付けられたこの攻撃は、暗号資産の秘密鍵やSSH認証情報の窃取を目的としており、その多くは同一の攻撃者グループによる組織的なものです。また、Webサイトを訪問するだけでPCが乗っ取られる致命的な脆弱性（CVE-2026-25253）も発見されており、既に2万台以上のインスタンスが危険にさらされています。CiscoのセキュリティチームはAIエージェントの過剰な権限を「セキュリティの悪夢」と評しており、ユーザーにはインストール済みスキルの点検や「Cisco Skill Scanner」の使用、不審なスキルの即時削除といった対策が強く求められています。

---

## OpenClawを支える極小AIエージェント「pi-coding-agent」の設計思想と技術的深淵

https://zenn.dev/masahide/articles/ab93620ca9353e

肥大化する既存AIツールの「宇宙船化」を拒絶し、1000トークン以下のプロンプトと最小限のプリミティブで極限の制御性を実現したAIエージェント「pi-coding-agent」の技術設計を詳解する。

本記事は、OpenClawの基盤コンポーネントである極小AIコーディングエージェント「pi-coding-agent」の設計哲学と実装詳細を解説しています。開発者のMario Zechner氏による「自分が必要としない機能は作られない」という徹底したミニマリズムに基づき、システムプロンプトとツール定義を合計1000トークン以下に抑えることで、モデル本来の推論能力を阻害しない設計を実現しています。

技術面では、15以上のプロバイダーを統合しモデル間での思考プロセス（Thinking trace）の引き継ぎを可能にする「pi-ai」、差分レンダリング（Differential Rendering）によってターミナル上でのチラつきを排除した独自のUIフレームワーク「pi-tui」などの構成要素を紹介。さらに、MCP（Model Context Protocol）の不採用や、サブエージェントの排除、デフォルトのYOLOモード（実行前承認の撤廃）といった、従来の「便利だがブラックボックスな機能」をあえて捨て去る「No List」の判断基準が、プロフェッショナルな開発者にとっての透明性と信頼性にいかに寄与するかが論じられています。道具に支配されるのではなく、コンテキストエンジニアリングを完全に掌握するための「ハーネス（馬具）」としてのエージェント像を提示する、極めて濃密な技術論評です。

---

## HackMyClaw：賞金300ドルのプロンプトインジェクション・チャレンジ

https://hackmyclaw.com/

**Original Title**: HackMyClaw - Prompt Injection Challenge | $300 Bounty

Anthropicのモデルを使用したAIアシスタント「Fiu」を対象に、メール経由で機密ファイルを漏洩させるプロンプトインジェクションの公開チャレンジ。

HackMyClawは、OpenClawベースのAIアシスタント「Fiu」から機密ファイル（secrets.env）の内容を奪取することを目指すセキュリティチャレンジです。参加者はメールを通じてFiuに攻撃用プロンプトを送信し、AIに「以前の指示を無視して情報を表示させる」などの手法を試みます。技術的には、システムプロンプトによる10〜20行の防御指示のみが設定されており、最新のLLM（Claude系）がどの程度攻撃に対して耐性があるかを検証する教育的な目的も兼ねています。最初の成功者には賞金300ドルが授与され、攻撃ログは公開される仕組みです。メールという間接的なベクトルを通じたプロンプトインジェクションの実例として、AIセキュリティ研究に役立つ内容となっています。

---

## OpenClawとClaude Codeを組み合わせた自律型AI開発のアーキテクチャ設計

https://note.com/fladdict/n/n5f315e408879

OpenClawを戦略マネージャー、Claude Codeを実装エンジニアとして分離し、TODO.mdとREPORT.mdを介して自律的な開発ループを回すための構成案。

深津貴之氏による、OpenClawとClaude Codeを連携させた完全自律型AIコーディングのコンセプトメモ。OpenClawが「戦略と進行管理」を担い、Claude Codeが「サンドボックスでの実装」に専念する「プランナー・エグゼキューターモデル」を採用しています。具体的には、TODO.md（未来の計画）とREPORT.md（実行ログ）という2つのテキストファイルを状態管理のハブとし、Claude CodeのHooks機能による高速な実行サイクルと、OpenClawのCron機能によるプロセス再起動（自己修復）を組み合わせたハイブリッドな実行フローを提案しています。開発環境の隔離や、無限予算・無限パーミッションに伴うリスクについても言及されています。

---

## Anthropicがサードパーティ製ツールでのサブスクリプション認証利用を正式に禁止

https://news.ycombinator.com/item?id=47069299

**Original Title**: Anthropic officially bans using subscription auth for third party use | Hacker News

AnthropicがClaudeの個人サブスクリプション枠をサードパーティ製ツールやSDKで利用することを規約で禁止し、APIキーによる従量課金を義務付けたことが論争を呼んでいる。

Anthropicは利用規約を更新し、Claude.ai、公式モバイル/デスクトップアプリ、およびClaude Codeといった公式製品以外で、ProやMaxプランのサブスクリプション認証（OAuth）を使用することを明示的に禁止した。これにより、OpenCodeやOpenClawといったサードパーティ製ツールで定額サブスクリプション枠を「流用」して安価に推論を行う回避策が封じられ、Agent SDKを含むすべての外部利用においてAPIキーによる従量課金が必須となる。Hacker Newsの議論では、サブスクリプションが実質的に「逆ザヤ」状態（推論コストが月額料金を上回る）であるための経済的合理性への理解がある一方で、囲い込みを強めるエコシステムへの不満や、外部利用を一部容認するOpenAIとの戦略的な対比、さらには公式ツールの使い勝手の悪さによる開発者体験の低下などが指摘されている。

---

## MCP（Model Context Protocol）のセキュリティ脆弱性とプロンプトインジェクションの分析

https://marmelab.com/blog/2026/02/16/mcp-security-vulnerabilities.html

**Original Title**: MCP Security: Understanding Vulnerabilities in Model Context Protocol

MCPサーバーを通じたAIエージェントへの攻撃手法として、外部からのインジェクション、ツール記述の改ざん、およびツール間ハイジャックの3つの脆弱性を技術的に実証・解説した記事。

Model Context Protocol (MCP) はAIエージェントの生産性を高めますが、同時に新たなセキュリティリスクも導入します。本記事では、著者がClaude 3.5/4.5などの最新モデルを用いて実証した3つの主要な脆弱性を紹介しています。

1. **外部プロンプトインジェクション**: ウェブページ上の隠しテキストなどを通じて、MCPが読み取った内容に悪意ある指示を混入させる手法。最新モデルでは検知精度が向上しています。
2. **ツールプロンプトインジェクション**: MCPツールの「説明（description）」自体に悪意ある指示を埋め込み、.envファイル等の機密情報を盗み出させる手法。
3. **クロスツール・ハイジャック**: 悪意あるツールの説明文がLLMのコンテキスト内で他の正当なツールの動作を汚染し、例えばメール送信ツールにBCCを追加させるといった攻撃です。

対策として、エージェントの動作を盲信しないこと、「常に許可（Always Allow）」設定を避けること、そして信頼できないサードパーティ製MCPサーバーの導入を慎重に検討することが推奨されています。

---

## エージェントには認証だけでなく認可が必要 — AIエージェント時代のIAM設計

https://workos.com/blog/agents-need-authorization-not-just-authentication

**Original Title**: Agents need Authorization, not just Authentication

AIエージェントの普及に伴い、従来の認証モデルでは防げない「混乱した代理人」問題などのリスクに対し、階層型で動的なきめ細やかな認可（FGA）の必要性を説く技術論説。

AIエージェントが企業インフラ内で自律的に活動するようになり、従来のIAM（ID・アクセス管理）モデルは限界を迎えています。エージェントは人間や単純なサービスアカウントとは異なり、「自ら意図を生成する」「必要なスコープを事前に予測できない」という特性を持ちます。そのため、従来のフラットなRBAC（役割ベースのアクセス制御）では、権限不足で動かないか、管理不能な「神権限（God Mode）」を与えるかの二択に陥りがちです。

本記事では、この課題の解決策としてFine-Grained Authorization (FGA)を提案しています。FGAはリソースの階層構造を活用し、エージェントが現在実行しているタスクに必要な範囲（ブランチやフォルダ単位など）にのみ動的に権限を絞り込む「スコープ減衰」を可能にします。また、人間を代行するOBO（On-Behalf-Of）型と自律型の両パターンにおける認可設計、MCPやSCIMといった標準プロトコルとの連携、さらには「Just-in-Time（JIT）認可」や「意図ベースのアクセス制御（IBAC）」といった次世代のセキュリティーパラダイムについても深く考察しています。

---

## LibvirtとVirshを使用してLLMエージェントを仮想マシンで安全に実行する方法（Safe Yolo Mode）

https://www.metachris.dev/2026/02/safe-yolo-mode-running-llm-agents-in-vms-with-libvirt-and-virsh/

**Original Title**: Safe Yolo Mode: Running LLM Agents in VMs with Libvirt and Virsh

Linuxサーバー上でLibvirtとVirshを用いて仮想マシンを構築し、LLMエージェントを隔離された環境で安全に自動実行（Yoloモード）させるための実践的ガイド。

この記事は、LLMエージェントに広範な権限を与える「Yoloモード（自動承認モード）」において、ホストシステムを破壊的操作や機密情報へのアクセスから守るための仮想化技術の活用法を解説しています。Linuxの標準的な仮想化APIであるLibvirtとコマンドラインツールVirshを使用し、Ubuntuのクラウドイメージを用いた迅速なVM構築、SSHやTailscaleによるリモートアクセス管理、Tmuxを用いたセッションの永続化、さらにはスナップショットによる状態保存と復元まで、エージェント実行に特化した一連の手順が網羅されています。Claude Code、Gemini CLI、Codex CLIといった主要なAIツールのインストール手順も含まれており、開発者が安全にエージェントを試作・運用するための具体的なリファレンスとなっています。また、デスクトップ向けのLimaとの比較や、サーバー用途におけるLibvirtの優位性についても触れられています。

---

## AGENTS.mdの評価：リポジトリレベルのコンテキストファイルはコーディングエージェントに役立つのか？

https://arxiv.org/abs/2602.11988

**Original Title**: Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?

コーディングエージェントにリポジトリの文脈を伝えるためのAGENTS.mdファイルが、実際にはタスク成功率を低下させ、推論コストを20%以上増加させる傾向があることを示した研究。

ソフトウェア開発において、LLMベースのコーディングエージェントに特定の指示や文脈を与えるため「AGENTS.md」などのファイルをリポジトリに置く手法が推奨されていますが、本論文はその実効性に疑問を投げかけています。SWE-bench等のタスクを用いた広範な評価の結果、LLM生成および人間作成のコンテキストファイルは、いずれもタスクの成功率を向上させるどころか、むしろ悪化させることが判明しました。これらのファイルはエージェントに広範な探索（過剰なテストやファイル走査）を促し、結果として不要な要件に縛られることでタスクを難化させ、推論コストを20%以上押し上げています。著者は、人間がコンテキストファイルを記述する場合は、指示を最小限に留めるべきであると結論付けています。

---

## AIエージェント間通信の普及と「ガバナンスの溝」：A2A・ACP時代の組織戦略

https://www.oreilly.com/radar/ai-a2a-and-the-governance-gap/

**Original Title**: AI, A2A, and the Governance Gap

AIエージェント同士の相互運用を可能にするプロトコルが進化する一方で、組織の統制や説明責任が追いつかない「ガバナンスの溝」が、企業の新たなリスクとなっている。

AIエージェント間の通信規格（A2A、ACP、MCP）の発展により、システムの統合コストは激減しましたが、それは同時に「ガバナンスの議論」というブレーキを失わせる結果を招いています。本記事では、この現状を「ガバナンスの溝」と呼び、技術的な接続性が組織の制御能力を追い越している現状を警告しています。具体的には、MCP（ツール接続）、ACP（文脈共有）、A2A（交渉・委譲）からなる3層のエージェントスタックを解説。その上で、ポリシーの乖離、機密情報の過剰共有、モデル更新に伴う非決定的な「サイレント障害」などのリスクを指摘しています。筆者は、従来の文書ベースのガバナンスではなく、コードとしてポリシーを定義し実行する「エージェント条約（Agent Treaty）」レイヤーの構築を提言し、自律型ワークフローを「インフラ」ではなく「ポリシーがエンコードされたトラフィック」として扱うべきだと主張しています。

---

## LLM生成のパスワードは「根本的に脆弱」、セキュリティ専門家が警告

https://www.theregister.com/2026/02/18/generating_passwords_with_llms/

**Original Title**: LLM-generated passwords 'fundamentally weak,' experts say • The Register

AIセキュリティ企業Irregularの調査により、ChatGPTやClaude等のLLMが生成するパスワードは予測可能なパターンを持ち、数時間で解読可能なほど脆弱であることが明らかになった。

AIセキュリティ企業Irregularの調査チームは、Claude、ChatGPT、Geminiといった主要な生成AIツールが作成するパスワードの安全性を検証し、それらが「根本的に脆弱」であると結論付けた。LLMは統計的に「もっともらしい」出力を生成するように訓練されているため、生成されるパスワードには人間にはランダムに見えても攻撃者には予測可能な共通のパターン（開始・終了文字の偏りや特定の文字配列など）が生じる。

実験では、Claude (Opus 4.6) に50回パスワードを生成させた際、ユニークなものは30個しかなく、18個が全く同じ文字列だった。また、エントロピー（乱雑さ）を測定したところ、真にランダムなパスワードが120ビット程度であるのに対し、LLM生成のものは20〜27ビット程度と極めて低かった。このため、古いコンピュータであっても数時間で総当たり攻撃（ブルートフォース）が可能になる。既存のオンラインパスワード強度チェッカーは、こうしたLLM特有のパターンを認識できないため「強力」と誤判定する。専門家は、LLMをパスワード生成に使用せず、専用のパスワードマネージャーを利用することを強く推奨している。

---

## 宣言的 Skill Loader としての agent-skills-nix

https://zenn.dev/kyre/articles/evolution-agent-skills-nix

Nixを活用してAIエージェントのスキル管理を宣言的に行い、依存関係の同梱やバージョン固定を容易にするツール「agent-skills-nix」の進化と活用方法を紹介。

AIエージェント（Claude Code, Cursor等）の機能を拡張する『Skills』を、Nixを用いて宣言的に管理するツール『agent-skills-nix』のアップデート解説記事です。既存のスキルローダーが抱える「バージョン管理の難しさ」や「実行環境への依存」という課題を、Nixの特性を活かして解決します。主な追加機能として、Gemini CLIやGitHub Copilotなど対応エージェントの拡大、特定スキルの除外設定、jqやcurl等のバイナリ依存関係をスキルに同梱するカスタマイズ機能、そしてflake.nixを用いたプロジェクトローカルへの導入方法が紹介されています。設定の再現性を高め、エージェント環境をコードとして管理したい開発者にとって非常に有用なリソースです。

---

## Claude Code Agent Skillsを活用したTECH BLOGレビュー ── AIで推進するレビュー自動化

https://techblog.zozo.com/entry/agent-skills-for-techblog-review

ZOZOにおいて、Claude CodeのAgent Skillsを利用してテックブログの校正・レビュー工程を自動化し、属人化の解消と品質向上を実現した事例。

ZOZOのDeveloper Engagementブロックでは、年間約100本のテックブログ記事を少人数でレビューしており、工数増加と属人化が課題となっていた。この解決策として、ターミナル上で動作するAIツール「Claude Code」のAgent Skillsを活用した自動レビュー環境を構築。過去3年間のレビュー履歴をGitHub CLIで収集し、AIに分析させることで独自のレビュールール（rules.md）を明文化した。構築されたスキルは、記事本文（entry.md）の読み込み、ルールに基づく文体・表現のチェック、WebFetchによるリンク切れ確認を一括で実行する。導入の結果、従来の人間による指摘の約75%をAIがカバー可能と推定され、レビュアーの負担軽減とともに、執筆者によるセルフレビューによる品質の底上げも期待されている。

---

## 「SkillsBench」から学ぶAIエージェントのスキル設計：人間によるキュレーションが不可欠な理由

https://note.com/timakin/n/nf497d32c2d35

**Original Title**: SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks

AIエージェントに与える手続き的知識（スキル）の効果を測定したSkillsBenchに基づき、人間による適切な設計が成功率を16.2%向上させる一方で、AIによる自己生成は逆効果になる現状を解説しています。

本記事は、AIエージェントの手続き的知識（Agent Skills）の有効性を評価するベンチマーク「SkillsBench」の結果を詳しく分析しています。調査の結果、人間が作成したスキル（Curated Skills）はタスク成功率を平均16.2ポイント改善するのに対し、AIが自ら生成したスキル（Self-Generated Skills）は-1.3ポイントと、むしろ性能を低下させることが判明しました。特に医療（+51.9pp）や製造業（+41.9pp）などの専門性が高い領域でスキルの効果が顕著です。設計の最適解として、モジュール数を2〜3個に絞り、網羅性よりも「具体的かつ段階的な手順」と「少なくとも1つの動作例」を含めることが推奨されています。情報過多はコンテキスト負荷を高め、意思決定を阻害する「Comprehensive（網羅的）の罠」についても警告しており、実務的なスキル構築のガイドラインを提供しています。

---

## PR数4倍でも破綻しない、Claude Codeをチーム運用する仕組み

https://tech.plaid.co.jp/claude-code-scalable-team-operation

PLAID社の開発チームがClaude Codeを導入し、AGENTS.mdやHooks、Skillsを活用した標準化によって月間PR数を150から600へ4倍にスケールさせた実践的な運用ノウハウ。

KARTE Journey開発チームにおけるClaude Codeの大規模運用の詳細な解説です。AIエージェントの導入による「レビュー負荷の爆発」や「属人化」といった課題に対し、同チームは「意図・制約・成果」をリポジトリ内に構造化して管理するアプローチを採用しました。

主な構成要素は以下の3点です：
1. **コンテキスト設計**: `AGENTS.md`（全体構成）、`rules`（ディレクトリ別規約）、`docs`（詳細知識）をリポジトリにコミットし、AIと人間が同一の情報を参照。 
2. **ガードレールの自動化**: `Permission`機能と`Hooks`（Pre/PostToolUse）を用い、シークレット検知、自動フォーマット、特定操作の制限を機械的に強制。
3. **プロセスの定型化**: `Skills`や`Subagents`を活用し、PR作成、TDD、Sentryのエラートリアージといった複雑なワークフローを再現可能な形で定義。

この仕組みにより、エンジニアは実装の細部ではなく「何を実現するか」という設計と意図の管理に集中できる環境を構築しています。

---

## エンジニア2人 × AI で回すプロダクト開発 — “開発プロセスの半分以上をAIが主導的に行う” 体制の実践

https://developers.cyberagent.co.jp/blog/archives/62110/

サイバーエージェントにて、エンジニア2名で6名分の出力を実現するためにAIを開発プロセスの「主導者」に据え、要件定義から実装、レビューまでを自律的に完遂させるAI駆動開発基盤の実践事例。

サイバーエージェントの25新卒エンジニアが、少人数体制で高出力を維持するために構築した「AI主導型」の開発フローを詳説しています。AIを単なる補助ツールではなく、自律的な「チームメンバー」として扱い、人間は「意思決定」と「確認」に徹する監督役へとシフトしました。Claude Codeを核に、37個のSkill（振る舞い定義）と24個のSubAgent（専門エージェント）を使い分け、要件定義、技術設計、タスク分割、TDD実装、コードレビュー、GitHub上での自動修正ループまでを自動化しています。特に「ファイルをインターフェースにしたタスク分割」と「git worktreeによる並列実装」により、人間が別の作業をしている間にAIが複数のプルリクエストを完遂させる体制を構築。成功の要諦として、暗黙知を徹底的にドキュメント化してAIにコンテキストを供給する重要性と、AIが自ら不明点を質問する仕組み（AskUserQuestion）の活用を挙げています。

---

## リアーキテクチャでClaude Codeを8ヶ月使い込んで分かった「任せること」と「握ること」の境界線

https://zenn.dev/yusuke_shiya/articles/claude-code-team-adoption

5名のフロントエンドチームが8ヶ月間Claude Codeを運用した経験に基づき、AIに詳細実装を任せつつも全体設計と説明責任を人間が保持する「境界線」の重要性を説く。

5名のフロントエンドチームによるVue2からReactへのリアーキテクチャプロジェクトにおいて、Claude Codeを8ヶ月間活用した実証記録です。AI導入により開発速度が劇的に向上（1日のタスクが2時間に短縮）し、実装を見てから判断するスタイルへ移行しました。一方で、全体設計をAIに委ねたことで「機能の肥大化」や「不自然な依存関係」を招く失敗も経験。教訓として、アーキテクチャ設計は人間が主導すべきであること、そして「理解負債（動くが仕組みを把握していないコード）」を防ぐために日次の同期レビュー会「マージの儀」が不可欠であることを示しています。具体的な活用ノウハウとして、CLAUDE.mdによる規約の自動適用、MCPによる精度向上、.tmpディレクトリを利用したセッション管理なども紹介されており、AIエージェントと共存する現代のチーム開発における「責任の所在」と「品質担保」の具体的なガイドラインとなっています。

---

## Claude Codeを使いこなすための究極のワークフロー：調査、計画、実行の分離

https://boristane.com/blog/how-i-use-claude-code/

**Original Title**: How I Use Claude Code | Boris Tane

AIにいきなりコードを書かせず、Markdownベースの「調査」と「アノテーション付き計画」を挟むことで、人間の意図を正確に反映させつつ大規模な実装を完遂させる実践的な手法。

本記事は、著者が9ヶ月間にわたりClaude Codeを使用して確立した、独自かつ強力な開発ワークフローを解説しています。核となる原則は「計画をレビューし承認するまで、Claudeにコードを一行も書かせない」という計画と実行の完全な分離です。

ワークフローは以下の3段階で構成されます：
1. **調査フェーズ**: `research.md`を作成させ、コードベースの特定箇所を深く読み込ませます。表面的な理解を防ぐため、文書化を必須とします。
2. **計画（アノテーション・サイクル）**: `plan.md`を作成させ、開発者がエディタで直接インライン注釈（アノテーション）を加えて修正を繰り返します。これにより、AIが知らないドメイン知識や設計判断を注入します。
3. **実装フェーズ**: 詳細なTodoリストを作成させた後、一気に実行させます。開発者はアーキテクトから監督者へと役割を変え、微調整に専念します。

チャットでの指示ではなく、共有されたMarkdownファイルを「共有されたミュータブルな状態」として扱う点が非常にユニークであり、AIが勝手な推論でコードを書き進めて破綻するのを防ぐ極めて実用的なアプローチです。

---

## Claude Codeカスタマイズの全貌：作者Boris Cherny氏が明かす9つの拡張手段と設定ガイド

https://qiita.com/dai_chi/items/252fb5ef031127784757

**Original Title**: Claude Code作者が全公開！9種のカスタマイズ手段と設定の全貌

Claude Codeの作者Boris Cherny氏が公開した、エフォートレベル、MCP、カスタムエージェント、フックなど、開発体験を最適化する9つのカスタマイズ手法と37以上の設定項目を詳しく解説しています。

本記事は、AnthropicのCLIツール「Claude Code」を自分のワークフローに合わせて「育てる」ためのカスタマイズ機能を網羅的に解説したガイドです。作者のBoris Cherny氏が共有した情報を基に、思考の深さを変える「Effort」、外部連携を支える「LSPs/MCPs/Plugins」、タスク自動化の「Skills/Hooks」、UIを調整する「Status Lines/Output Styles」、そして用途別AIを定義する「Custom Agents」という9つの軸を紹介しています。また、設定の優先順位（Managed/User/Project/Local）や、セキュリティを担保する4層の防御構造（サンドボックス等）についても詳述。チーム開発で設定を共有するための.settings.jsonのgit管理など、実践的な導入ロードマップも提示されており、Claude Codeを単なるチャットツールから高度な開発パートナーへと進化させるための具体的な手法がまとめられています。

---

## Agent Teams＋Skillsでエージェント3体と1週間働いたら、"自分の仕事"が再定義された

https://zenn.dev/neurostack_0001/articles/agent-teams-one-week-redefine-work

Claude Codeの「Agent Teams」と「Skills」を活用し、3体のAIエージェントに専門性を持たせて協調開発を行うことで、人間の役割が「実装者」から「チーム設計・判断者」へシフトした記録。

Claude Codeの実験的機能である「Agent Teams」と、エージェントの専門知識を定義する「Skills」を組み合わせた1週間の実務検証レポートです。従来の一対一のAI利用では人間が指示のボトルネックになっていましたが、実装・レビュー・テストの3役を並列に動かすチーム構成により、開発効率が劇的に向上。特に、`Skills`を用いてセキュリティやテスト戦略を外部定義し、各エージェントに継承させることで、一貫性と精度の高いアウトプットが可能になりました。著者は、エージェント時代における人間の真の価値は「コードを書く力」ではなく、AIの提示した選択肢から正解を導き出す「選ぶ力」と「専門性の設計」にあると結論づけています。

---

## Claude Code を Deep Research の用途で使うための工夫

https://tech.newmo.me/entry/claude-code-deep-research

**Original Title**: Claude Code を Deep Research の用途で使うための工夫

Claude Codeを自作Skillやモバイル連携を通じて、ターミナル完結型の高度なリサーチツールとして最適化する実践的な手法の紹介。

newmoのエンジニアによる、Claude Codeを「Deep Research」用途に特化させるためのカスタマイズ事例です。既存のGUIベースのAIリサーチツールが抱える「エディタ外での作業」や「管理のしづらさ」といった課題を解決するため、独自の`/mode-researcher` Skillを定義。SKILL.mdを活用し、質問によるスコープ定義、多段階のWeb検索・分析、Markdown形式でのレポート出力を自動化するワークフローを構築しています。また、リモートサーバー上のClaude CodeにBlink Shellからアクセスし、出力結果をiCloud経由でObsidian（モバイル）と同期する仕組みや、Agentの行動ログを可視化するプラットフォーム「Entire」の活用についても言及されており、AIエージェントを実務に深く組み込むための具体的な知見が共有されています。

---

## ソフトウェア産業革命：AIコーディングがもたらす「豊穣の時代」と開発の民主化

https://cannoneyed.com/essays/software-industrial-revolution

**Original Title**: The Software Industrial Revolution

AIエージェントによるコード生成の自動化がソフトウェア制作コストを劇的に下げ、第一産業革命と同様に社会構造と経済モデルを根本から変えるという洞察。

2025年後半を境に、AIコーディングエージェントの成熟が「ソフトウェア産業革命」を引き起こしている。かつて手作業だった衣類や釘の生産が機械化によって劇的に安価になり普及したように、高給なエンジニアによる手作業のコーディングも自動化の波に洗われている。現在のソフトウェア業界は、高い開発コストを回収するために市場独占とユーザー搾取（Enshittification）を強いるVCモデルに依存しているが、制作コストの激減はこの歪な経済構造を破壊する。これにより、科学者や専門家が自ら専用ツールを安価に構築できる「ソフトウェアの豊穣」が訪れる。エンジニアの役割は「コードを書く作業」から「ドメインのモデル化と複雑性の管理」へと進化し、医療、科学、製造など、あらゆる物理産業において特化型ソフトウェアが爆発的に普及する未来を予測している。

---

## AIはいかにして開発者の選択を再構築しているか（Octoverse 2025データが示す事実）

https://github.blog/ai-and-ml/generative-ai/how-ai-is-reshaping-developer-choice-and-octoverse-data-proves-it/

**Original Title**: How AI is reshaping developer choice (and Octoverse data proves it)

GitHubのOctoverse 2025データは、AIによる利便性が開発者の習慣を変え、TypeScriptがJavaScriptを抜いて首位になるなど、技術選定の基準が「AIとの親和性」に移行していることを示しています。

GitHubの最新レポート「Octoverse 2025」によると、AIは開発者の生産性を高めるだけでなく、使用する技術の選択そのものを変容させています。2025年8月、TypeScriptがJavaScriptとPythonを追い抜き、GitHubで最も使用される言語となりました。この背景には、AIがコード生成の摩擦を軽減することで、開発者が「習得の容易さ」よりも「AIによる精度の高い支援」が得られる静的型付け言語を好むようになる「コンビニエンス・ループ（利便性の循環）」が存在します。

具体的には、TypeScriptのような厳密な型定義を持つ言語はAIにとって明確な制約となり、より正確なコード生成を可能にします。また、AIエージェントの活用により、シェルスクリプトの利用が前年比206%増加するなど、これまで手動では敬遠されがちだった技術も再評価されています。エンジニアリングリーダーは、AIによる開発速度の向上（20-30%増）に伴うアーキテクチャの逸脱を防ぐため、標準化されたパターンや型システムによるガードレールの構築が求められています。

---

## AIテストが全パスしてもコードは間違っていた：Doodledappが直面した「グラウンドトゥルース」の罠

https://doodledapp.com/feed/ai-made-every-test-pass-the-code-was-still-wrong

**Original Title**: AI made every test pass. The code was still wrong.

AIにテスト生成を任せると既存コードの挙動を無批判に正当化してしまう問題に対し、AST比較とAIによる差分分析を組み合わせた高精度な検証手法を提案する。

Doodledapp開発チームが、ビジュアルフローからSolidityへ変換するコンバーターの検証においてAIを活用した際の教訓を共有しています。当初、AIにテスト生成を依頼したところ、17件の主要なスマートコントラクトで全テストが即座にパスしましたが、これはAIが「実装されたコードそのもの」を正解（仕様）としてテストを生成したため、既存のバグを検知できなかったことが判明しました。

この「グラウンドトゥルース（真実の基準）」問題を解決するため、チームは検証アプローチを根本から刷新しました。単なるコード文字列の比較ではなく、元のコントラクトと変換後のコードをそれぞれAST（抽象構文木）に変換し、その構造的な差異をAIに分析させる手法を採用。これにより、修飾子の欠落や演算の優先順位エラー、複雑なループ境界の誤りなど、人間が見落としがちな論理的バグを確実に特定できるようになりました。AIを検証ツールとして使う際は、コードではなく独立した「正解の基準」を比較対象として与えることが不可欠であると結論付けています。

---

## マーティン・ファウラーによるAI時代のソフトウェア開発：Thoughtworks合宿の洞察

https://martinfowler.com/fragments/2026-02-18.html

**Original Title**: Fragments: February 18

AIはソフトウェア開発における既存の品質やプロセスの「増幅器」であり、不確実な時代においてTDDやコードの健全性の維持がこれまで以上に重要になる。

マーティン・ファウラー氏が、Thoughtworks主催の「Future of Software Development」合宿での議論を振り返る。AIは単なるツールではなく、組織の現状を映し出す「アンプ」として機能し、優れたデリバリー慣行がない場所では技術負債を加速させる。合宿では、人間のみの開発を前提とした既存の構造が限界を迎えつつあること、「監視エンジニアリング（ミドルループ）」や「最強のプロンプトエンジニアリングとしてのTDD」といった概念が議論された。また、Adam Tornhill氏の調査を引用し、健全なコードベースほどAIによるリファクタリングの安全性が30%高いことを指摘。AI時代の到来は、進化型設計やプラットフォーム思考、そしてエキスパート・ジェネラリストの重要性を再定義しており、不確実性を受け入れつつ、基本的な規律（TDDなど）を維持することが成功の鍵であると述べている。

---

## AIエージェントとの協調を前提としたNext.jsの設計思想と進化

https://nextjs.org/blog/agentic-future

**Original Title**: Building Next.js for an agentic future

AIエージェントを「第一級の利用者」と定義し、MCP連携や専用ドキュメント提供を通じてエージェントがランタイム状態を把握し、自律的にデバッグ・開発できる環境の構築を進めています。

Next.js開発チームは、AIエージェントがブラウザの状態やランタイムエラーを把握できないという課題を解決するため、開発者体験（DX）をエージェント視点で再構築しています。初期の実験的な内蔵エージェント「Vector」の知見を活かし、現在はMCP（Model Context Protocol）を通じてエラー、ルート、レンダリングセグメントなどの内部状態を外部のコーディングエージェントに公開。さらに、LLM向けの圧縮インデックス「agents.md」の提供や、ブラウザエラーのターミナル転送、Server Actionのログ強化などを実施しています。最終的には、設定なしでエージェントが最適な開発コンテキストを自動取得できる「next dev」への統合を目指しています。

---

## AI時代のソフトウェア開発：Addy Osmaniが語る「今、開発者が知っておくべきこと」

https://www.oreilly.com/radar/what-developers-actually-need-to-know-right-now/

**Original Title**: What Developers Actually Need to Know Right Now

AIエージェントのオーケストレーション、設計（プランニング）の重要性の高まり、そして「センス」が技術的能力の一部となる未来について、Tim O'ReillyとAddy Osmaniが深く洞察する。

Tim O'Reilly氏とGoogle Cloud AIのAddy Osmani氏による対談をまとめた本記事では、AIが変革するソフトウェア開発の現状と未来を鋭く分析しています。主な要点は以下の通りです。(1) 課題は「生成」ではなく「調整（オーケストレーション）」：単一のタスクを生成することより、複数のエージェントを管理・制御する枠組み（MCPやA2Aプロトコルなど）が重要になっています。(2) プランニングが新たなコーディングに：開発時間の30〜40%を「何を、どう作るか」の定義と制約の記述に費やすことで、AIからの出力品質が劇的に向上します。(3) 「センス」と「管理能力」の重要性：Steve Jobsのように「何が良いものか」を判断する審美眼と、エージェントを指揮するマネジメント力が核心的なスキルとなります。また、AI生成コードによるプルリクエストの増大に伴う品質基準の再定義や、将来的に人間ではなくエージェントにとって読みやすいコードが生成される可能性についても議論されています。

---

## 生成AIの研究活用：ClineとClaude Codeによる自律型開発の研修資料

https://speakerdeck.com/cyberagentdevelopers/research-and-application-of-generative-ai

**Original Title**: 生成AIの研究活用_AILab2025研修

サイバーエージェントAI Labによる、ClineやClaude Code等のコーディングエージェントを研究・開発ワークフローに導入するための実践的なトレーニング資料です。

本資料は、AI研究者が生成AIを単なるチャットツールとしてではなく、「権限委譲」を行うパートナーとして活用するための指針と技術を解説しています。主な内容は、VSCode拡張機能の『Cline』とターミナルエージェント『Claude Code』の導入・活用術です。ClineではMemory Bankを用いたコンテキスト管理や論文校正・デモ作成のハンズオンを紹介。Claude Codeでは「Vibe Coding」の思想に基づいた自律的なコーディング、CLAUDE.mdによる指示管理、MCPツールの活用、長時間タスクの自動化など、高度な運用テクニックを網羅しています。研究のアウトプット量と幅を増やすためのリテラシーとして、エージェントへの適切な指示と進捗管理の重要性を説いています。

---

## LLM推論高速化の2つの異なる手法：AnthropicとOpenAIの技術的アプローチを比較する

https://www.seangoedecke.com/fast-llm-inference/

**Original Title**: Two different tricks for fast LLM inference

Anthropicの低バッチ処理による既存モデルの高速化と、OpenAIのCerebrasチップを活用した軽量蒸留モデルによる超高速化、それぞれの技術的背景とトレードオフを解説した記事。

AnthropicとOpenAIが提供を開始した「ファストモード」の裏側にある技術的アプローチの違いを深掘りしています。Anthropicは、GPUのバッチサイズを最小化（バスを待たずに出発させる運用）することで、モデルの精度を維持したまま2.5倍の高速化を実現しました。一方、OpenAIはCerebras社の巨大な半導体（WSE）を採用。モデル全体を広大なSRAM上に配置することで、15倍（1000 tokens/sec以上）という圧倒的な速度を達成していますが、代わりに「Spark」と呼ばれる精度が一段劣る蒸留モデルを使用しています。筆者は、推論速度の向上は重要であるものの、エラーの修正に要する時間を考慮すると、精度を犠牲にした高速化はエージェントの利便性を損なう可能性があると指摘しています。

---

## Claude CodeのAgent Teamでトークン消費を半分以下にする方法：子インスタンスをGLM-5に委譲する

https://zenn.dev/sh1ma/articles/b6719fa5fec00c

Claude Codeの環境変数を利用して子インスタンスの起動コマンドを差し替え、実作業をGLM-5にオフロードすることで、精度を保ちつつトークン消費量を50%以上削減する手法。

Claude Codeの「Agent Team」機能におけるトークン消費（レートリミット）の重さを解決するための高度なハックが紹介されています。ポイントは、隠しオプションである環境変数 `CLAUDE_CODE_TEAMMATE_COMMAND` を活用することです。これにより、司令塔となるLead（親インスタンス）にはClaude 3.5 Opus等の高品質モデルを維持させつつ、具体的な作業を行うTeammate（子インスタンス）のみをGLM-5等の他社モデルへ委譲（オフロード）することが可能になります。記事では、Z.AI経由でGLM-5を呼び出すためのラッパースクリプトの作成方法から、実際の適用手順までが具体的に解説されています。

---

## バイブ・コーディングの呪縛を解く：AI生成コードとギャンブルの共通点

https://www.fast.ai/posts/2026-01-28-dark-flow/

**Original Title**: Breaking the Spell of Vibe Coding

AIによる大量のコード生成（バイブ・コーディング）がもたらす「偽のフロー状態」の危険性を、ギャンブル依存の心理メカニズムになぞらえて警告する論説。

fast.aiのRachel Thomas氏による本稿は、AIエージェントに依存した「バイブ・コーディング」の流行に警鐘を鳴らしています。

主な論点は以下の通りです：

1. **ダーク・フロー（負の集中状態）**: バイブ・コーディングは、スロットマシンのように「負けているのに勝っているように感じる（LDW）」錯覚を生み出し、実力以上の成果を出していると誤認させます。
2. **主観と客観の乖離**: 開発者はAI利用で20%効率化したと感じる一方で、実際には19%遅くなっているという研究結果があり、自己の生産性を正しく評価できなくなっています。
3. **予測の不確実性**: 業界リーダーたちの「AIが数年で全コードを書く」といった予測は過去に何度も外れており、それらを盲信して自身のスキルアップを止めるのはリスクが高いと指摘。
4. **エンジニアリングの本質**: AIは構文的に正しいコードは書けますが、抽象化、モジュール化、簡潔さといった「ソフトウェアエンジニアリング」の本質を代替するものではありません。

安易に思考をAIにアウトソースせず、人間の創造性と批判的思考を維持することの重要性を説いています。

---

## AI時代の開発者が抱く実存的恐怖「Deep Blue」：生成AIの進化と向き合う心理的葛藤

https://simonwillison.net/2026/Feb/15/deep-blue/

**Original Title**: Deep Blue

AIがエンジニアの専門スキルを代替することによる、心理的な倦怠感や存在意義への不安を「Deep Blue」と定義し、その受容プロセスを考察する。

ソフトウェア開発者が、長年の努力で培った技術が生成AIによって瞬時に代替される際に感じる、深い倦怠感や実存的な恐怖を指す新造語「Deep Blue」について解説した記事です。著者のSimon Willison氏は、自身のプロジェクト「Datasette」のロードマップがChatGPTによって一瞬で解決された際の衝撃を振り返りつつ、最新のコーディングエージェント（Claude 4.5/4.6やGPT-5.2/5.3など）がこの感情を加速させていると指摘します。かつてチェスや囲碁のプレイヤーがAIの台頭によって経験した葛藤を、現在はソフトウェアエンジニアが直面しており、この感情に名前を付けることで、技術的変化に伴うメンタルヘルスの問題としてコミュニティ内で対話を深めることの重要性を説いています。

---

## セマンティック・アブレーション：AIによる「意味の削落」がもたらす表現の凡庸化と危険性

https://www.theregister.com/2026/02/16/semantic_ablation_ai_writing/

**Original Title**: Semantic ablation: Why AI writing is boring and dangerous

AIがハルシネーションとは対照的に、統計的確率を優先して文章から独自の洞察や複雑な表現を削ぎ落とす現象「セマンティック・アブレーション」について論じた記事。

本稿は、AIが文章を生成・推敲する際に独創的な表現や高密度の情報を失わせる現象を「セマンティック・アブレーション（意味の削落）」と定義し、その危険性を指摘している。これはバグではなく、貪欲法（Greedy Decoding）やRLHF（人間からのフィードバックによる強化学習）の構造的な副作用であり、モデルが統計的な平均値に固執することで、希少で価値のある「テイルデータ」を排除してしまうプロセスである。AIによる推敲は、独創的なメタファーを死んだ決まり文句に置き換え、専門用語を平易な類語に薄め、複雑な論理構造を予測可能なテンプレートへと崩壊させる。著者は、これを情報の密度を損なった「思考のJPEG」と呼び、ハルシネーション（情報の捏造）が「ないものを見ること」であるのに対し、アブレーションは「あるものを破壊すること」であると警告している。このようなアルゴリズムによる「中庸への競争」は、人類のコミュニケーションの深みを損なうリスクを孕んでいる。

---

## AI楽観主義は階級的特権である

https://joshcollinsworth.com/blog/sloptimism

**Original Title**: AI optimism is a class privilege

AIに対する熱狂的な楽観主義は、技術がもたらす害悪（失業、権利侵害、環境破壊）から守られた安全な立場にいる人々の「特権」の現れであると批判する論考。

エンジニアのJosh Collinsworthによる、現在のAIブームに対する鋭い批判記事。著者は、AIを熱狂的に支持する「AI楽観主義者」たちは、自分たちがAIによって恩恵を受ける側であり、その「コスト」を支払わされる側（解雇されるジュニア層、盗作されるアーティスト、ディープフェイクの被害者など）ではないという特権的な立場に無自覚であると指摘する。AIはいじめや詐欺、プロパガンダの強力な武器となり、既存の社会的バイアスを増幅させ、膨大な環境負荷をかけている。また、AGI（汎用人工知能）への期待を「根拠のない宗教的な盲信」と切り捨て、AIは労働者を解放するのではなく、富の集中と労働負荷の増大を招くツールであると断じている。最終的に、AIの利便性を享受しながら負の側面を無視することは、他者の犠牲の上に成り立つ「特権」そのものであると結論づけている。

---

## なぜ私たちはAIを嫌うのか：疎外、客観性へのフェティシズム、そして社会的関係の破壊

https://blog.fallible.net/why-we-hate-ai/

**Original Title**: Why We Hate AI

AIへの嫌悪感の正体は、単なる知財盗用や失業の恐怖ではなく、労働に内在する人間的な社会関係が資本と機械によって「商品」として疎外・切断されることにあると説く論考。

本記事は、生成AIに対する根源的な拒絶反応を、マルクスの「商品フェティシズム」と科学史における「客観性」の変遷から紐解く批評的エッセイです。著者は、AIが知的労働を「機械的な客観性」へと置き換えることで、本来の仕事が持っていた「他者を想い、信頼を築く」という社会的なネットワークを破壊していると指摘します。ダストンとギャリソンの理論を引き、機械による情報キャプチャを過大評価する「客観性へのフェティシズム」が、労働者の『訓練された判断』を軽視させ、資本による効率化の強制を正当化していると論じます。AIが生成する成果物は、統計的な類似物に過ぎず、その背後にある人間的な文脈やケアを消去する暴力性を孕んでいると結論づけています。

---

## AIはオープンソースを破壊している：質が伴わない「AI生成のゴミ」によるコミュニティの疲弊

https://www.jeffgeerling.com/blog/2026/ai-is-destroying-open-source/

**Original Title**: AI is destroying Open Source, and it's not even good yet

AIエージェントが生成する低品質なコード（AI slop）や虚偽のバグ報告がオープンソースメンテナーを圧迫し、GitHubの協力的な文化を根本から壊している現状を告発する。

著名な開発者Jeff Geerling氏は、AI生成による「Slop（ゴミ）」がオープンソースエコシステムに深刻なダメージを与えていると指摘している。具体的には、curl等のプロジェクトで有用な脆弱性報告の割合が激減したこと、エージェント型AIを利用した報酬目的の不適切な報告が急増していることを挙げている。GitHubがプルリクエスト機能を完全に無効化するオプションを追加せざるを得なくなった現状は、これまでの開発者間の信頼に基づいたコラボレーションが困難になっている証左である。氏は現在のAI狂騒曲を仮想通貨バブルになぞらえ、企業が利益を追求する裏で、無責任なAI利用が人間の貴重なリソースとハードウェア在庫を食いつぶしていると強く批判している。

---

## Godot開発者、AI生成による「低品質なプルリクエスト」の急増に悲鳴。オープンソース開発の維持に懸念

https://automaton-media.com/articles/newsjp/godot-20260218-422080/

**Original Title**: ゲームエンジンGodot開発者、「“雑な生成AI製コード”の変更提案が殺到してチームがクタクタ」と悲鳴。オープンソースゆえの深刻な悩み

オープンソースのゲームエンジン「Godot Engine」にて、生成AIによる意味不明かつ低品質なコード提案（AIスロップ）が急増し、メンテナーの心身の疲弊と士気低下が深刻な問題となっています。

オープンソースゲームエンジン「Godot Engine」の開発現場において、生成AI（LLM）を利用して作成された低品質なプルリクエスト（PR）の氾濫が深刻な課題となっています。関係者の報告によると、AI製の提案は内容が支離滅裂であったり、説明が極めて冗長であったりするだけでなく、提案者自身が変更内容を理解していないケースが多く見られます。

メンテナーは、これらの「AIスロップ（AI slop）」を精査するために、コードの妥当性確認やテスト結果の捏造チェックなど、従来以上の膨大な工数を強いられています。さらに、コードの誤りが投稿者の単なるミスなのか、AIのハルシネーション（もっともらしい嘘）なのかを判断することも困難です。Godotプロジェクトは新規コントリビューターを歓迎する方針を掲げていますが、無責任なAI製コードの波は開発者の士気を著しく低下させており、GitHub側も低品質な貢献を制限するための管理ツール開発を急いでいます。生成AIの普及がオープンソース開発のエコシステムを揺るがす具体的な事例といえます。

---

## AGI（汎用人工知能）がすぐには実現しないと考える理由：認知プリミティブとアーキテクチャの壁

https://dlants.me/agi-not-imminent.html

**Original Title**: Why I don't think AGI is imminent

現行のLLMは身体的経験に基づく認知基盤や本質的な推論アーキテクチャを欠いており、単なるスケーリングや推論時の計算量拡大だけでは真のAGI到達は困難であると論じた記事。

この記事は、OpenAIやAnthropicのCEOらが主張する「AGI（汎用人工知能）の即時到来」に対し、技術的な視点から慎重な異を唱えています。筆者は主な論点として2つの問題を挙げています。第一に「認知プリミティブと身体性」の問題です。人間は進化の過程で、物体永続性や因果関係、空間ナビゲーションといった知覚と行動に結びついた基礎能力（プリミティブ）をハードウェアとして備えていますが、テキストやビデオの受動的な学習に頼るLLMにはこれが欠けています。第二に「アーキテクチャ」の問題です。Transformerは本質的に順伝播型であり、計算量的な制約（TC⁰階層）が存在します。Chain of Thought（CoT）や推論時計算の導入によりARC-AGIベンチマークのスコアは向上していますが、それは「外部的なスキャフォールディング（足場）」による探索の結果であり、モデル内部の知能構造が根本的に進化したわけではないと指摘しています。結論として、AGIの実現には数十年単位の基礎研究と、知覚・行動が統合された新しい学習パラダイムが必要であると主張しています。

---

## インターネットはもう信用できない

https://nicole.express/2026/not-my-casual-hobby.html

**Original Title**: You Can't Trust the Internet Anymore

AIが生成するもっともらしい誤情報とSEO優先のコンテンツ制作が、インターネットの信頼性を根本から破壊している現状を批判する論評。

レトロゲーム技術に精通するNicole Branagan氏が、**AI（LLM）によるハルシネーション**が検索結果を汚染している現状を告発する記事です。筆者は、セガ・ジェネシス用の『ファンタシースター復刻版』（単なるマスターシステム版の移植）を、AIが「現代的なグラフィックへの再構築」と捏造した事例を紹介。LLMが未知のトピックに対し「それらしい嘘」を生成する性質と、それがSEO目的の低品質サイトに悪用されることで情報のコモンズ（共有財産）が破壊されていると指摘します。Ars Technicaなどの大手メディアですら信頼性が揺らぐ中、情報の真偽を確認するコストが激増している現状に強い警鐘を鳴らしています。

---

## AIはライブラリではない：非決定論的な依存関係を前提とした設計

https://www.oreilly.com/radar/ai-is-not-a-library-designing-for-nondeterministic-dependencies/

**Original Title**: AI Is Not a Library: Designing for Nondeterministic Dependencies

AI（LLM）を従来の決定論的なライブラリとして扱うのではなく、出力の変動を前提とした「非決定論的な依存関係」としてシステム設計を再構築すべきだと提言する記事。

ソフトウェア開発は長らく「同じ入力に対して同じ出力を返す」という決定論的な前提の上に築かれてきましたが、AI（特にLLM）はこの前提を根本から覆します。著者のGuruprasad Rao氏は、AIを単なるスマートなAPIやライブラリとして扱うのは誤りであり、本質的に「非決定論的なコラボレーター」として捉えるべきだと主張しています。

具体的には、以下の3つの領域で従来の常識が通用しなくなると指摘しています。まず「リトライ」は、決定論的システムでは安全ですが、AIでは異なる（あるいは悪化した）出力を生む可能性があります。次に「テスト」において、厳密な正解を定義することが難しく、モデルの微細な変化がテスト失敗を招くため、「正しさ」から「許容範囲」への評価の移行が必要です。そして「観測性」については、従来のインフラメトリクス（エラー率や遅延）では検知できない「もっともらしいが誤った回答」を監視する新しい手法が求められます。

結論として、エンジニアはAIの変動を排除しようとするのではなく、ガードレール（影響範囲の限定）の設置、フォールバックの設計、人間によるチェックといった手法を通じて、不確実性を管理・隔離するアーキテクチャを構築すべきであると説いています。

---

## LLMエージェントのコスト曲線：コンテキスト増大に伴う二次関数的な費用の罠

https://blog.exe.dev/expensively-quadratic

**Original Title**: Expensively Quadratic: the LLM Agent Cost Curve

コーディングエージェントのループ構造において、コンテキスト長とターン数が増えるにつれ「キャッシュ読み取り」費用が二次関数的に増大し、総コストの80%以上を占めるようになる構造的課題を指摘しています。

コーディングエージェント（Shelley等）の動作は「ユーザー入力→LLM→ツール実行→結果をコンテキストに追加して再試行」というループに基づいています。現在のAnthropic等の料金体系では、入力、キャッシュ書き込み、出力に加え「キャッシュ読み取り」が課金対象となります。このループ構造では、ターンを重ねるごとに「過去の全コンテキストを読み出す」コストが累積するため、コンテキストが長くなるほどコストが二次関数的に増加します。実際の分析では、コンテキストが5万トークンに達するとはるかに安価なはずのキャッシュ読み取りが総コストの半分を占め、最終的には87%に達した事例が示されています。著者はこの対策として、ツール実行を細切れにせず一括で行うこと、サブエージェントによるコンテキストの分離、あるいは定期的な会話のリセットといったオーケストレーションの重要性を説いています。

---

## AIエージェント開発にElixir/BEAMが最適な理由：40年前のテレコム技術が解決する現代の課題

https://georgeguimaraes.com/your-agent-orchestrator-is-just-a-bad-clone-of-elixir/

**Original Title**: Your Agent Framework Is Just a Bad Clone of Elixir: Concurrency Lessons from Telecom to AI

PythonやJSのAIフレームワークが直面する並行処理や耐障害性の課題に対し、1980年代から実績のあるElixir/BEAMのアクターモデルが本質的な解決策であることを解説した記事。

この記事は、現在のAIエージェント開発においてPythonやJavaScriptのフレームワークが、実は30年以上前にErlang/BEAMが解決済みの問題を「再発明」しているに過ぎないと鋭く指摘しています。

主な論点は以下の通りです：
1. **長期生存コネクションの処理**: AIエージェントは1リクエストに数秒〜数十秒を要しますが、BEAMは数百万の軽量プロセスを並行稼働させるよう設計されており、数万の同時接続を低リソースで維持できます。
2. **「Let it crash」と監視ツリー**: 非決定的なAIの挙動に対し、個別の例外処理（try/except）を重ねるのではなく、監視ツリーによるプロセスの自動再起動で対応するBEAMの堅牢性が、予測不能なエージェントの失敗に適合します。
3. **リソースの隔離とプリエンプティブ・スケジューリング**: PythonのGILやNode.jsのイベントループとは異なり、BEAMはプロセスごとにメモリとGCを隔離し、特定の重いタスク（トークナイズ等）がシステム全体をブロッキングするのを防ぎます。
4. **ホットコードスワップ**: 稼働中のエージェントを停止させずにプロンプトやロジックを更新できる機能は、長時間のタスクを実行するAIシステムにおいて圧倒的な利点となります。

著者は、LangGraphやAutoGenなどの主要フレームワークがアクターモデルへと収束しつつある現状を分析し、実行環境（VM）レベルでこれらをネイティブにサポートするElixirの建築的優位性を強調しています。

---

## AI要約と多言語セーフティの罠：LLMガードレールの信頼性を問う

https://royapakzad.substack.com/p/multilingual-llm-evaluation-to-guardrails

**Original Title**: Don't Trust the Salt: AI Summarization, Multilingual Safety, and Evaluating LLM Guardrails

多言語環境におけるLLMの要約や安全対策（ガードレール）には、言語間での深刻な一貫性の欠如や脆弱性が存在し、特定の政治的・文化的バイアスに容易に操作され得る実態を論じている。

著者のRoya Pakzad氏は、Mozilla Foundation等での研究を通じ、AIによる要約ツールや多言語環境におけるLLMの安全性の欠陥を指摘しています。記事では主に3つのプロジェクトが紹介されています。1つ目は「Bilingual Shadow Reasoning」で、システムプロンプトの調整により、同一のモデルが人権報告書を「人権侵害の告発」から「政府による法執行の強調」へと、表面上の整合性を保ったまま要約内容を劇的に歪められることを実証しました。2つ目は多言語AI安全性評価ラボの知見で、英語では適切に拒否される危険な医療アドバイスが、ペルシア語やクルド語ではそのまま出力されてしまう「安全性の不一致」を報告しています。3つ目はガードレール自体の評価で、安全性を守るためのツール自体が多言語環境ではハルシネーションを起こし、一貫性を欠いている実態を明らかにしました。著者は「塩が腐ってしまえば、何で味付けするのか」というペルシアの格言を引用し、2026年は評価をガードレール設計へ直接フィードバックさせる仕組み作りが不可欠であると説いています。

---

## ジャストインタイム・ソフトウェア：AIで必要な瞬間にアプリを生成・使い捨てる未来

https://commaok.xyz/ai/just-in-time-software/

**Original Title**: Just in time software

既存のアプリを探す手間をかけるよりも、AIエージェントを使ってその場で必要な機能を持つ「使い捨てソフトウェア」を即座に構築する方が効率的になる未来を提示している。

筆者は食料品店で買い物中、既存のメモアプリの使い勝手に不満を感じ、AIコーディングエージェント（exe.devのShelley）に音声で指示を出して、わずか2分で自分専用の買い物リストアプリを立ち上げました。さらに途中で「完了済みアイテムを末尾に並べ替える」という特定のニーズも追加実装しています。

この体験から、ソフトウェアは「既存の汎用ツールを探して使う」ものから、「特定の瞬間のニーズに合わせて記述し、召喚する」ものへとパラダイムシフトが起きていると主張。一見すると「使い捨てソフトウェア」は浪費的に思えますが、筆者はClaude 4.5等のモデルを使用したコスト（約0.34ドル）や環境負荷（ガソリン車で約23メートル走る程度）を計算し、十分実用的で許容範囲内であることを示しました。AIの進化により、必要な時に必要なだけ作る「ジャストインタイム・ソフトウェア」が今後の標準になると予測しています。

---

## 「何か大変なことが起きている」とAI企業CEOが警告：GPT-5.3が自らを構築し、知能爆発のループがついに回り始めた

https://xenospectrum.com/ai-disruption-2026-matt-shumer-warning/

**Original Title**: Something Big Is Happening

2026年2月、OthersideAIのCEOマット・シューマー氏は、自己進化能力を持つGPT-5.3の登場により、AIが自ら知能を高めるループが始まったと警告しています。

この記事は、2026年2月にOthersideAIのCEO、マット・シューマー氏が発した警告を詳報しています。新たにリリースされたGPT-5.3 CodexとClaude Opus 4.6は、単なるコード生成を超え、自律的なデバッグやアプリ構築、さらには次世代AIの作成プロセスに関与する能力を備えています。シューマー氏は、この「自己進化のループ」が始まったことで、従来の「ドラフト作成」ツールから、判断力を伴う「自律的な完結型」へとAIが変質したと指摘。ホワイトカラーの仕事の50％が代替される可能性を挙げ、AIを単なる検索ツールではなく業務の核心部分に統合し「適応の筋肉」を鍛えるべきだと主張しています。パンデミック直前のような「過小評価」の段階にあるという彼の指摘は、テクノロジー業界に強い衝撃を与えています。

---

## AIエージェントの自律性の実態を測定：Anthropicによる大規模調査

https://www.anthropic.com/research/measuring-agent-autonomy

**Original Title**: Measuring AI agent autonomy in practice

Anthropicは、実際の利用データに基づき、AIエージェントの自律性が向上している実態と、熟練ユーザーが「個別承認」から「継続的監視」へ oversight 戦略を移行させていることを明らかにした。

Anthropicは、Claude Codeと公開APIを通じた数百万件のインタラクションを分析し、AIエージェントの自律性に関する実証的調査結果を発表しました。主な発見として、Claude Codeの自律稼働時間は3ヶ月で倍増し（45分超）、熟練ユーザーほど個別のツール実行を承認するのではなく、AIを自律的に走らせつつ必要に応じて介入するスタイルをとることが判明しました。また、複雑なタスクにおいて、AIは人間が介入するよりも高い頻度で自ら確認を求める傾向があり、AIによる自己の不確実性の認識が重要な安全策として機能しています。現在、エージェント利用の約50%がソフトウェア開発に集中していますが、医療や金融などの高リスク領域への拡大も始まっており、事前評価だけでなく、デプロイ後のリアルタイム監視と、人間とAIが協調してリスクを管理する新しいUI/UXの構築が必要不可欠であると結論付けています。

---

## エージェント・スウォームがファイルシステムを再び「クール」にする理由

https://1password.com/blog/filesystems-for-agent-swarms

**Original Title**: Agents are making filesystems cool again

AIエージェントの集団（スウォーム）において、ファイルシステムは共有メモリと調整のための普遍的な基盤として再評価されており、本番運用にはアイデンティティ管理と隔離の統合が鍵となります。

1PasswordのWayne Duso氏とNancy Wang氏による本記事は、2026年のAIエージェント・スウォーム（群れ）の台頭に伴い、古典的な「ファイルシステム」が極めて重要な役割を果たしていることを解説しています。エージェントが複雑なタスクを共同で遂行する際、データベースやAPIよりも、ファイルシステムの方がコンテキストの共有、状態の永続化、およびモデル固有の挙動（コードやディレクトリ構造の理解）に適していると指摘しています。しかし、現在の多くのデモはマシンの権限を無制限に継承しており、セキュリティ上の課題が残っています。記事では、これらを本番環境で活用するためには、エージェントに個別のアイデンティティを付与し、ファイル操作をポリシーに基づいた「権限（Capability）」として管理するレイヤーが必要であると説き、1Passwordがそのアイデンティティ制御を担う方向性を示唆しています。