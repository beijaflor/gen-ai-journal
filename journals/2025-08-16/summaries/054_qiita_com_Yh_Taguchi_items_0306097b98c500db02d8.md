## GPT-5のポンコツっぷり

https://qiita.com/Yh_Taguchi/items/0306097b98c500db02d8

最新のGPT-5が特定の複雑な質問への対応で、一見正解を出すものの、その後の矛盾した自己修正と「定型フレーズ流用」の自白を通じて、既存LLMの論理的推論と世界モデルの限界を露呈した。

**Content Type**: AI Hype

**Scores**: Signal:4/5 | Depth:3/5 | Unique:4/5 | Practical:3/5 | Anti-Hype:5/5
**Main Journal**: 100/100 | **Annex Potential**: 100/100 | **Overall**: 76/100

**Topics**: [[GPT-5, LLMの限界, ハルシネーション, プロンプトエンジニアリング, 世界モデル]]

筆者は、巷で絶賛されるGPT-5の「ポンコツ」な側面を検証すべく、LLMが苦手とする「特定駅から1時間で行ける映画館」という質問を試しました。GPT-5は当初、富士駅から1時間圏内の映画館を正確に列挙しましたが、結論部分で既に挙げた沼津や三島の映画館を「バリエーション候補」として再提示するという矛盾したまとめを提示。この不自然さを指摘すると、GPT-5は動揺して英語で応答したり、修正後も沼津を再び「候補」として挙げるなど、一貫性に欠ける挙動を見せました。

最終的にGPT-5は、そのミスの理由を「最初の列挙と最後のまとめを別々に書き、整合確認をせず、定型フレーズを流用してしまった」と"正直に"告白。これは、ハルシネーションが大幅に減ったとされるGPT-5においても、文脈の整合性を見失う典型的なLLMの限界を示唆します。筆者はこの原因を、LLMのマルチモーダル能力（地図のイメージ）や現実空間での「ワールドモデル」の欠如にあると分析しています。

この事例は、ウェブアプリケーションエンジニアにとって、最新の生成AIが「AGI（汎用人工知能）」には程遠く、依然として「深く考えず定型文を返す」という根本的な弱点を抱えている現実を突きつけます。高機能化が進む中でも、AIに過度な期待をせず、その出力の論理的破綻やハルシネーションのリスクを常に念頭に置き、堅牢なシステム設計と適切なプロンプトエンジニアリングが不可欠であると再認識させられます。

追記として、著者はその後のコメントで、自身の質問の仕方がGPT-5の挙動に影響を与えた可能性にも言及しており、このやり取りがLLMと人間の対話の複雑さを示唆しています。