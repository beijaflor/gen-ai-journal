## ChatGPT 5 はGPUだけで一般家庭約42万世帯分の電力を消費

https://gazlog.jp/entry/chatgpt-5-200k-gpu-power-consumption/

ChatGPT 5の運用には20万台以上のGPUが導入され、その莫大な電力消費量がAIインフラの持続可能性に疑問を投げかけます。

**Content Type**: Research & Analysis

**Scores**: Signal:4/5 | Depth:4/5 | Unique:4/5 | Practical:3/5 | Anti-Hype:4/5
**Main Journal**: 77/100 | **Annex Potential**: 78/100 | **Overall**: 76/100

**Topics**: [[AIインフラ, GPU電力消費, データセンター, LLM運用コスト, 持続可能性]]

OpenAIは最新の生成AI「ChatGPT 5」の運用に20万台以上のGPUを投入しており、その電力消費が極めて大規模であると報じられました。具体的には、H200やB200といった高性能GPUを仮に半々で利用した場合、GPU単体で約190MW、これは日本の一般家庭約42万世帯分の年間消費電力に匹敵します。さらに、CPUやメモリ、ネットワーク機器、冷却システム、電力変換損失なども含めたデータセンター全体の消費電力は、推定で500MW近くに達し、これは一般家庭110万世帯分、原子力発電所1基の定格出力の半分にも相当する膨大な量です。

この情報は、webアプリケーションエンジニアにとって、利用しているAIサービスの裏側にある巨大なインフラとコスト構造を理解する上で非常に重要です。AIモデルの高性能化は計算能力の向上と同時に、劇的な電力消費量の増加を意味します。NVIDIAの次世代GPU「Vera Rubin」は1基あたり1800Wに達すると予測されており、この傾向は今後も続くと見られます。

なぜこれが重要かというと、このような莫大なインフラ投資と運用コストは、AIサービスの提供価格や利用可能な機能、ひいてはAI全体の商業的な持続可能性に直接影響を与えるからです。私たちが構築するアプリケーションが利用するAIは、無尽蔵の資源の上で動いているわけではありません。今後のAI活用においては、モデルの効率性やインフラコストを考慮した設計がますます求められるようになるでしょう。この電力問題は、AIの進化が直面する現実的な壁であり、エンジニアが意識すべき新たな制約となります。