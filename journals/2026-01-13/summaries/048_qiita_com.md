## VSCodeとローカルLLM（gpt-oss120b）で和訳・RAGする

https://qiita.com/segfo/items/ed8343e036a9f973c77d

VSCodeとローカルLLMを連携させ、オンラインAIの回答拒否やトークン制限を回避しつつ技術文書を自動翻訳・分析する高度なローカルRAG環境を構築する。

**Content Type**: 📖 Tutorial & Guide
**Language**: ja

**Scores**: Signal:4/5 | Depth:3/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 54/100 | **Annex Potential**: 53/100 | **Overall**: 80/100

**Topics**: [[ローカルLLM, VSCode, RooCode, RAG, 技術文書翻訳]]

著者は、英語の技術ドキュメント読解における翻訳ツールの制限や、オンラインAIエージェント特有の「回答拒否（セーフティフィルター）」という課題を解決するため、ローカルLLMを活用した翻訳・RAG環境の構築方法を提示している。特に、セキュリティ関連の「Red Teaming」的な内容を含む文書は、ChatGPTなどの商用LLMでは安全ガイドラインに抵触して回答を拒否されるケースが多いが、ローカルLLMを用いることでこうした検閲を回避し、自由な研究・分析環境を確保できると説明している。

構築プロセスの核となるのは、Obsidian Web Clipperによるドキュメント収集と、Pythonによる独自の自動翻訳スクリプト（translate.py）の活用である。このスクリプトは、大規模なマークダウンファイルをセクション単位で見出しごとに分割して処理することで、文脈の破綻やトークン制限による精度低下を防ぐ工夫がなされている。翻訳エンジンには「gpt-oss-120b」などのOpenAI互換APIを持つローカルモデルを使用し、翻訳済みの日本語文書群をVSCodeの拡張機能「RooCode（旧Roo-Cline）」のAskモードに読み込ませることで、オフラインでのRAG（検索拡張生成）を実現している。

エンジニアにとっての重要性は、単なる翻訳の効率化に留まらず、機密性や外部制限に縛られない「完全なコントロール下にあるナレッジベース」を構築できる点にある。オンラインサービスのトークン制限やコスト、プライバシーポリシーを気にすることなく、膨大な技術文書を横断的に要約させたり、具体的なPoC（概念実証）コードの生成を行わせたりすることが可能になる。著者は、この環境を構築することで英語原典の読解コストを大幅に削減し、RooCode本来の強みであるコード実装やTODO管理をさらに高い次元で統合できると主張している。