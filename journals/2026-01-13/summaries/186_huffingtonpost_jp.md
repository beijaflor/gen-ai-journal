## ChatGPTに絶対に共有してはいけない5つの情報を、専門家が警告。すでにしてしまった場合の対処法も紹介

https://www.huffingtonpost.jp/entry/story_jp_693fbd8ee4b0775c5077e079

主張する：AIチャットボットへの入力データは「半公開情報」として扱い、機密情報の共有を避けるべきであることを。

**Content Type**: 🤝 AI Etiquette
**Language**: ja

**Scores**: Signal:4/5 | Depth:1/5 | Unique:2/5 | Practical:3/5 | Anti-Hype:4/5
**Main Journal**: 76/100 | **Annex Potential**: 73/100 | **Overall**: 56/100

**Topics**: [[プライバシー, データガバナンス, LLM学習, サイバーセキュリティ, プロンプトエンジニアリング]]

昨今、米国人の半数がChatGPTやGemini、ClaudeといったAIチャットボットを日常的に利用しているが、スタンフォード大学の「Human-Centered AI研究所（HAI）」による最新の研究は、主要AI開発企業6社（OpenAI、Google、Anthropic、Amazon、Meta、Microsoft）のすべてが「デフォルトでユーザーの会話をモデルの学習および改善に利用している」実態を明らかにした。本記事では、サイバーセキュリティの専門家たちが、AIとの対話において決して共有してはいけない5つの情報カテゴリを挙げ、その背後にある技術的・倫理的なリスクを詳説している。

専門家が警告する最大の理由は、大規模言語モデル（LLM）への入力が学習や微調整に使われることで、会話内容が直接的または間接的に将来の応答に影響を及ぼし、ユーザーの管理が及ばないデータセットの一部と化す点にある。特に共有を避けるべき情報として、1.氏名や住所などの個人を特定できる情報（PII）、2.法的保護のない私生活のデリケートな相談、3.HIPAA（医療保険の携行性と責任に関する法律）の対象外となることが多い医療情報、4.NDA（秘密保持契約）に抵触する職場の機密・専有情報、5.詐欺やソーシャルエンジニアリングに悪用されかねない金融情報、の5つを挙げている。エンジニアにとって特に重要なのは、履歴書のブラッシュアップやソースコードのデバッグ目的でアップロードされたファイルも学習対象となり得る点だ。一度学習データに取り込まれた情報は「取り戻すことがほぼ不可能」であり、機密情報の入力が規制違反や契約違反に直結するリスクがある。

また、AIが人間らしい対話を行うことでユーザーに「誤った安心感」を与え、静的な検索エンジンよりも多くの情報を打ち明けてしまう心理的な罠も指摘されている。本記事は、AIとの対話空間を「私的な日記」ではなく「半ば公開された空間」として扱うべきだと提唱する。具体的な自衛策として、チャット履歴の無効化や学習利用のオプトアウト設定に加え、プロンプト作成時に仮名や一般化した表現（例：「セント・メアリー病院の患者」ではなく「医療分野のクライアント」）を用いる手法を推奨している。最終的に、AIが高度なコミュニケーションを模倣するほど、それが「単なるデータ処理装置」であることを忘れやすくなるため、技術者とユーザー双方がプライバシー意識を高める「ガードレール」の必要性を強調している。