## MCP ツールのコンテキスト圧迫の問題とその解決策

https://azukiazusa.dev/blog/mcp-tool-context-overflow/

本記事は、Model Context Protocol (MCP) ツールの普及が引き起こすLLMのコンテキスト圧迫問題を解決するため、Progressive disclosureやコード実行による効率的なツール呼び出し手法を提案する。

**Content Type**: ⚙️ Tools
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:4/5
**Main Journal**: 80/100 | **Annex Potential**: 80/100 | **Overall**: 80/100

**Topics**: [[MCP, コンテキストエンジニアリング, Progressive disclosure, LLMのコンテキスト管理, AIエージェント]]

Model Context Protocol (MCP) が事実上の標準となるにつれ、その課題として、LLMのコンテキストウィンドウが多数のツール定義によって圧迫される問題が浮上している。LLMはタスク達成のために提供されるコンテキストに大きく依存し、不適切な情報や過剰な情報は「Context Rot」を引き起こし、モデルの推論能力を低下させることが知られている。現在のMCPクライアントは、通常、タスクに不必要なツール定義までシステムプロンプトに事前ロードするため、例えば4つのMCPサーバーを同時に利用した場合、55.7kトークンものコンテキストがツール定義に消費されるといった非効率が生じる。

この課題に対し、著者は複数の解決策を提示している。第一に「Progressive disclosure（段階的開示）」の概念だ。これはAnthropicによって提唱され、MCPクライアントがツールの定義を一度にすべてLLMに渡すのではなく、まず各ツールがいつ使用されるべきか判断できる最小限の情報（例: Claude SkillsのSkill.mdにおけるnameとdescriptionを100トークン以内に抑える）のみを提供し、必要に応じて残りの詳細ドキュメント（5,000トークン以内）を段階的に読み込ませる手法である。

第二に、MCPに直接ツールを公開するのではなく、TypeScriptなどのコードAPIを公開し、LLMがそのコードを呼び出すことでタスクを達成させる「MCPを使ったコード実行」アプローチが挙げられる。この方法により、LLMは必要なツールのみを選択的に呼び出し、中間結果をコンテキストに載せずにフィルタリングなどの処理を行ってから結果を受け取ることが可能となる。CloudflareのCode Modeは、AI SDKで定義したツールを`codemode`関数でラップし、TypeScript APIとして変換することで、LLMが呼び出すコードを記述できるようにする。また、Anthropicはファイルシステムを探索してツール定義を段階的に読み込む手法を提唱している。

さらに、関連するツール定義を検索するための単一の検索ツール（例: `search_tools`）をMCPツールとして提供する方法や、Sentry MCPサーバーの「エージェントモード」（`use_sentry`という単一ツールを通じて組み込みAIエージェントが自然言語リクエストを処理し、内部でツールを呼び出す）も紹介されている。これらのアプローチは、LLMに渡すコンテキスト量を削減し、より効率的で高性能なAIエージェントの構築に貢献すると著者は強調している。