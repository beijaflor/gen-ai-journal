## ライオン、独自LLMを内製へ　社内の“暗黙知”を学習、AWSが協力

https://www.itmedia.co.jp/aiplus/articles/2510/08/news109.html

ライオンは、AWSと協力して社内の研究開発データや暗黙知を学習させた独自の大規模言語モデル（LLM）の内製開発に着手したと発表しました。

**Content Type**: News & Announcements

**Scores**: Signal:4/5 | Depth:3/5 | Unique:3/5 | Practical:3/5 | Anti-Hype:4/5
**Main Journal**: 92/100 | **Annex Potential**: 93/100 | **Overall**: 68/100

**Topics**: [[自社LLM開発, タシット知識, 分散学習環境, Qwen 2.5-7B, AWS ParallelCluster]]

生活用品大手のライオンがAWSと協力し、社内の研究開発データや長年培われた暗黙知を学習させた独自LLMの内製開発に着手したことは、ウェブアプリケーションエンジニアにとって重要な先行事例となります。従来のAIツールでは専門知識が前提となる高度な質問や体系的な知識整理が必要な複雑な業務に対応が困難であったため、自社固有の課題解決に生成AIを深く組み込む具体的なアプローチを示しています。

このプロジェクトでは、オープンソースのQwen 2.5-7Bをベースモデルとし、数十年にわたる研究報告書、製品組成情報、品質評価データといった社内知見を中心に学習させています。特に注目すべきは、AWS ParallelClusterとNVIDIA Megatron-LMを組み合わせた分散学習環境の構築です。これにより、多数のGPUを効率的に連携させ、大量のデータを高速で処理できるパラレル実行を可能にしています。

この取り組みは、一般的な生成AIサービスだけではカバーしきれない、企業の特定のドメイン知識、特に明文化されていない「暗黙知」をAIモデルに反映させる手法として、非常に実用的です。自社データでモデルをファインチューニングし、専用の分散学習インフラを構築する具体的な技術スタックは、今後同様の企業内AIプロジェクトを推進するエンジニアにとって、実装のヒントとなり得ます。また、将来的にはプレゼンテーション形式の非構造化データへの対応や、経済産業省主導の国産モデル活用も視野に入れており、データ整備とモデル最適化における継続的な課題と可能性も示唆しています。