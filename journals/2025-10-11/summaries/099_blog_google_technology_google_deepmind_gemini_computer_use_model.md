## Introducing the Gemini 2.5 Computer Use model

https://blog.google/technology/google-deepmind/gemini-computer-use-model/

Google DeepMindは、AIエージェントがユーザーインターフェースと直接対話できるGemini 2.5 Computer Useモデルを発表し、API経由で提供を開始しました。

**Content Type**: News & Announcements

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 91/100 | **Annex Potential**: 87/100 | **Overall**: 88/100

**Topics**: [[AIエージェント, UI自動化, Gemini API, ソフトウェアテスト, Web開発ワークフロー]]

Google DeepMindは、AIエージェントがユーザーインターフェース（UI）と直接対話できる「Gemini 2.5 Computer Useモデル」のパブリックプレビューを開始しました。このモデルは、Gemini 2.5 Proの視覚理解と推論能力を基盤とし、ウェブアプリケーションエンジニアが抱える多くの課題を解決する可能性を秘めています。

このモデルの画期的な点は、AIが単なるAPI呼び出しを超え、人間のようにウェブページやモバイルアプリを操作できる点にあります。具体的には、画面のスクリーンショットと過去の操作履歴、ユーザーのリクエストを入力として分析し、「クリック」「入力」といったUIアクションを生成します。これにより、フォームへの入力、インタラクティブ要素の操作、ログインを伴うタスクの実行など、これまで自動化が困難だった多くのデジタルタスクが可能になります。

私たちウェブアプリケーションエンジニアにとって、これは非常に重要な意味を持ちます。まず、UIテストの劇的な効率化が期待できます。実際にGoogle社内では、Firebase Testing Agentなどに活用され、ソフトウェア開発の速度向上に貢献しているとのこと。次に、既存のウェブサービスに特化したAPIが存在しない場合でも、AIエージェントがUI経由で操作できるため、ワークフローの自動化やパーソナルアシスタントの構築が飛躍的に容易になります。これにより、より汎用性の高い、実用的なAIエージェントの開発が可能となります。Googleは、モデルに組み込まれた安全機能と、開発者向けのコントロール（高リスクアクションに対する確認要求など）も提供しており、責任あるAI開発を支援しています。今すぐAPIを通じてGoogle AI StudioやVertex AIで利用可能であり、PlaywrightやBrowserbaseとの連携も示されており、新たな開発パラダイムを切り開くでしょう。