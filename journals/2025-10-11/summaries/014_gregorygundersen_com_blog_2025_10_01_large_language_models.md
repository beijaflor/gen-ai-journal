## 大規模言語モデルの歴史

https://gregorygundersen.com/blog/2025/10/01/large-language-models/

大規模言語モデル（LLM）の学術的歴史を解き明かし、分散表現、再帰型ニューラルネットワークからアテンションメカニズム、Transformerアーキテクチャ、現代の事前学習・RLHFまで、その進化とスケールの重要性を強調する。

**Content Type**: Research & Analysis

**Scores**: Signal:5/5 | Depth:5/5 | Unique:4/5 | Practical:3/5 | Anti-Hype:5/5
**Main Journal**: 90/100 | **Annex Potential**: 90/100 | **Overall**: 88/100

**Topics**: [[LLMの歴史, Transformerアーキテクチャ, アテンションメカニズム, 分散表現, スケール則]]

大規模言語モデル（LLM）の40年以上にわたる学術的進化を紐解く本記事は、WebアプリケーションエンジニアがAIコーディングツールやLLM機能を最大限に活用するために不可欠な背景を提供する。初期の分散表現は、単語をベクトル化することで統計的NLPの次元の呪いを克服し、類似した単語に共通の特徴を与えることでモデルの汎化能力を高めた。これは、コード補完や意味検索の基盤となっている。

その後、可変長シーケンスを扱う再帰型ニューラルネットワーク（RNN）が登場したが、固定長コンテキストベクトルの制約から長距離依存性のモデリングに課題を抱えた。この課題を解決したのが「アテンションメカニズム」である。これは、デコーダーがエンコーダーの隠れ状態のどの部分に「注意を払うか」を動的に決定することで、文脈のボトルネックを解消した。アテンションは、複雑なコードの依存関係を理解し、大規模なプロジェクトで正確なコード提案を行う上で中核的な役割を果たす。

2017年に発表されたTransformerは、RNNを完全に排除し、アテンションメカニズムのみで構成される画期的なアーキテクチャだった。最大の利点は、アテンションが並列計算可能であることで、これによりモデルの訓練効率が飛躍的に向上し、現在のGPTシリーズのような大規模LLMの基礎を築いた。これは、Copilotのようなツールが高速かつ大規模に動作する理由そのものである。

訓練パラダイムでは、大量のラベルなしデータを用いた生成的事前学習が主流となり、汎用的な「基盤モデル」を構築後、タスク固有のデータで識別的ファインチューニングや人間からのフィードバックによる強化学習（RLHF）が行われるようになった。これにより、LLMは単なる次の単語予測を超え、人間の意図に沿った「役に立ち、正直で、無害な」出力を生成できるようになり、AIアシスタントの信頼性とユーザ体験が向上した。

記事が強調する「ビターレッスン」は、シンプルでスケーラブルな手法が、精巧なアルゴリズムや専門知識に勝るという教訓だ。数兆のパラメータを持つLLMが、単純な次の単語予測を通じて、数学の問題解決などの複雑なタスクで人間レベルの性能を発揮するのは、このスケール則の賜物である。この理解は、Webエンジニアが個別の最適化よりも、大規模モデルとデータドリブンなアプローチを優先すべき理由を明確にする。