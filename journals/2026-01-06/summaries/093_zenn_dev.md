## AttentionをMarkov連鎖として捉える

https://zenn.dev/horiyuki42/articles/535fcf5544bc3c

TransformerのAttention行列をマルコフ連鎖の確率遷移行列として再定義し、定常分布への収束特性を利用したモデル性能向上の可能性を提示する。

**Content Type**: 🔬 Research & Analysis
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:4/5 | Practical:2/5 | Anti-Hype:4/5
**Main Journal**: 52/100 | **Annex Potential**: 53/100 | **Overall**: 72/100

**Topics**: [[Transformer, Self-Attention, マルコフ連鎖, NeurIPS 2025, LLM内部構造]]

本記事は、Transformerの根幹を成すSelf-Attention行列を、統計学における「離散時間マルコフ連鎖」の遷移行列として解釈する理論的なアプローチを解説したものである。Webアプリケーションエンジニアにとっても、ブラックボックス化しがちなAttentionの挙動を数学的な裏付けを持って理解する助けとなる内容だ。

著者はまず、Attention行列の数学的性質に着目している。Softmax適用後のAttention行列は、すべての要素が非負であり、各行の合計が1になる「行確率行列」である。これはまさに、入力系列内の各トークン位置を「状態」と見なしたマルコフ連鎖における確率遷移行列の定義そのものである。この視点に立つと、Attentionを1回作用させる操作は状態を1ステップ遷移させることに相当し、操作を繰り返すことで最終的には「定常分布（固有値1の左固有ベクトル）」へと収束していく過程として捉え直すことができる。

なぜこの解釈が重要なのか。著者が引用するNeurIPS 2025採択論文（Attention (as Discrete-Time Markov) Chains）によれば、このマルコフ連鎖の「収束の速さ」を計る指標である「第二固有値（λ₂）」が鍵となる。この指標を用いることで、各Attention Headがどれだけ情報を拡散、あるいは集中させているかを定量化できる。筆者によれば、この収束特性に基づいたHeadの重み付け平均を行うことで、モデルの下流タスクにおける性能が向上することが実験的に示されているという。

エンジニアにとっての意義は、単なる精度向上手法の紹介に留まらない。Attentionという複雑な機構を、古典的かつ堅牢な数学モデルであるマルコフ連鎖として記述することで、モデルの内部表現の解析や、より効率的なアーキテクチャ設計への足がかりが得られる点にある。深層学習の「直感」を「数理」で整理し、エンジニアリングに応用しようとする著者の鋭い視点が示されている。具体的な実装にすぐ転用できる性質のものではないが、LLMの挙動を「情報の遷移確率」として解釈する視点は、モデルのデバッグや評価指標の策定において新たなインスピレーションを与えるだろう。