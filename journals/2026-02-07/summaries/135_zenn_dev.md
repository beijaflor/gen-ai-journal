## RAGの精度が出ない原因は『LLM』ではなかった話

https://zenn.dev/kazusa_nakagawa/articles/article11_rag

RAGシステムの回答品質低下の原因を検索とランキングの設計不備と特定し、実用的なコードレベルの改善策を提示する。

**Content Type**: 📖 チュートリアル・ガイド
**Language**: ja

**Scores**: Signal:4/5 | Depth:3/5 | Unique:3/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 80/100 | **Annex Potential**: 75/100 | **Overall**: 76/100

**Topics**: [[RAG, Supabase, 検索エンジニアリング, 日本語トークナイズ, ベクトル検索]]

本記事は、**RAG（検索拡張生成）**アシスタントの開発において、回答精度が向上しない真の原因をLLMの性能ではなく、**Retrieval（検索）**と**Ranking**の設計にあると分析し、その具体的な改善策を共有しています。**Notion**をデータソースとし、**Supabase (pgvector)**と**OpenAI API**を組み合わせた標準的な構成をベースに、実務で直面しがちな「低関連情報の混入」や「日本語検索の不備」といった落とし穴を技術的に解説しています。

主な改善ポイントとして、最高類似度が閾値未満の場合に**再検索ツール**を動的に有効化するロジックの導入、英数字に限定されていたキーワード抽出を**Unicode（ひらがな・カタカナ・漢字）**に対応させる日本語トークナイズの修正、そして「最新」などの時間的意図を検知して更新日時をランキングに反映させる**Recency Score**の実装を挙げています。

これらのチューニングにより、LLMへ渡すコンテキストの質を直接的に高める手法が具体コードと共に示されています。**RAG**を実装中で、回答が「ズレている」あるいは「内容が薄い」と感じている開発者が、プロンプト調整の前に着手すべき検索品質向上のための実践的なガイドです。