## GoogleのAIがイスラエル軍の標的特定を支援、内部指針に抵触か

https://news.ycombinator.com/item?id=46856384

**Original Title**: Google helped Israeli military contractor with AI, whistleblower alleges

告発文書に基づき、Googleが自社のAI倫理原則を実質的に回避し、イスラエル軍の標的識別精度向上を支援していた疑いを報じている。

**Content Type**: 📰 News & Announcements
**Language**: en

**Scores**: Signal:4/5 | Depth:2/5 | Unique:3/5 | Practical:1/5 | Anti-Hype:4/5
**Main Journal**: 78/100 | **Annex Potential**: 80/100 | **Overall**: 56/100

**Topics**: [[AI Ethics, Google Gemini, Military AI, Project Nimbus, Whistleblower]]

Washington Postの報道によると、Googleのクラウド部門がイスラエル軍の契約企業に対し、**Gemini**を用いてドローンや装甲車、兵士などの標的識別精度を向上させる技術支援を行っていたことが、内部文書から明らかになりました。元従業員によるSEC（米証券取引委員会）への告発では、同社が対外的に掲げる厳格な**AI倫理原則**が、本件に関しては「二重基準」で運用されていたと主張されています。

注目すべきは、Googleが2023年2月にAIポリシーを密かに更新し、民主主義政府の支援を名目に、武器や監視への技術提供を禁じる条項を削除していた点です。**Project Nimbus**を巡るこの動向は、Big Techの倫理指針と軍事利用の実態との乖離を浮き彫りにしています。AI開発の倫理的境界線や、企業のポリシー変更が社会実装に与える影響を注視するエンジニア、およびコンプライアンスに関わるリーダー層は必読です。