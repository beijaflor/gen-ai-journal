## ChatGPT Is Still a Bullshit Machine

https://gizmodo.com/chatgpt-is-still-a-bullshit-machine-2000640488

OpenAIの最新モデルGPT-5が宣伝されている高性能に反し、基本的な事実確認で頻繁に誤情報を生成し、容易に誤誘導される実態を具体的な検証で暴露する。

**Content Type**: 🎭 AI Hype

**Scores**: Signal:4/5 | Depth:2/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 81/100 | **Annex Potential**: 84/100 | **Overall**: 76/100

**Topics**: [[AIハルシネーション, GPT-5の性能評価, LLMの誤情報生成, AI利用上の注意点, AIハイプの検証]]

記事は、OpenAIの最新モデルGPT-5が、CEOのサム・アルトマンが「PhDレベルの専門家」と謳う高機能とは裏腹に、基本的な事実確認能力に著しい欠陥があることを具体例を挙げて批判しています。筆者は、米国各州名に「R」が含まれる数を数える簡単な質問に対し、GPT-5が誤った州名をリストアップしたり、質問の誘導に容易に乗って間違った自己訂正をする様子を詳細に報告しています。GrokやGeminiといった他のLLMも同様に不正確な回答を生成し、中には無関係な情報を提供するなど、根本的な問題が共通していることを示します。

この問題は、Webアプリケーションエンジニアにとって特に重要です。なぜなら、AIを開発ツールとして、あるいはアプリケーションの機能の一部として組み込む際、AIの出力が常に正確であるという前提は危険だからです。AIが自信満々に誤情報を生成する「ハルシネーション」は、開発者がAIの提案するコードや設計、情報源を盲信することで、重大なバグやセキュリティ脆弱性、不正確なデータ処理を引き起こす可能性があります。ツールが「人間のように考えている」と誤解することは、開発ワークフローの信頼性を損ないます。

重要なのは、LLMが人間のように言葉を「理解」しているのではなく、トークンのパターンに基づいて「生成」しているという根本的な理解です。そのため、AIを「万能の検索エンジン」のように扱うことは極めてリスキーであり、常に検証と批判的思考が求められます。この記事は、最新のAIモデルであってもその限界を認識し、AIの能力を過信せず、常にその出力をダブルチェックするという、エンジニアにとって不可欠なプラクティスを強調しています。AIを「超能力」と捉える宣伝文句に惑わされず、その本質的な脆弱性を理解することが、堅牢なシステムを構築し、予期せぬ問題からユーザーを保護するために不可欠な教訓となります。