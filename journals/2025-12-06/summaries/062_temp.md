## AI エージェント開発におけるルールベースと LLM の使い分け

https://creators-note.chatwork.com/entry/2025/12/03/070000

AIエージェント開発において、ハルシネーション対策とHuman in the Loop（HITL）の強制には、ルールベースの「決定論」とLLMの「非決定論」を組み合わせたアプローチが効果的であると提唱します。

**Content Type**: Tutorial & Guide
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:3/5 | Practical:5/5 | Anti-Hype:5/5
**Main Journal**: 86/100 | **Annex Potential**: 82/100 | **Overall**: 84/100

**Topics**: [[AIエージェント開発, ハルシネーション対策, Human in the Loop, ルールベースシステム, LLM活用]]

AIエージェント開発において、大規模言語モデル（LLM）のハルシネーション（幻覚）問題や、Human in the Loop（HITL）を無視してしまうといった課題は、プロンプトエンジニアリングのみで解決しようとするとメンテナンス性の低い複雑なプロンプトを生み出してしまいます。著者は、これらの課題に対し、ルールベースの「決定論」とLLMの「非決定論」を効果的に使い分ける「とんち」のようなアプローチを提案します。

例えば、画像から構造化データを抽出する際にLLMが本来存在しない電話番号などのフィールドを勝手に生成してしまうハルシネーションのケースでは、LLM処理後にスキーマバリデーションのようなルールベース処理を導入し、不正なフィールドを削除することで安定したデータ出力を実現します。これは、LLMの柔軟性を活かしつつ、決定論的なルールで信頼性を担保する具体的な方法です。

また、LLMが人間の確認を無視して処理を進めてしまうHITLの問題に対しては、高精度なモデルへのアップデートに加え、アプリケーション側で制御を強制することが重要だと指摘します。特に、エージェントのツール呼び出し後に必ずユーザーの承認を経るルートを設ける設計が有効であり、例えば`FunctionTool`の`require_confirmation`パラメータを使うといった具体的な実装例も提示されています。ただし、UI側での専用の確認フロー開発が必要となる点や、現時点での特定のLLMサービスにおけるサポート状況についても言及し、実装の現実的な課題も示しています。

著者は、これらのプラクティスを通じて、ソフトウェアエンジニアがAIエージェントの導入を諦めることなく、効率性と信頼性を両立させ、ユーザーに誠実な価値を届けるための知恵を発揮することの重要性を強調しています。