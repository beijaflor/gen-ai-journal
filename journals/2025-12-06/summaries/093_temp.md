## AIモデルがCounter-Strike構築に挑戦：各モデルの強みと限界

https://www.instantdb.com/essays/agents_building_counterstrike

**Original Title**: Codex, Opus, Gemini try to build Counter Strike

主要なAIモデル3種がCounter-Strikeの基本版を構築する課題に挑戦し、各モデルの得意分野と限界、そして現代のAI開発における課題を浮き彫りにした。

**Content Type**: ⚙️ Tools
**Language**: en

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:4/5
**Main Journal**: 84/100 | **Annex Potential**: 83/100 | **Overall**: 84/100

**Topics**: [[AIエージェント, 3Dゲーム開発, LLM性能比較, マルチプレイヤー実装, デバッグ戦略]]

最近の主要AIモデルであるGemini 3 Pro、Codex Max 5.1、Claude Opus 4.5が、3Dマルチプレイヤー版のCounter-Strike基本機能を構築する課題に挑戦しました。この実験は、各モデルの実際の開発能力と、高レベルの指示から複雑なアプリケーションを生成する際の強みと限界を浮き彫りにすることを目的としています。

開発プロセスは、フロントエンド（ゲームメカニクス、UI、サウンド）とバックエンド（マルチプレイヤー機能、永続性）の2つのカテゴリに分けられた計7つの連続したプロンプトで行われました。結果として、Claude Opus 4.5は、より魅力的で視覚的に豊かなマップ、キャラクター、武器の生成において、フロントエンドのタスクで特に優位性を示しました。一方、Gemini 3 Proは、マルチプレイヤー機能の追加やデータ永続性の実装といった論理的な変更を伴うバックエンドタスクで最高のパフォーマンスを発揮し、エラーの少なさとビルドステップを積極的に実行するデバッグアプローチが特徴的でした。Codex Max 5.1は、フロントエンドとバックエンドの両方で堅実な「中間」のパフォーマンスを見せ、他のモデルのような突出した強みはないものの、バランスの取れた能力を示しました。

特筆すべきは、ClaudeがReactの`useEffect`フックが複数回実行される問題や依存配列の誤りといった、人間がよく遭遇するような微妙なバグに直面し、最終的に人間のエンジニアの介入が必要となった点です。これは、AIが「Vibe Coding」（高レベルのアイデアをコードに変換する）から「本物のプログラミング」へと移行する際の課題の大きさを物語っています。著者は、これらの課題に対してReactのDXを改善することが、人間とAIエージェントの双方をさらに前進させる可能性があると指摘しています。

この実験は、AIモデルがCLIを介して高レベルのフィードバックに基づいて自身の作業を反復処理する能力が大幅に向上していることを示していますが、「コードを一切見ないで済む」という理想的な状態にはまだ到達していないことを指摘しています。エンジニアの視点からは、各AIモデルの特性（例えば、Claudeの視覚的デザイン能力、Geminiの論理処理と堅牢なデバッグ、Codexのバランスの取れたアプローチ）を理解し、AIツールを導入する際にどの領域で人間の専門知識が依然として不可欠であるかを判断するための重要な洞察を提供します。