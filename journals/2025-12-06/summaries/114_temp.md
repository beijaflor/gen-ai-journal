## オラ、ラマにコードレビューしてもらうわ #ollama

https://qiita.com/ussy_k/items/0f48fb0fb6cbe0aedb3c

著者は、Ollamaを使用してGoogle Java Style Guideを学習させたローカルLLMベースのコードレビュアーを構築し、外部接続が制限される環境でのコード品質向上と効率化の可能性を探求した。

**Content Type**: ⚙️ Tools
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:3/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 81/100 | **Annex Potential**: 77/100 | **Overall**: 80/100

**Topics**: [[Ollama, ローカルLLM, コードレビュー, プロンプトエンジニアリング, Javaスタイルガイド]]

株式会社システムアイの著者は、クローズドな環境下でのコードレビュー効率化を目指し、ローカル環境で大規模言語モデル（LLM）を動作させるOSSツール「Ollama」を活用したコードレビューシステムを構築した経緯を解説しています。外部へのデータ送信が制限される環境での作業効率化が課題となる中、Ollamaのプライバシー保護特性に着目し、具体的な導入プロセスと試行錯誤を通じてその可能性を探りました。

記事では、まずOllamaの概要とMac miniへのインストール手順を説明。その上で、Google Java Style GuideのHTMLドキュメントを読み込ませるための「Modelfile」を定義し、Ollama上にカスタムのJavaコードレビュアーモデル「java-reviewer-offline」を作成する具体的なステップを示しています。これは、Dockerfileのようにモデルの挙動を定義する重要な部分です。

著者は、Claudeが生成した「レビューしがいのある」Javaサンプルコードを用いて、作成したレビュアーモデルの性能を検証しました。最初のレビュー結果では、命名規則や例外処理、コードの組織化に関する指摘が得られましたが、特に命名規則の改善余地が大きいと評価。そこで、Modelfileを改良し、命名規則を含む詳細なチェック項目を明示的に記述することで、レビュー精度向上を図りました。しかし、この改良が意図せず、レビューの焦点がロジックやセキュリティに移り、本来見たかった命名規則への言及が薄れるという興味深い結果も示され、プロンプト設計の難しさと奥深さを浮き彫りにしています。

本記事の意義は、外部ネットワークに接続できない環境下でもAIによるコードレビューをローカルで実現できる可能性を具体的に示した点にあります。著者は、この試みを通じて「意外と使えそう」という感触を得ており、Docker経験者にはOllamaの概念が理解しやすいと指摘。また、既存のコード規約をAIに精度高く学習させるプロンプト設計の重要性、そしてそれがAI活用の共通課題であることを強調しています。これは、開発現場におけるAIツールの導入と、それに伴うプロンプトエンジニアリングスキルの必要性を示唆しており、将来のコードレビュープロセスを考える上で重要な示唆を与えています。