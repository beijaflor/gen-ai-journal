## ローカルRAGを構築したいあなたへ

https://blog.yakkomajuri.com/blog/local-rag

**Original Title**: So you wanna build a local RAG?

Skaldは、データプライバシーを重視する組織向けに、オープンソース技術を活用した完全にローカルなRAGシステムを構築し、その構成と実際のベンチマーク結果を公開して、ローカル環境でも実用的な性能が達成可能であることを示しています。

**Content Type**: ⚙️ Tools
**Language**: en

**Scores**: Signal:5/5 | Depth:5/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:5/5
**Main Journal**: 96/100 | **Annex Potential**: 93/100 | **Overall**: 96/100

**Topics**: [[ローカルRAG, オープンソースLLM, ベクトル埋め込み, RAGベンチマーク, データプライバシー]]

この記事では、Skaldがプライバシーに配慮した完全にローカルなRAG（Retrieval Augmented Generation）システムを構築した経緯と、その構成、さらには性能ベンチマークの結果を詳述しています。企業がフロンティアモデルの恩恵を受けつつもデータプライバシー要件を遵守できるよう、サードパーティにデータを送信しないRAGソリューションが求められている背景を説明しています。

筆者は、基本的なRAGシステムに必要なコンポーネント（ベクトルデータベース、ベクトル埋め込みモデル、LLM、リランカー、ドキュメント解析）を挙げ、それぞれのオープンソースの代替案を示しています。Skaldが実際に採用したローカルスタックは以下の通りです。

*   **ベクトルDB**: 既存のPostgresインフラを活用し、pgvectorを導入。数十万のドキュメントに対応可能と評価しています。
*   **ベクトル埋め込み**: デフォルトではSentence Transformersのall-MiniLM-L6-v2を使用（高速で汎用性が高いが英語のみ）。多言語対応のためbge-m3もテストしました。
*   **LLM**: Skaldではデフォルトモデルをバンドルせず、ユーザーが管理。筆者はllama.cppでGPT-OSS 20BをEC2インスタンス上で実行してテストしました。
*   **リランカー**: デフォルトはSentence Transformersのcross-encoder（英語のみ）。多言語対応のbge-reranker-v2-m3やmmarco-mMiniLMv2-L12-H384-v1も使用しました。
*   **ドキュメント解析**: Doclingをdocling-serve経由で利用。

性能検証では、PostHogウェブサイトのコンテンツをデータセットとして使用し、Skaldの実験機能を通じてRAGの精度を評価しました。比較対象として、Voyage AIの埋め込み・リランカーとClaude Sonnet 3.7を組み合わせたクラウドベースのセットアップをベースラインとしました。

ベンチマーク結果からは、クラウドベースのセットアップは高いスコアを記録した一方で、完全なローカルセットアップ（デフォルトのSentence TransformersモデルとGPT-OSS 20B）は、英語の「点」検索クエリには優れるものの、非英語クエリ、曖昧な質問、複数のドキュメントから情報を集約する質問（例：PostHogの資金調達ラウンドすべてを列挙する）において課題が見られました。

しかし、より性能の高い多言語対応モデル（bge-m3とmmarco-mMiniLMv2リランカー）をローカル環境で用いた場合、クラウドベースのセットアップに迫る平均スコア8.63を達成しました。特に多言語対応が改善され、完全な失敗は見られませんでしたが、依然として複数ドキュメントからの情報集約には課題が残ることを指摘しています。

筆者は、ローカルRAGは多くのユースケースで機能し、モデルとオープンソースオプションの進化により、今後さらに性能が向上すると結論付けています。Skaldは今後、ローカルデプロイ向けにこのセットアップをさらに洗練させ、オープンソースモデルに関するより厳密なベンチマークを公開していく意向を示しています。