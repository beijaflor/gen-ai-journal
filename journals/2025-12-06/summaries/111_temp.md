## ローカル LLM でコード補完エディタを作った話

https://qiita.com/Xudev/items/4b7df3f703c7e9da38e7

無料枠の制限に直面し、著者はローカルLLMのOllamaとNext.js、Monaco Editorを活用して、コストを気にせず利用できるパーソナルなコード補完エディタを自作した。

**Content Type**: Tutorial & Guide
**Language**: ja

**Scores**: Signal:4/5 | Depth:3/5 | Unique:3/5 | Practical:4/5 | Anti-Hype:3/5
**Main Journal**: 92/100 | **Annex Potential**: 88/100 | **Overall**: 68/100

**Topics**: [[ローカルLLM, Ollama, コード補完, Next.js, Monaco Editor]]

VS Codeのコード補完機能の無料枠利用制限に直面した著者は、利用回数を気にせず使えるローカルLLMの利点に着目し、個人用のコード補完エディタを構築した。これは、外部サービスへの依存を減らし、プライバシーを確保しながらAIによるコーディング支援を享受したいと考えるウェブアプリケーションエンジニアにとって重要な試みだ。

開発プロセスは以下のステップで進められた。まず、Windows環境（CPUのみ）にオープンソースのOllamaをインストールし、LLMモデル（当初はgpt-oss、後に軽量なgemma2:2bへ変更）をダウンロードしてローカル環境で実行できるようにした。次に、Ollamaが起動するローカルAPI (`localhost:11434`) をPythonから呼び出す方法を解説。ストリーミング形式で返されるレスポンスの処理方法についても具体的なコード例と共に示されている。

コード補完エディタのUI設計においては、VS Codeのようなリアルタイム予測表示の複雑さを避け、左右分割画面のNext.jsローカルアプリを採用。左側をエディタ、右側を予測補完パネルとし、ユーザーの入力が止まってから（デバウンス処理後）Ollama APIを呼び出すことで、ローカル環境での性能制約に対応した。Monaco Editorを組み込んだNext.jsのフロントエンドコードも公開されており、現在のコードと希望する言語をプロンプトとしてローカルLLMに渡し、補完コードを受け取る仕組みが実装されている。

著者は、このプロジェクトが「VS Codeの再発明」のような側面を持ちつつも、ローカルLLMの活用が非常に楽しい経験であったと結び、性能と精度のトレードオフを認識しつつも、パーソナルな開発ツールとしての可能性を示唆している。この取り組みは、開発者が自身のツールチェインをより深く制御し、クラウドサービスの制約から解放されるための実践的な一歩となる。