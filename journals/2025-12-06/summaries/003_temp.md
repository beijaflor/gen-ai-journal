## AGIを実感するのは難しい

https://tensorlabbet.com/2025/11/30/hard-to-feel-agi/

**Original Title**: It's Hard to Feel the AGI

主要な研究者たちが、既存のLLMやAIエージェントの根本的な限界を指摘し、AGI出現の時期やその実用性に関する見通しを修正している。

**Content Type**: AI Hype
**Language**: en

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:3/5 | Anti-Hype:5/5
**Main Journal**: 89/100 | **Annex Potential**: 90/100 | **Overall**: 84/100

**Topics**: [[LLMの限界, AIエージェント, AGI予測, AIの過度な期待, 次世代AI研究]]

AI業界の過熱した状況に対し、著名な研究者たちがAGI（汎用人工知能）やLLM（大規模言語モデル）ベースのAIエージェントに対する現実的な見方を提示し、これまで楽観的だった予測を修正し始めています。

OpenAIの元チーフサイエンティストであるイリヤ・サツケバー氏は、TransformerベースのLLMがスケール限界に達し、今後数年で停滞する可能性を指摘。評価での優れた性能と実際の経済的インパクトの低さとの間に顕著な乖離があるとし、この停滞を打破するには根本的に新しい研究が必要だと主張します。また、現在のLLMビジネスモデルの収益性にも疑問を呈し、人間のような学習能力を持つシステムの出現時期を5〜20年後方に修正しました。

アンドレイ・カルパシー氏は、AIエージェントに関する現在の業界の過度な期待に警鐘を鳴らしています。現状のAIエージェントは「認知的に欠けており、機能していない」と述べ、「エージェントの年」ではなく「エージェントの10年」が必要だと主張。経済成長への貢献は産業革命以来続く漸進的なパターンの一部であり、突然のGDP急増には繋がらないと見ています。自動運転技術の例を挙げ、その開発が予想以上に時間を要していることと比較し、期待を現実的なレベルに引き戻そうとしています。

リッチ・サットン氏は、LLMがAI研究の行き止まりであるという見解を示しています。LLMは内部に「世界モデル」を持たず、行動の結果を予測できないと指摘。単なる模倣学習であり、継続学習能力や明確な目標を持って行動する能力が根本的に欠けていると論じます。「ビッグワールド仮説」やモラベックのパラドックスを引用し、深層学習や勾配降下法の概念的限界を強調しています。

ヤン・ルカン氏は長年にわたり、LLMが人間レベルの知能にスケールするという考え方を批判しています。彼は言語は知能ではなく、低帯域で限定されたモダリティに過ぎないと主張。人間が物理世界を経験する高次元で連続的な表現とは異なり、LLMには物理世界の適切なメンタルモデルがないため、計画的な行動や新しい問題解決ができないと考えます。現在のLLMへの熱狂は、80年代のエキスパートシステムへの期待と失望に類似していると警告し、真に知的なシステムにはマルチモーダル入力からの常識習得、持続的記憶、推論・計画能力が必要だと述べています。

これらの知見は、LLMがテキスト生成、画像生成、ブレーンストーミング、要約などの特定のタスクで依然として大きな価値を提供する一方で、その自律性と人間レベルの知能を過大評価すべきではないというコンセンサスが広がりつつあることを示しています。投資家が「AIの冬」に陥り幻滅しないよう、技術の限界と現実的な応用範囲を明確にすることが重要です。