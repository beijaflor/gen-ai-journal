## 卓上サイズでVRAM 128GB搭載ミニPC、ASUS「ASCENT GX10」を遊び尽くしてみた

https://pc.watch.impress.co.jp/docs/column/nishikawa/2072125.html

BlackwellアーキテクチャのFP4演算と128GBの大容量メモリを活用し、超大型LLMの実行や高度なローカルAI生成ワークフローにおけるASCENT GX10の実用性を検証する。

**Content Type**: ⚙️ Tools
**Language**: ja

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 89/100 | **Annex Potential**: 86/100 | **Overall**: 88/100

**Topics**: [[ASUS ASCENT GX10, Blackwell FP4, ComfyUI Nunchaku, Local LLM, Unified Memory]]

本記事は、NVIDIA DGX Spark互換機であるASUSのミニPC「ASCENT GX10」を用い、128GBという巨大なユニファイドメモリを活かした生成AIの応用検証を詳細にレポートしている。特に、最新のBlackwellアーキテクチャがサポートするFP4（4ビット浮動小数点）演算の活用と、メモリ容量がボトルネックとなる超大型モデルのローカル実行に焦点を当てている。

筆者がまず注目したのは、ComfyUIでFP4を利用可能にするカスタムノード「Nunchaku」の導入である。Blackwell GPUを内包する本機では、演算フォーマットにFP4を採用することで、画像生成モデル「Qwen-Image」の生成時間を従来の17.08秒から7.89秒へと大幅に短縮できることを示した。これは、リソース制約の厳しいローカル環境において、精度を維持しつつ推論速度を劇的に向上させる技術として、開発者にとって極めて重要な検証結果である。

さらに、GeForce RTX 5090（32GB VRAM）との比較では、純粋な生成速度では劣るものの、VRAM容量の差が実用上の決定的な違いを生むことを強調している。例えば、1,200億パラメータを持つ「gpt-oss-120b」のような超大型LLMをVRAM上に完全にロードした状態を維持しながら、同時に画像生成AI（Qwen-ImageやZ-Image-Turbo）を並行動作させるワークフローが可能となる。これにより、「LLMでプロンプトを拡張し、そのまま画像生成へ渡す」といった高度なマルチモーダル処理を、モデルのロード・アンロードを繰り返すことなくシームレスに実行できる。

筆者は、本機が提供する128GBのユニファイドメモリの価値を、単なるスペック向上ではなく「用途の拡張性」にあると定義している。MacBook Pro（M4 Max）と比較して、画像生成速度やLLMの初回トークン生成速度（1st token）において本機が優位性を持ち、かつLoRA学習などを省VRAMオプションなしで実行できる柔軟性を高く評価した。結論として、特定の処理速度ではコンシューマー向けハイエンドGPUに譲る場面はあるものの、LLM、画像、動画、音声（T5Gemma-TTSなど）のすべてを一台で、かつ実用的な速度でこなせる「オールマイティなAI開発環境」としての有用性を著者は強く主張している。ウェブアプリケーションエンジニアにとっても、高価なクラウドAPIに頼らず、機密性を保ちながら巨大なモデルを用いたRAG（検索拡張生成）やエージェント開発をローカルで完結させるための有力な選択肢となるだろう。