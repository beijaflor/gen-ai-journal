## FunctionGemmaのファインチューニング・ガイド：エージェントの意思決定を最適化する手法

https://developers.googleblog.com/a-guide-to-fine-tuning-functiongemma/

**Original Title**: A Guide to Fine-Tuning FunctionGemma

特化型軽量モデル「FunctionGemma」を自社のビジネスルールや特定のツール選択ロジックに適合させるための、具体的なファインチューニング手法とノーコードツールを解説する。

**Content Type**: 📖 Tutorial & Guide
**Language**: en

**Scores**: Signal:5/5 | Depth:4/5 | Unique:3/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 88/100 | **Annex Potential**: 81/100 | **Overall**: 84/100

**Topics**: [FunctionGemma, ファインチューニング, ツール呼び出し, エージェントAI, 軽量LLM]

Google DeepMindが開発した、関数呼び出し（Function Calling）に特化した「FunctionGemma 3 270M」モデルの真価を引き出すための実践的なガイドである。筆者は、エージェントAIにおいて自然言語を実行可能なアクションに変換する「ツール呼び出し」能力の重要性を強調した上で、なぜ汎用モデルではなくファインチューニングが必要なのか、その理由を明確に示している。

著者によれば、ベースモデルは一般的な知識には強いものの、個別の企業のビジネスルールや内部ポリシーを理解していない。例えば、「旅行ポリシー」について尋ねられた際、ベースモデルはGoogle検索を提案する可能性があるが、企業エージェントとしては内部ナレッジベースを優先すべきである。このような「ツール選択の曖昧さ」の解消こそが、ファインチューニングの主な目的である。

記事では、Hugging FaceのTRLライブラリを用いた教師あり学習（SFT）の具体的なケーススタディを紹介している。ここでは「内部ナレッジ検索」と「Google検索」を適切に使い分けるルーティングロジックの学習に焦点を当てている。特に重要な指摘として、学習データの分布（シャッフル）の落とし穴に言及している。データがカテゴリごとにソートされた状態でシャッフルせずに学習させると、モデルが特定のツールに偏り、識別能力を失う「破滅的なパフォーマンス低下」を招くリスクを警告しており、エンジニアにとって極めて実践的なアドバイスとなっている。

さらに、プログラミング不要で学習プロセスを完結できる「FunctionGemma Tuning Lab」が紹介されている。これはHugging Face Spaces上で動作し、JSON形式のツール定義やCSV形式の学習データのアップロード、学習率の調整、リアルタイムの損失曲線（Loss Curve）の監視、学習前後の自動評価までをノーコードで提供する。270Mという超軽量モデルをターゲットにしているため、エッジデバイスでの高速動作と低コストな運用を両立しつつ、特定の業務ドメインに特化した高度なエージェントを構築できる点が最大のメリットである。

総じて、本書は単なるツールの紹介にとどまらず、エージェントの意思決定精度を高めるためのデータ準備の勘所から、最新のノーコード環境までを網羅しており、AIエージェントの実装に携わる開発者にとって即戦力となる情報を提供している。