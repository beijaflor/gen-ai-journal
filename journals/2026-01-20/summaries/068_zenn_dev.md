## 超小型日本語LLM LFM 2.5-JPと会話してみた

https://zenn.dev/kok1eeeee/articles/lfm25-local-llm-trial

1.2Bパラメータの超小型日本語LLM「LFM 2.5-JP」の性能を検証し、モバイルアプリやローカル環境への組み込み可能性を評価する。

**Content Type**: ⚙️ Tools
**Language**: ja

**Scores**: Signal:4/5 | Depth:3/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:3/5
**Main Journal**: 73/100 | **Annex Potential**: 75/100 | **Overall**: 72/100

**Topics**: [[LFM 2.5-JP, ローカルLLM, Liquid AI, エッジコンピューティング, mlx-lm]]

Liquid AIが2025年1月に発表した超小型LLMシリーズ「LFM 2.5」、その中でも日本語特化モデルである「LFM 2.5-JP」の試用と考察をまとめた記事。本記事は、1.2B（12億）という非常に小さいパラメータ数でありながら、高い日本語能力と驚異的な推論速度を持つこのモデルが、Webエンジニアや個人開発者にとってどのような可能性を秘めているかを論じている。

著者が最も注目しているのは、モデルサイズが量子化版で1GBを切り（約731MB）、スマートフォンや低スペックのミニPC（N100など）でも軽快に動作するという点だ。筆者はこの特性を活かし、従来のようなクラウドAPI経由ではなく、モバイルアプリや個人開発ツール内に「AIモデルそのものを内蔵」するアーキテクチャに期待を寄せている。これにより、API料金のコストを回避し、APIキーの管理も不要な「完全ローカルで完結するAI機能」をアプリに実装できる可能性があるからだ。

実際の試用検証では、M4 Pro搭載Mac上でmlx-lmを使用。基本的な技術解説や質問に対しては的確な応答が得られ、推論速度は非常に高速であった。一方で、複雑なやり取りや長い出力になると、特定のフレーズを無限に繰り返すといった挙動の不安定さも確認されており、生成の安定性にはまだ課題があることも率直に報告されている。

筆者は、本格的な商用アプリストアでの配布にはアプリ容量やプラットフォームの審査といったハードルがあることを認めつつも、個人利用やローカル環境での活用には十分な実用性があると見ている。特に、機密データを外部に送りたくないローカルRAG（検索拡張生成）への適用や、LangChainを活用したツール連携、関数呼び出し（Function Calling）の管理など、エッジAIとしての発展性に強い関心を示している。

結論として、本モデルはまだ実験的な段階ではあるものの、開発者が「AIを単なるAPIとして消費する側」から、ローカル環境で「モデルそのものの実装やデプロイと向き合う側」へシフトする有力なきっかけになり得ると主張している。Ollamaやllama.cppなどの主要な推論フレームワークへの対応も進んでおり、Webアプリケーションエンジニアが低コストかつ高速なAI機能を試作・実装する上での、現実的な選択肢として高く評価されている。