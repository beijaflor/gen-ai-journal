## LLMへの継続的なコンテキスト提供における最適解を議論

https://news.ycombinator.com/item?id=46626639

**Original Title**: Ask HN: What is the best way to provide continuous context to models?

LLMに大規模なコンテキストを効率的に与えるためのエージェントによる動的検索、KVキャッシュの最適化、再帰的なツール呼び出しなどの多様な手法を、Hacker Newsの技術者たちが議論する。

**Content Type**: 🛠️ Technical Reference
**Language**: en

**Scores**: Signal:4/5 | Depth:4/5 | Unique:3/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 81/100 | **Annex Potential**: 78/100 | **Overall**: 80/100

**Topics**: [[Context Window, Agentic RAG, KV Caching, Coding Assistants, Prompt Engineering]]

本記事は、Hacker Newsにおける「モデルに継続的なコンテキストを提供するための最善の方法は何か」という問いに対するエンジニアたちの議論をまとめたものである。主な焦点は、単純に長い履歴を送り続けるのではなく、いかに「必要な情報だけを動的に抽出・管理するか」という実装レベルのパターンにある。

多くの参加者が推奨しているのは、Claude Codeに見られる「エージェント型検索（Agentic Search）」だ。これはメインのエージェントとは別に、ファイルを探索するサブエージェント（Haikuのような高速・安価なモデル）を走らせ、grepやbashなどのUNIXツールを駆使して必要な断片のみを特定・報告させる手法である。これにより、メインモデルのコンテキスト窓が不要な情報で埋まることを防ぎつつ、高い精度で情報を取得できる。筆者の一人は、これを「bashはすべてを解決する」アプローチと呼び、単純なRAGよりも優れたパフォーマンスを発揮していると指摘している。

一方で、Cursorのようにコードをベクトルデータベース（Vector DB）にチャンク化して保存し、セマンティック検索でロードする手法も根強い。しかし、議論の中では「コンテキストのドリフト（変質）」を防ぐために、元のセッションログを「黄金の真実」として保持し、必要に応じてサブエージェントに詳細を回収させる手法など、より洗練された管理方法も提案されている。

技術的な深みとしては、API側の「KVキャッシュ（Key-Value Cache）」の挙動に基づいた最適化が挙げられている。頻繁に参照される不変のコンテキストを冒頭に配置し、変化する部分を末尾に置くことで、キャッシュヒット率を高め、コストとレイテンシを劇的に削減する設計思想が共有されている。また、コンテキストがいっぱいになった際の「圧縮（Compaction）」についても、単純な要約は重要な制約情報を失うリスクがあるため、TODOリストや状態の要約を維持しつつセッションを切り替える「ロールオーバー」手法など、実戦的なテクニックが紹介されている。

ウェブアプリケーションエンジニアにとっての重要性は、LLMのコンテキスト窓が拡大しても、依然として「トークン密度」と「ノイズ除去」が回答の質に直結するという点にある。検索、キャッシュ、エージェントによる動的なコンテキスト構成という3つの軸を組み合わせることが、現在の高度なAIコーディングツールの裏側にある共通の設計パターンであることが浮き彫りになっている。