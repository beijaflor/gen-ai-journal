## AI業界の内部関係者が「データ汚染」で反撃、AIモデルの無効化を狙う「Poison Fountain」プロジェクト始動

https://www.theregister.com/2026/01/11/industry_insiders_seek_to_poison/

**Original Title**: AI insiders seek to poison the data that feeds them

AI業界の内部告発者グループが、AIクローラーに意図的にバグを含んだコードを学習させることでモデルの品質を低下させる「Poison Fountain」プロジェクトを開始した。

**Content Type**: 📰 News & Announcements
**Language**: en

**Scores**: Signal:5/5 | Depth:3/5 | Unique:5/5 | Practical:2/5 | Anti-Hype:5/5
**Main Journal**: 54/100 | **Annex Potential**: 57/100 | **Overall**: 80/100

**Topics**: [[Data Poisoning, Model Sabotage, AI Ethics, LLM Training, Cyber Security]]

米大手IT企業の従業員を含むAI業界の内部関係者らが、AIモデルの学習プロセスを妨害し、その知的な完全性を損なわせることを目的としたプロジェクト「Poison Fountain」を立ち上げた。このプロジェクトは、ウェブサイト運営者に対して、AIクローラーが収集するデータの中に「汚染された（ポイズニング）」トレーニングデータを混入させるためのリンクを設置するよう呼びかけている。

この動きの背景には、現在のAI開発の在り方に対する強い危機感がある。プロジェクトの参加者は、AIの先駆者であるジェフリー・ヒントン氏の「機械知能は人類への脅威である」という主張に同意しており、規制による介入はもはや手遅れ、あるいは不十分であると考えている。匿名を条件に取材に応じた関係者によれば、技術が世界中に普及してしまった現在、対抗手段として残されているのは、モデルの「認知」を直接攻撃する「武器」としてのデータ汚染であるという。彼らは、自社の顧客がAIを用いて構築しているものに対して強い懸念を抱いていると述べている。

技術的な手法として、Poison Fountainが提供する汚染データには、一見正しく見えるが微妙な論理エラーやバグを含んだプログラミングコードが含まれている。これをLLM（大規模言語モデル）に学習させることで、モデルが出力するコードの品質を密かに低下させ、信頼性を損なわせることを狙っている。この試みは、アンソロピック（Anthropic）が発表した「データポイズニングは、わずか数個の悪意あるドキュメントでモデルの品質を劣化させられるため、以前考えられていたよりも実用的である」という研究結果に触発されたものだ。

エンジニアの視点から見れば、このニュースは「モデルの崩壊（Model Collapse）」やトレーニングデータの信頼性という、現代のAI開発が抱える根本的な脆弱性を浮き彫りにしている。内部関係者が自社の技術を「毒」で破壊しようとするこの過激なアプローチは、AI開発における倫理的対立が、単なる議論の域を超えて直接的な妨害工作へと発展している現状を示唆している。AIツールの生成物に依存する開発者にとって、トレーニングソースが意図的に汚染されるリスクは、今後システムの信頼性を担保する上で無視できない課題となるだろう。