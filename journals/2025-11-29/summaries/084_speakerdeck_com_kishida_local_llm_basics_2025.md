## ローカルLLM基礎知識 2025

https://speakerdeck.com/kishida/local-llm-basics-2025

本資料は、Transformerの基本からローカルLLMを個人PCで動かすためのハードウェア、主要モデル、フレームワーク、ファインチューニングまで、Webアプリケーションエンジニアが知るべき基礎知識を網羅的に解説します。

**Content Type**: 📖 Tutorial & Guide
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:3/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 85/100 | **Annex Potential**: 79/100 | **Overall**: 80/100

**Topics**: [[ローカルLLM, LLMアーキテクチャ, ハードウェア要件, LLMモデル比較, ファインチューニング]]

岸田直樹氏による「ローカルLLM基礎知識 2025」は、Webアプリケーションエンジニアが個人PCで大規模言語モデル（LLM）を動かすための包括的な知識を提供します。著者は、オフライン利用、データ管理、カスタマイズ性、学習機会といったローカルLLMの利点を強調し、これらの恩恵を享受するための実用的な情報が提示されています。

本資料は、LLMの基盤であるTransformerアーキテクチャから解説を開始し、アテンション機構と計算量に言及します。モデル実行に必要なメモリについては、16bit floatから4bit量子化への進化により、70億パラメータ（7B）モデルが14GBから3.5GBへと大幅に削減できることを解説。さらに、MoE（Mixture of Experts）構造が、必要な専門家モデルのみを呼び出すことでリソースを節約する仕組みも説明しています。

具体的なハードウェアとして、NVIDIA製GPU（RTX 5060 Tiなど）やApple Silicon（Mac Studioなど）が推奨され、サーバーサービス（Open Router、さくらのAI）も紹介。AIの処理速度は今後も向上するが、賢さは不確かであると現実的な見通しを示しています。

ローカルLLMのモデル選定では、Qwen3、Gemma 3、GPT-oss 20Bなどの「お手頃」モデルから、GLM 4.5 Air、Kimi K2といった「巨大」モデルまで、それぞれの日本語対応度と必要リソースが詳細に比較されます。チャット用途では8B以上、商用AIの代替としてはGPT-oss 20Bが推奨されており、日本語性能が高いGLM 4.5 Airが注目すべきモデルとして挙げられています。

実行フレームワークとしては、PyTorch、Hugging Face Transformers、軽量なC++エンジンであるllama.cpp、Apple Siliconに最適化されたMLX、ファインチューニングフレームワークのUnslothが紹介されています。実行環境はLM Studio（推奨）、Ollama、Dockerなどが挙げられ、それぞれの特性と注意点が説明されています。

最後に、LLMのカスタマイズ手法であるファインチューニングについて、CPT、SFT、RLHF、DPOといった種類と、データセットの準備、NVIDIA GPUが必要な実行環境（Google Colabが推奨）、モデル選定の方法が解説されています。著者は、ローカルLLMは小さい単機能用途が多くなると予測し、より大きなモデルでデータセットを作成し、小さなモデルをファインチューニングする戦略が、コストとレスポンスの両面で優位であると強調しています。