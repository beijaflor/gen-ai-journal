## vLLM+Structured Outputを使ったテキストのラベリング高速化

https://www.wantedly.com/companies/wantedly/post_articles/1021372

Wantedlyのデータサイエンティストが、ローカルLLMを用いたテキストラベリングをvLLMによる推論高速化とStructured Output活用によって効率化する実践事例を解説します。

**Content Type**: ⚙️ Tools
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:3/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 81/100 | **Annex Potential**: 77/100 | **Overall**: 80/100

**Topics**: [[ローカルLLM, テキストラベリング, vLLM, Structured Output, 推論高速化]]

ウォンテッドリーのデータサイエンティストが、自然文のテキストデータに対する効率的なラベリング手法として、ローカルLLMの活用事例を紹介しています。人間による手作業では負荷が高く、機械的な自動化も難しいテキストラベリングにおいて、LLMを使うことでプロンプトで基準を制御し、柔軟な対応が可能になる点が強調されています。特に、データセキュリティの確保、APIレートリミットの懸念解消、独自データセットでのファインチューニングの自由度といったメリットから、ローカルLLMの利用が推奨されています。

本記事の核心は、このローカルLLMによるラベリングを高速化し、かつ構造化された形式で出力するための具体的な技術です。推論の高速化にはvLLMライブラリが採用され、少量のコード変更で高いスループットを実現できることが示されています。これにより、大量のテキストデータに対しても実用的な速度でラベリングが可能になります。また、マルチラベルや階層構造を持つラベル付けに対応するため、Pydanticスキーマとoutlinesライブラリを用いたStructured Outputの活用方法が解説されています。これにより、事前に定義した構造に準拠した形式でLLMからラベルを取得できるため、データの利用効率が大幅に向上します。

実際の実験では、Qwen/Qwen2.5-7B-InstructモデルとLLM-jp Toxicity Dataset v2を用いて、テキストの有害性判定タスクで検証が行われました。結果として、Structured Outputを利用しつつ、vLLMを導入することで、outlinesのみを利用した場合と比較してウォールタイムで約6.5倍の推論高速化が達成されたことが報告されています。これは、データ量が多い場合でもローカルLLMとStructured Outputを組み合わせることで、実用的な速度で高度なテキストラベリングが可能になることを示しており、クラウドAPIの利用が難しい環境での有力な解決策を提示しています。著者は、この手法がユーザーへのより良い推薦システム構築に貢献し、多くのデータサイエンティストにとって参考になることを期待しています。