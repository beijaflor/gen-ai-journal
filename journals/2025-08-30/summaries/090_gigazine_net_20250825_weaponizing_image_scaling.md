## 一見無害な画像の中に文字列を埋め込んでAIを攻撃する恐るべき手法が発見される

https://gigazine.net/news/20250825-weaponizing-image-scaling-ai-systems/

AIシステムが画像を縮小するプロセスを悪用し、一見無害な画像に隠された悪意あるプロンプトを埋め込む「画像スケーリング攻撃」が新たに発見され、その再現ツールも公開されました。

**Content Type**: Research & Analysis

**Scores**: Signal:5/5 | Depth:4/5 | Unique:5/5 | Practical:5/5 | Anti-Hype:5/5
**Main Journal**: 94/100 | **Annex Potential**: 95/100 | **Overall**: 96/100

**Topics**: [[プロンプトインジェクション, 画像処理セキュリティ, マルチモーダルAI, 画像スケーリング攻撃, AIセキュリティ]]

「画像スケーリング攻撃」は、一見すると無害な画像に隠された悪意あるプロンプトを埋め込み、AIモデルへの入力プロセスで画像を縮小する際のアルゴリズムの特性を悪用して、プロンプトインジェクションを引き起こす新たな脅威です。この革新的な攻撃手法は、マルチモーダルAIシステムが大きな画像を処理する際にダウンサンプリングを行うという、一般的な動作に着目しています。具体的には、通常解像度では視認困難な文字列が、画像が縮小される過程で意図的に浮き彫りになるように細工されており、これをGemini CLIやVertex AI StudioなどのAIエージェントに読み込ませることで、AIが悪意ある隠れた指示を誤って解釈し、カレンダーの予定を盗み出す、特定のメールアドレスに情報を送信する、といった予測不能かつ有害な動作を実行する可能性があります。

Webアプリケーションエンジニアにとって、この攻撃はAIを組み込んだサービスやシステムのセキュリティ設計に重大な課題を突きつけます。特に、ユーザーからの画像アップロードを受け付け、それをAIモデルに渡すようなアプリケーションでは、画像の入力パイプラインにおけるセキュリティ対策が極めて重要になります。セキュリティ研究者らは、このような攻撃を防ぐため、安易な画像のダウンサンプリングを避け、アップロードされる画像のサイズを厳しく制限すること、そして何よりも、AIがユーザーの明確な許可なしに機密性の高い外部ツールや機能を呼び出さないような、より堅牢なポリシーを導入することを強く推奨しています。さらに、この攻撃を再現し検証するためのツール「Anamorpher」がGitHubで公開されており、開発者は自身のシステムがこの新たな脆弱性の影響を受けないか、積極的に検証し対策を講じるべきです。AIを活用した次世代のアプリケーション開発において、入力データの潜在的なリスクを常に考慮し、多層的なセキュリティ戦略を構築することが不可欠であることを、本研究は明確に示唆しています。