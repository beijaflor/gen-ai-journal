## Block unsafe prompts targeting your LLM endpoints with Firewall for AI

https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai/

Cloudflareが、LLMのエンドポイントを標的とする安全でないプロンプトをネットワークレベルでブロックする「Firewall for AI」の新機能を発表した。

**Content Type**: Tools

**Scores**: Signal:5/5 | Depth:4/5 | Unique:3/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 85/100 | **Annex Potential**: 80/100 | **Overall**: 84/100

**Topics**: [[LLMセキュリティ, プロンプトモデレーション, エッジセキュリティ, Llama Guard, Webアプリケーションファイアウォール]]

Cloudflareは、AIアプリケーション向けのセキュリティ強化として、「Firewall for AI」に不正なプロンプトのモデレーション機能を追加しました。これは、MetaのオープンソースモデルであるLlama Guardを直接統合することで、悪意のあるプロンプトがLLMエンドポイントに到達する前に、ネットワークのエッジでリアルタイムに検知しブロックすることを可能にします。

Webアプリケーションエンジニアにとって、LLMの導入はプロンプトインジェクション、機密データ漏洩（PII）、有害コンテンツの生成、さらにはモデル汚染といった新たなセキュリティリスクをもたらします。従来のアプリケーションセキュリティ対策だけでは対応しきれない、LLM特有の予測不可能な振る舞いが課題となります。このCloudflareの新機能は、OWASP Top 10 for LLMsに挙げられる多くのリスクに対処し、特にMicrosoftのTayボットの事例のように、ユーザー入力がモデルに到達する前に有害なプロンプトをブロックする点で極めて重要です。既存のアプリケーションコードやインフラに変更を加えることなく、CloudflareのネットワークレベルでOpenAIやGemini、あるいは自社モデルといった特定のモデルに依存しない一貫した保護が適用できる点は、複数モデルを運用する企業にとって大きな利点です。

Llama Guardの統合により、ヘイト、暴力、性的コンテンツ、犯罪計画、自傷行為など、多岐にわたる安全カテゴリでプロンプトを分析します。検知された脅威は即座にブロックされるか、詳細な分析のためにログ記録されることで、迅速な対応とモデルのチューニングに役立ちます。技術的には、CloudflareのWorker AI上で高性能GPUを活用し、非同期アーキテクチャによって、検出機能を追加しても目立った遅延なしに大規模なリクエスト処理を可能にしています。さらに、ボット管理やレートリミットといった既存のCloudflareセキュリティ機能と組み合わせることで、LLMアプリケーションに対する多層的な防御を構築できます。これにより、開発者はLLMを活用したアプリケーションを迅速にデプロイしつつ、堅牢なセキュリティ体制を維持し、ユーザーの信頼を保護することが可能になります。今後のロードマップには、プロンプトインジェクションやジェイルブレイクの検出強化、モデル応答のハンドリング機能追加も含まれており、より包括的なAIセキュリティ対策が期待されます。