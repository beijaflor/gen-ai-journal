## LLMは問題の難易度を符号化する

https://arxiv.org/abs/2510.18147

**Original Title**: LLMs Encode How Difficult Problems Are

大規模言語モデル（LLM）が問題の難易度を内部的にどのように符号化しているか、そしてそれが人間の判断や強化学習による汎化とどのように関連するかを調査する。

**Content Type**: Research & Analysis
**Language**: en

**Scores**: Signal:5/5 | Depth:5/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 93/100 | **Annex Potential**: 92/100 | **Overall**: 92/100

**Topics**: [[LLM内部表現, 問題難易度, 強化学習, 脱幻覚化, モデルチューニング]]

大規模言語モデル（LLM）は複雑な問題を解きつつも、なぜ一見シンプルな問題で頻繁に失敗するのか、という矛盾を抱えています。本研究は、LLMが内部的に問題の難易度を人間の判断と一致する形で符号化しているか、そしてこの表現が強化学習（RL）後の汎化にどのように影響するかを調査しました。

研究者らは、60のモデルに対してEasy2HardBenchの数学およびコーディングサブセットを用い、線形プローブを層とトークン位置にわたって訓練しました。その結果、人間がラベル付けした難易度は強く線形的にデコード可能（AMC: ρ ≈ 0.88）であり、モデルサイズに応じた明確なスケーリングが見られました。一方、LLMから導かれる難易度表現は著しく弱く、スケーリングも不十分でした。

興味深いことに、「より簡単な」表現へとモデルを誘導すると、幻覚（hallucination）が減少し、精度が向上することが明らかになりました。さらに、Qwen2.5-Math-1.5BでのGRPOトレーニング中、人間の難易度プローブは強化され、テスト精度と正の相関を示しました。対照的に、LLM由来の難易度プローブは劣化し、モデルのパフォーマンスと負の相関を示しました。

これらの結果は、人間によるアノテーションが強化学習によって増幅される安定した難易度シグナルを提供する一方で、モデルのパフォーマンスから派生する自動化された難易度推定は、モデルが改善するにつれて不整合になることを示唆しています。これは、開発者がLLMのファインチューニングを行う際に、人間のフィードバックに基づく難易度情報が、モデルの汎化能力を向上させる上で極めて重要であることを意味します。特に、幻覚を抑制し、LLMの予測精度を高めるためのモデル誘導戦略において、この知見は実践的な価値を持つでしょう。