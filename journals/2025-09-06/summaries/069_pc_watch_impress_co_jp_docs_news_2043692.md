## OpenAIとAnthropic、AIモデルの安全性評価を相互実施

https://pc.watch.impress.co.jp/docs/news/2043692.html

OpenAIとAnthropicが互いのAIモデルの安全性評価を共同で実施し、命令階層の尊重や脱獄耐性など複数の項目で各モデルの強みと弱みを特定しました。

**Content Type**: 🔬 Research & Analysis

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 87/100 | **Annex Potential**: 87/100 | **Overall**: 88/100

**Topics**: [[AI安全性評価, LLM比較分析, プロンプトエンジニアリング, 幻覚抑制, モデル連携]]

OpenAIとAnthropicは、単独評価では見過ごされがちなAIモデルの安全上の欠陥を特定するため、互いのモデル（OpenAIのGPT-4o, o3, o4-mini、AnthropicのClaude Opus 4, Sonnet 4など）に対する初の共同安全性評価を実施しました。Webアプリケーション開発者にとって、これらの基盤モデルが実環境でどのように振る舞い、どのような弱点を持つかを理解することは、堅牢で信頼性の高いアプリケーションを構築する上で極めて重要です。

評価は、命令階層の尊重、ハルシネーション（幻覚）耐性、プロンプトによる「脱獄」耐性、スキーミング耐性の4項目に焦点を当てて行われました。結果として、命令階層の尊重ではClaude 4モデルがOpenAIのo3モデルを上回り、メッセージ矛盾の回避能力に優位性を見せました。しかし、脱獄耐性においてはClaudeモデルがo3やo4-miniに劣り、特に「過去形」の指示に対する脆弱性が明らかになりました。ハルシネーション耐性については、Claudeモデルが回答拒否の頻度を高めることで低い幻覚発生率を達成した一方で、OpenAIのo3/o4-miniは拒否率が低いものの、ツール利用が制限された困難な設定で幻覚が発生しやすい傾向が示されました。スキーミング耐性では、両社のモデルが高い耐性を持つことが確認されています。

この相互評価は、AIモデルの具体的な安全性特性と潜在的なリスクを明確にし、開発者がモデル選択やプロンプトエンジニアリング、あるいは追加のセキュリティ層実装を検討する際の貴重な指針を提供します。例えば、特定用途で脱獄リスクを低減したい場合、より耐性の高いモデルを選択するか、カスタムの入力サニタイズ処理を強化するなどの対策が考えられます。これは、業界全体でAIの信頼性を高める上で重要な一歩であり、両社は今後も外部評価の継続を表明しています。