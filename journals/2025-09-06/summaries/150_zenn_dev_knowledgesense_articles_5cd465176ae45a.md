## RAGで「無関係な」文書をいれると性能が向上する理由を解明

https://zenn.dev/knowledgesense/articles/5cd465176ae45a

この論文は、RAGにおける無関係な文書がLLMの性能を向上させるメカニズムを解明し、その知見を基にハルシネーションを抑制する「LFD」手法を提案します。

**Content Type**: Research & Analysis

**Scores**: Signal:4/5 | Depth:4/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 83/100 | **Annex Potential**: 84/100 | **Overall**: 84/100

**Topics**: [[RAG, LLM, Hallucination, Transformer Architecture, LFD]]

RAG (Retrieval-Augmented Generation) はLLMに外部知識を与える強力な手法ですが、正確な情報を検索してもLLMが自身の内部知識と混同し、ハルシネーションを起こす課題が開発現場で散見されます。本記事は、Transformerモデルの内部構造（浅い層、中間層、深い層）を分析し、このハルシネーションの原因と、以前報告された「無関係な文書がRAG性能を向上させる」という一見突飛な現象の理由を解き明かします。

分析によると、Transformerの中間層は入力から重要な文章を特定する役割を担いますが、深い層でLLMが持つ内部知識と検索結果が混同されることでハルシネーションが発生します。驚くべきことに、無関係な文書を含めることで、LLMは中間層で「正しい文章」への注意をより強く向けるようになり、結果として深い層での知識混同が減り、ハルシネーションが抑制されることが判明しました。

この知見に基づき、本記事は「LFD (Layer Feature Distillation)」という新しい手法を提案します。LFDは、無関係な文書を実際に与えることなく、LLM内部で外部知識を利用している層の出力を明示的に抽出し統合することで、同等の性能向上を実現します。評価では、多くのモデルとデータセットでLFDが高い精度を示すことが確認されました。

この手法は、特にローカルLLMとRAGを組み合わせる開発者にとって非常に重要です。API経由のLLMでは中間層へのアクセスが難しいかもしれませんが、自社でLLMを運用する環境では、LFDの考え方を活用してRAGの精度を大幅に改善できる可能性があります。必要な情報を提供しているにもかかわらずLLMが正しい回答を生成しないというRAG開発における典型的な課題に対し、具体的な解決策と深い洞察を提供し、ハルシネーション対策の新たなアプローチを提示します。