## プロンプトキャッシング：10倍安価なLLMトークン、その仕組みとは？

https://ngrok.com/blog/prompt-caching/

**Original Title**: Prompt caching: 10x cheaper LLM tokens, but how?

LLMプロバイダーが提供するプロンプトキャッシング（KVキャッシング）の技術的仕組みを詳細に解説し、その結果としてトークンコストが10倍安くなり、応答速度が最大85%向上する理由を解き明かす。

**Content Type**: Technical Reference
**Language**: en

**Scores**: Signal:5/5 | Depth:5/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 92/100 | **Annex Potential**: 90/100 | **Overall**: 92/100

**Topics**: [[プロンプトキャッシング, LLMアーキテクチャ, アテンションメカニズム, KVキャッシング, トークンコスト最適化]]

この記事は、OpenAIやAnthropicが提供するプロンプトキャッシング機能によって、LLMのトークン料金が最大10倍安くなり、応答速度が最大85%向上する技術的なメカニズムを、行列計算レベルまで掘り下げて解説している。著者は、ベンダーのドキュメントでは明示されない「具体的に何をキャッシュしているのか」という疑問に答えることを目的としている。

LLMは、入力されたテキストを数値に変換し、それを処理して出力する巨大な数学関数として機能する。そのプロセスは大きく「トークナイザー」「埋め込み」「トランスフォーマー（アテンションとフィードフォワード）」「出力」の4段階に分けられる。
まず「トークナイザー」がプロンプトを単語やサブワード単位に分解し、それぞれに一意の整数ID（トークン）を割り当てる。次に「埋め込み」ステージで、これらのトークンが多次元空間内の位置を表すベクトル（埋め込み）に変換され、そのトークンの意味とプロンプト内での位置がエンコードされる。

最も重要な部分である「トランスフォーマー」ステージでは、アテンションメカニズムが機能する。これは、プロンプト内の各トークンが次のトークン生成にどの程度「注意を払う」べきかを決定するものだ。このプロセスでは、入力された埋め込みからWQ、WK、WVという学習済み行列を用いてQ（クエリ）、K（キー）、V（バリュー）の行列が生成され、これらの行列積によってアテンションウェイトが計算される。このウェイトに基づいてトークンの埋め込みが結合され、新しい埋め込みが生成される。

LLMの推論ループでは、新しいトークンが生成されるたびに、それまでの全てのトークンと生成された新しいトークンを含むプロンプト全体が再処理される。しかし、注目すべきは、以前のトークンに対するアテンション計算、特にK行列とV行列の値が、新しいトークンが追加されても変化しない点である。プロンプトキャッシング（KVキャッシング）は、このKとVの行列をキャッシュすることで、後続の推論ステップや、同じプレフィックスを持つ異なるリクエストがあった場合に、これらの冗長な計算を回避する。これにより、大幅なコスト削減とレイテンシーの短縮が可能となる。

OpenAIはプロンプトキャッシングを自動で処理するが、Anthropicはより詳細な制御をユーザーに提供する。また、温度やtop_pなどのランダム性制御パラメータは、アテンションメカニズムの後に適用されるため、キャッシングに影響を与えない。著者は、この技術的な理解が、LLMのコストとパフォーマンス最適化において開発者にとって非常に重要であると結んでいる。