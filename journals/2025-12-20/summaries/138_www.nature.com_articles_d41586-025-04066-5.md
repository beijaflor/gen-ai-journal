## 査読にAIを使用する研究者が半数以上、多くはガイドラインに反して

https://www.nature.com/articles/d41586-025-04066-5

**Original Title**: More than half of researchers now use AI for peer review — often against guidance

学術論文の査読において半数以上の研究者がAIツールを使用しており、その多くが出版社のガイドラインに反している現状が明らかになりました。

**Content Type**: News & Announcements
**Language**: en

**Scores**: Signal:5/5 | Depth:2/5 | Unique:3/5 | Practical:3/5 | Anti-Hype:4/5
**Main Journal**: 98/100 | **Annex Potential**: 95/100 | **Overall**: 68/100

**Topics**: [[AI倫理, 査読プロセス, 大規模言語モデル, 開発者向けAIツール, AIガイドライン]]

「Nature」の記事によると、出版社のFrontiersが111カ国の約1,600人の研究者を対象に実施した調査で、半数以上の研究者が学術論文の査読プロセスにAIツールを使用していることが判明しました。このうち約4分の1が過去1年でAIの使用を増やしており、これはChatGPTのような大規模言語モデル（LLM）の普及を反映した現実です。

重要なのは、多くの出版社が未発表の原稿をサードパーティのAIツールにアップロードすることを機密保持や知的財産の懸念から禁じているにもかかわらず、研究者がガイドラインに反してAIを使用している点です。Frontiersの研究倫理ディレクターであるElena Vicario氏は、「査読作業でAIが使われている現実と向き合うことは良いことだ」と述べつつも、多くの外部勧告に反する形での使用が課題だと指摘しています。ウェブアプリケーションエンジニアの視点からは、この状況は、機密性の高いコードや顧客データをAIツールで処理する際のデータガバナンスや知的財産保護における同様の課題を浮き彫りにします。

具体的に、AIを使用する回答者の59%が査読レポートの作成に、29%が原稿の要約、ギャップ特定、参考文献チェックに、28%が盗用や画像重複などの不正行為の兆候を検出するためにAIを利用していると報告されています。

このような現状を受け、Frontiersを含む一部の出版社は、AIの使用を限定的に許可し、開示を義務付けていますが、未発表原稿のアップロードは引き続き禁止しています。Frontiersは、この「新しい現実」に対応するため、社内AIプラットフォームを立ち上げ、明確なガイドライン、人間の責任、適切なトレーニングを伴うAIの責任ある利用を提唱しています。

また、ヒューストン大学のMim Rahimi氏が行った実験では、GPT-5が査読レポートの構造を模倣し、洗練された言語を使用できる一方で、建設的なフィードバックを生成できず、事実誤認を犯すことが示されました。これは、AIツールの能力にはまだ限界があることを示唆しており、ウェブアプリケーションエンジニアがAIを開発やコード生成に利用する際にも、その出力の信頼性や深度について同様の注意が必要であることを示唆しています。この動向は、AIの導入が専門職にもたらす倫理的・実務的課題と、それに対応するためのポリシー策定の必要性を浮き彫りにします。