## RAGの「リランキング」を10倍速くする「MixLM」

https://zenn.dev/knowledgesense/articles/4eb785fd0e9a2b

LinkedInの研究チームが開発した「MixLM」は、RAGにおけるリランキング処理の速度を事前ベクトル化によって10倍以上高速化し、エンタープライズRAGの応答性能を飛躍的に向上させます。

**Content Type**: 🛠️ Technical Reference
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:5/5
**Main Journal**: 87/100 | **Annex Potential**: 87/100 | **Overall**: 88/100

**Topics**: [[RAG, リランキング, MixLM, LLM最適化, 検索システム]]

この記事は、RAGにおける応答速度の課題、特にリランキング処理の遅さを解決する新しい手法「MixLM」について解説しています。LinkedInの研究チームが2025年11月に発表したこの手法は、RAGシステムで必須とされるリランキングの速度を、精度を維持しつつ10倍以上高速化することを目的としています。

従来のRAGでは、ベクトル検索で取得した多数の文書（チャンク）を、ユーザーの質問との関連性に基づいて並べ替える「リランキング」が精度向上のために不可欠です。しかし、著者はこのリランキングが、特にチャンクが長く数が多い場合に、数秒単位の遅延を引き起こすことがあり、リアルタイム性が求められるプロダクトでの利用において致命的なボトルネックとなることを指摘しています。例えば、有名なCohere Rerankモデルでは、約2,000文字のチャンク96件を処理するのに約4秒かかるとされています。

MixLMは、この問題を解決するために、RAGチャンクを事前に「リランキング用のベクトル」に変換して保存しておくという革新的なアプローチを提案します。具体的には、以下の手順で動作します。

1.  **事前準備**: 通常のRAG用ベクトルとは別に、各チャンク（数千トークン）をEncoder LLMに投入し、出力の最後の1〜2トークン分のベクトルのみを「リランキング用のベクトル」として保存します。これは、「人間が読むには数千トークン必要な文章でも、LLMが理解するために圧縮すると1〜2トークンで済む」という発想に基づいています。
2.  **ユーザー質問時**:
    *   まず、通常のBi-Encoderによるベクトル検索で、参考文書を絞り込みます（例：100チャンク）。
    *   次に、絞り込まれた各チャンクの「リランキング用のベクトル」とユーザーの質問を結合します。
    *   最後に、この結合されたクエリをリランキングモデルに投入し、チャンクと質問の関連性スコアを高速で算出します。

この手法により、MixLMは従来の強力なベースライン手法と比較して、同じレイテンシ予算内でスループットを10.0倍に向上させ、フルテキストLLMとほぼ同等の精度（NDCG@10で0.02ポイント差）を維持することに成功しました。実際にLinkedInの求人検索で本番デプロイされた結果、Daily Active Usersが0.47%向上したと報告されています。

著者は、エンタープライズRAGにおいて回答速度と精度のバランスが極めて重要であると指摘しており、AIエージェントの導入で精度が向上する一方で待ち時間が増える現状に鑑みても、MixLMのような高速化技術の意義は大きいと述べています。Meta社の「REFRAG」のような研究も進んでおり、「人間にとって分かりやすい文章はLLMにとって冗長であり、もっと圧縮して渡すべき」という共通の思想が背景にあるとのことです。これにより、将来的には現在遅いとされるAIエージェント間の会話も一瞬で済むようになる可能性を示唆しています。RAGシステムを構築するエンジニアにとって、このMixLMはパフォーマンス向上のための有力な選択肢となるでしょう。