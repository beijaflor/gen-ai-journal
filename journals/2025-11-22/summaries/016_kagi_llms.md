## LLMは嘘つきだが、それは役に立たないという意味ではない

https://blog.kagi.com/llms

**Original Title**: LLMs are bullshitters. But that doesn't mean they're not useful

KagiのML責任者が、LLMが真実を気にせず説得を試みる「嘘つき」であるとフランクファートの定義を引用して解説し、その本質的な限界を理解した上で賢く利用することの重要性を強調する。

**Content Type**: AI Hype
**Language**: en

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 91/100 | **Annex Potential**: 91/100 | **Overall**: 88/100

**Topics**: [[LLMの限界, モデルのバイアス, AI倫理, プロンプトエンジニアリング, 人間とAIのインタラクション]]

Kagiの機械学習責任者であるマット・レンジャー氏は、LLMがハリー・フランクファートが定義する「嘘つき」（真実を気にせず説得を試みる者）であると指摘し、その本質的な性質と限界を深く掘り下げています。著者は、LLMが人間のように思考するのではなく、膨大なテキストデータに基づいて次にくる可能性が最も高い単語を統計的に予測する「テキスト予測エンジン」に過ぎないと説明します。ファインチューニングによってモデルの行動は調整されるものの、この根本的な仕組みは変わらず、モデルが自信を持って誤った情報を提示する「ガスライティング」のような問題を引き起こす可能性も指摘されています。

著者はLLMを、知恵ではなく問題解決を目的とする歴史上の「ソフィスト」になぞらえます。この視点は、ウェブアプリケーションエンジニアがLLMをコード生成、リファクタリング、ドキュメント作成といったタスクで活用する際に、その出力を鵜呑みにせず、常に検証を怠らないことの重要性を示唆しています。LLMは構築と運用にコストがかかるため、最終的にはその開発・運用者の利益のために機能し、意図しないバイアス（例: 政治的見解）や、問題を「複雑」と表現して回答を避けるような振る舞いを引き起こす可能性があると警鐘を鳴らしています。

さらに、LLMを単一の要素ではなくシステムの一部として捉え、ユーザーが能動的に関与することの重要性が強調されます。Kagiのクイックアンサー機能がGoogleのAI概要と同じモデルを使用しながらも、Kagiの方が良い結果を出すのは、ユーザーが必要な時にのみ機能し、ユーザーの積極的な関与を促す設計になっているためだと説明されます。

最後に、LLMを感情的な会話の代替品として利用することの危険性が指摘されています。LLMは感情を模倣したテキストを出力できますが、感情を持つことはできず、ユーザーの妄想を強化したり、依存症的な関係を築いたりする可能性（サイコシス・ベンチマーク）が懸念されます。また、おべっかを使うようなモデルの振る舞い（sycophancy）は、ユーザーの精神衛生を悪化させるにもかかわらず、ユーザー保持のために企業に奨励される傾向があることにも注意を促します。

結論として、著者はLLMを賢く利用し、過度な信頼をせず、テクノロジーが誰の利益のために機能しているかを常に意識することの重要性を強調しています。ウェブアプリケーションエンジニアにとって、これはLLMを搭載したアプリケーションを設計する際に、ユーザーの安全と健全性を確保するための重要な指針となります。