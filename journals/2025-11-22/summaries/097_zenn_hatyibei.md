## Anthropic社の「Code Execution with MCP」が示す未来 ―― ツールコール時代の終焉と、エージェント設計の次

https://zenn.dev/hatyibei/articles/6b26d6bd27a9c2

Anthropicが提示するエージェント設計の新たな哲学は、Model Context Protocol（MCP）における直接的なツールコールではなく、コード実行を介してツールを扱うことで、AIエージェントのスケール問題を解決し、トークン消費を劇的に削減すると主張します。

**Content Type**: 🛠️ Technical Reference
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 84/100 | **Annex Potential**: 84/100 | **Overall**: 84/100

**Topics**: [[AIエージェント設計, LLM最適化, トークン効率, MCP, コード実行]]

Anthropicが提示する「Code Execution with MCP」という新しい設計思想は、AIエージェントの未来を大きく変える転換点です。従来のエージェント設計では、MCPなどのツールを直接呼び出し、全てのツール定義や中間結果をLLMのコンテキストに詰め込むため、トークン消費が異常に増大し、特に大規模なマルチエージェント環境では最大15倍にも達するトークン地獄に陥ることが問題視されていました。実際には、質問に答える前に15万トークンを消費するケースも報告されており、このままではレイテンシとコストの面で運用が破綻します。

Anthropicはこの問題に対し、「ツールを直接叩くのではなく、コードに変換して実行せよ」という解決策を提示しています。この「Code Execution × MCP」の仕組みでは、まずエージェントはツール群を「コードファイル」（例：`progress-server/src/list_tasks.ts`）として認識し、必要な時だけ`read_file`で読み込みます。これにより、全てのツール定義をコンテキストに最初から積む必要がなくなり、トークン消費が劇的に削減されます。LLMは「コードを書く」役割に徹し、生成したTypeScriptやPythonのスクリプト内でMCPツールをimportして使用します。中間処理、状態管理、フィルタリングなどは全てコード側で完結させ、最終結果だけをAIに返すことで、トークン消費を最大98.7%削減する実例が報告されています。

この方式が次世代の標準となる理由は、エージェントが肥大化する未来に耐えうる拡張性、ChatGPT/Claude/Geminiといったマルチモデル時代への互換性、そしてLLM（意思決定）、MCP（接続の標準化）、コード（状態・処理・制御）という明確な役割分担による効率的な三層構造にあります。

記事の著者は、これからMCPを扱う開発者に対し、最初から「コード実行前提」で設計することを強く推奨しています。ツール定義をコンテキストに全て詰める旧式のMCPサーバは1〜2年以内に陳腐化するとし、ツールは`src/*.ts`のようなコードファイルとして配置し、エージェントは必要時に`read_file`で読み込み、処理はコードで完結させ、AIには意思決定のみを行わせる設計が、長期的に持続可能なエージェント構築の鍵であると結論付けています。この思想を理解することは、2025年以降のAIエージェント開発において一歩先を行くための決定的な要素となるでしょう。