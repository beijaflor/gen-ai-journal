## Anthropic irks White House with limits on models’ use

https://www.semafor.com/article/09/17/2025/anthropic-irks-white-house-with-limits-on-models-uswhite-house-with-limits-on-models-use

AnthropicがそのAIモデルを特定の法執行機関による監視活動に使用することを拒否したことで、トランプ政権との間に緊張が生じ、AIツールの使用制限に関する広範な議論を巻き起こしている。

**Content Type**: 📰 News & Announcements

**Scores**: Signal:4/5 | Depth:2/5 | Unique:4/5 | Practical:3/5 | Anti-Hype:4/5
**Main Journal**: 94/100 | **Annex Potential**: 97/100 | **Overall**: 68/100

**Topics**: [[AI倫理, AI利用ポリシー, 政府契約, 法執行機関AI, AI規制]]

Anthropicが主力AIモデルClaudeの利用を、FBI、シークレットサービス、ICEなどの連邦法執行機関による国内監視活動に限定することを拒否し、トランプ政権との間で摩擦が生じている。この事態は、AIモデルを提供する企業がその製品の利用用途をどこまで制限できるか、特に政府機関への提供においてという、重要な議論を提起している。

Webアプリケーションエンジニアの視点から見ると、これはAIモデルの「利用規約」が単なる形式的な文書ではなく、アプリケーションの設計や導入に甚大な影響を及ぼす実体であることを示している。政府機関や高度に規制された業界向けのAIソリューションを開発する際、基盤となるAIモデルの利用制限がプロジェクトの実現可能性や市場価値を直接左右するため、そのポリシーを深く理解し、遵守することが極めて重要となる。

記事では、Anthropicの「国内監視」に関するポリシーが曖昧であると指摘されており、他のAIプロバイダーがより明確な定義や法執行活動への例外規定を設けていることと対比されている。高性能なAnthropicのモデルが、政治的または倫理的な理由で利用が制限されることは、政府契約に携わる企業にとって予期せぬ障害となる。これは、AIの安全性と利用促進のバランスを巡るAI業界と政府間の広範な緊張関係の一部であり、エンジニアは今後のAIツールの選定や活用において、技術的な性能だけでなく、倫理的・政治的な利用制限についても慎重に考慮する必要があることを示唆している。