## コスパの高いAIモデル「Qwen3-Next」登場、従来より10倍高速で同等以上の性能を実現

https://gigazine.net/news/20250912-qwen3-next/

Alibabaが開発した高性能AIモデル「Qwen3-Next」は、トレーニングコストを大幅に削減しつつ、既存モデルを上回る推論速度と性能を実現し、GoogleのGemini-2.5-Flash-Thinkingをも凌駕すると発表しました。

**Content Type**: News & Announcements

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:4/5
**Main Journal**: 91/100 | **Annex Potential**: 87/100 | **Overall**: 88/100

**Topics**: [[LLM効率化, Mixture of Experts (MoE), 推論高速化, 低コストAIモデル, ベンチマーク性能]]

AlibabaのAI研究チームが、ウェブアプリケーション開発者にとって画期的なAIモデル「Qwen3-Next」を無償公開しました。このモデルの最大の特長は、従来のQwen3-32Bと比較してトレーニングコストを10分の1以下に抑えつつ、同等以上の性能、さらには入力トークン量が多い状況で10倍以上高速な推論処理を実現している点です。これは、リアルタイム性が求められるWebサービスや、大量のプロンプトを処理するAIエージェントにとって極めて大きなメリットをもたらします。

Qwen3-Nextは、複数の専門家モデルを効率的に組み合わせるMixture of Experts (MoE)アーキテクチャを採用しており、特にGated DeltaNetとGated Attentionを3：1の割合で用いることで、高い性能と低コストを両立させています。800億パラメータを持つモデルでありながら、推論時にアクティブになるのは最大30億パラメータに限定されるため、リソース効率も非常に優れています。ベンチマークテストでは、PrefillフェーズでQwen3-32Bの10.6倍、Decodeフェーズで10倍の高速化を達成し、GoogleのGemini-2.5-Flash-Thinkingをも上回るスコアを記録しています。

この技術的進歩は、AIモデルの利用コストとレイテンシがボトルネックとなっていた開発現場に、新たな選択肢を提供します。特に、開発者が自社のデータでファインチューニングを施す場合でも、トレーニングコストの削減は大きな魅力です。無償で公開された「Qwen3-Next-80B-A3B-Instruct」と「Qwen3-Next-80B-A3B-Thinking」は、Hugging Faceから利用可能であり、これによりウェブアプリケーションエンジニアは、より高性能かつ経済的なAI機能をアプリケーションに組み込む道が開かれました。AIを活用した新機能やエージェントベースのシステム開発において、パフォーマンスとコスト効率を両立させる上で、Qwen3-Nextは重要な基盤となるでしょう。