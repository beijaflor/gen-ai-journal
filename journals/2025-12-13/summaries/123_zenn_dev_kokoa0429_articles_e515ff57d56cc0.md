## 8GBメモリでローカルLLMを動かす

https://zenn.dev/kokoa0429/articles/e515ff57d56cc0

著者は、RTX 4090のような高価なGPUなしに、8GBメモリのCPUオンリー環境でもローカルLLM（特にQwen3）を実用的に動かすための具体的な設定とトラブルシューティングを詳述し、Misskeyボット開発への応用例を示しています。

**Content Type**: 📖 Tutorial & Guide
**Language**: ja

**Scores**: Signal:4/5 | Depth:4/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:5/5
**Main Journal**: 89/100 | **Annex Potential**: 88/100 | **Overall**: 88/100

**Topics**: [[ローカルLLM, 量子化, Ollama, Qwen3, Misskey bot開発]]

多くのエンジニアがローカルLLMの導入を検討する際、「高スペックなGPUが必須」という情報に直面しがちですが、筆者は8GBメモリとCPUのみのProxmox VM環境で、Misskeyのローカルタイムライン監視ボットを動かすためのLLM推論サーバーを構築した経験を共有しています。これは、限られたリソースでLLMを活用したいと考える開発者にとって非常に重要な示唆を与えます。

記事では、まずメモリ使用量を劇的に削減する「量子化」の基礎（Q4_K_M, Q5_K_M, Q8_0など）を解説し、8GBメモリがあればQ4_K_MやQ8_0でも動作可能であることを示しています。次に、ローカルLLMを動かす主要ツールであるOllamaとllama.cppを比較。導入が容易なOllamaを推奨しつつ、より細かい制御が必要な場合のllama.cppへの移行パスを提示しています。

特に実用上のハマりどころとして、以下の点が詳細に解説されています。
1.  **コンテキスト長**: Ollamaのデフォルトコンテキスト長2048トークンはKVキャッシュを大量に消費するため、分類タスクでは512〜1024に削減すべきと指摘。
2.  **Qwen3のthinkingモード**: Qwen3モデルが内部で思考プロセスを生成するため、たとえ短い回答でも推論が遅延する問題に対し、システムプロンプトでの強制無効化、Modelfileでのカスタム、あるいはthinkingモード無効化済みのモデル利用（筆者推奨）といった具体的な回避策を提示。
3.  **VMでのAVX命令無効化**: Proxmox VMでCPUタイプを`host`に設定しないとAVX2/AVX512が無効になり、llama.cpp系の処理速度が劇的に低下する問題を指摘。
4.  **Ollamaのモデルアンロード**: アイドル時にモデルがアンロードされる挙動を、`OLLAMA_KEEP_ALIVE=-1`設定で常駐化させる方法を解説。

ベンチマーク結果として、Qwen3の軽量モデル（`hoangquan456/qwen3-nothink:4b`）をCPUオンリー環境で動かした場合、分類タスクが約1.85秒、200トークンのコード生成が約18.3秒（約12トークン/秒）で完了し、ボット用途には十分実用可能であることを示しました。

最終的に、8GBメモリ環境での最適な構成として、`hoangquan456/qwen3-nothink:4b`とコンテキスト長512の組み合わせを推奨し、その設定のポイントをまとめています。記事の最後には、実際に構築したMisskey監視ボットのNode.jsコードが提供されており、プロンプトにFew-shot形式の例を含めることや、JSON部分を抽出するロバストな処理など、具体的な実装のヒントも得られます。本記事は、低スペック環境でのローカルLLM運用における具体的な課題と解決策を提供し、エンジニアがクラウドAPIに依存せずに自律的なAIシステムを構築する道筋を示す、非常に実践的なガイドです。