## RTX 3090でLLMをゼロから学習する、パート28：ベースモデルのスクラッチからの学習

https://www.gilesthomas.com/2025/12/llm-from-scratch-28-training-a-base-model-from-scratch

**Original Title**: Writing an LLM from scratch, part 28 -- training a base model from scratch on an RTX 3090

著者は、個人用RTX 3090を使用してGPT-2小型ベースモデルのスクラッチ学習に約48時間で成功し、ローカルでのLLM構築の可能性を示したが、性能面ではOpenAIオリジナルモデルにわずかに及ばない点を詳細に分析している。

**Content Type**: Research & Analysis
**Language**: en

**Scores**: Signal:4/5 | Depth:5/5 | Unique:4/5 | Practical:4/5 | Anti-Hype:5/5
**Main Journal**: 89/100 | **Annex Potential**: 89/100 | **Overall**: 88/100

**Topics**: [[LLMトレーニング, 個人用GPU, GPT-2アーキテクチャ, データセットキュレーション, トレーニング最適化]]

著者は、RTX 3090グラフィックカードを搭載した自身のPCで、GPT-2小型モデル（1.63億パラメータ）をゼロからトレーニングする詳細な実験を行いました。個人環境でのベースモデル学習は困難という従来の認識に対し、その実現可能性を実証した点が重要です。

モデルはSebastian Raschkaの書籍に基づきGPT-2小型の構成パラメータで設定され、学習データにはHugging FaceのFineWeb 10Bデータセットを使用。Chinchillaのヒューリスティックに基づき、パラメータ数の20倍にあたる約32億トークンを学習目標としました。

学習効率を高めるため、PyTorchのTF32テンソルコア活用（`torch.set_float32_matmul_precision("high")`）と自動混合精度（AMP）を導入。これにより、トークン/秒処理能力はFP32のみの場合の約12,599から約19,997に向上し、約3日かかる見込みだった学習時間を約44時間に短縮。この高速化により、個人用ハイエンドPCでの実用的なベースモデル学習の道が開かれました。

しかし、学習したモデルの性能はOpenAIのオリジナルのGPT-2小型モデルには及びませんでした。著者のモデルの検証ロスは3.94（パープレキシティ約51.4）に対し、OpenAIモデルは3.50（パープレキシティ約33.1）。Alpaca形式の指示応答タスクで微調整後、GPT-5.1による評価では、著者のモデルが平均16.14点、OpenAIモデルが20.39点という結果です。

著者はこの性能差の要因として、OpenAIが約100億トークンを40〜60エポックで学習したと推定されるのに対し、自身の学習データ量（32億トークン）と実質1エポック相当のトークン量であったこと、またオリジナルのGPT-2にあったバイアスやウェイトタイイングなど、アーキテクチャの細かな違いや、学習率スケジューリングの洗練度の差を指摘しています。FineWeb-Eduデータセットや学習トークン倍増の追加実験でも、性能改善は限定的でした。

本実験は、個人用ハードウェアでのベースモデル学習が十分に可能であり、学習体験として非常に有益であることを示しました。一方で、既存の高性能モデルに追いつくためには、データパイプラインの改善、学習トークン量の増加、アーキテクチャの調整、学習率スケジューリングの最適化など、さらなる探求が必要であると結論付けています。今後はクラウドGPUを活用し、これらの仮説をより迅速に検証していく計画です。