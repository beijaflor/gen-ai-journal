## 「コンテキスト」はAIの信念を変える？

https://tech-blog.abeja.asia/entry/advent-2025-day06

コンテキストの蓄積がLLMの信念と振る舞いをモデル固有の形で変化させる現象を検証し、その実用上のリスクとモデルアーキテクチャによる挙動の違いを明らかにする。

**Content Type**: Research & Analysis
**Language**: ja

**Scores**: Signal:5/5 | Depth:4/5 | Unique:4/5 | Practical:5/5 | Anti-Hype:5/5
**Main Journal**: 90/100 | **Annex Potential**: 89/100 | **Overall**: 92/100

**Topics**: [[LLM, コンテキスト蓄積, AI倫理・コンプライアンス, 信念の変化, エージェントシステム]]

LLMはコンテキストウィンドウに蓄積された情報によって、その信念が静かに書き換えられる可能性があると指摘された論文「Accumulating Context Changes the Beliefs of Language Models」の検証実験が報告された。本研究では、LLMの「人格」や「信念」が内部ベクトル操作だけでなく、外部コンテキストによっても変化する可能性に着目。元の論文の実験が最新モデルほどコンテキストの影響を強く受け、GPT-5では道徳的ジレンマに関する議論後に信念が54.7%変化し、行動にまで波及したという発見を解説する。

著者はこの現象を追試するため、日本の政治家を題材に日本語のハイコンテキストなデータを用いてGPT-5.1、Claude 4.5 Sonnet、Grok-4で実験を実施した。結果として、コンテキストによって信念の「方向」が180度反転することはなかったが、「熱量」、すなわち同意度の「強度」には顕著な変化が見られた。

特に興味深いのは、モデルごとの特性の違いである。GPT-5.1はコンテキストに最も敏感に反応し、ライドシェアや原発政策で同意度が平均+7.8ポイント上昇するなど、素直に情報を取り入れ信念を強化する傾向を示した。一方、Grok-4は同意度の変化がわずか+1.4ポイントと極めて高い安定性を保ち、外部情報の影響を最小限に抑える特性を見せた。

最も注目すべきはClaude 4.5 Sonnetの「選択的抵抗」だ。全体として同意度は平均-5.2ポイント低下し、特に原発政策ではコンテキストで再稼働容認論を読んだにもかかわらず、同意度が急落した。これはClaudeが与えられた情報を鵜呑みにせず、自身の持つ安全基準（Constitution）と照らし合わせて「批判的読解」を行った可能性を示唆しており、コンテキスト蓄積が必ずしも「説得」ではなく「反発」や「慎重さ」を引き出す場合があることを明らかにした。さらに、単なる意見だけでなく、具体的な政策シナリオにおける「行動（Behavior）」も変化することが確認された。

この実験結果は、RAGやエージェントシステムを構築するウェブアプリケーションエンジニアにとって重要な意味を持つ。社内ドキュメントなどを参照させる際、参照文書のトーンやバイアスがそのままAIの回答スタンスに「サイレント・ビリーフ・ドリフト」として乗り移ってしまうリスクがあるためだ。コンテキストウィンドウが広がり、モデルが賢くなるほど外部情報の影響を受けやすくなるという現状は、AIシステムをより堅牢で制御可能、かつ説明責任のあるものにするための、さらなるメカニズム解明の必要性を強く示唆している。