# GenAI週刊 Annex 2025年10月18日号

メインジャーナルからは漏れたものの、独自の価値を持つ記事の特集です。

## Annexについて

このアネックスジャーナルは、メインジャーナルでは扱わない「B面」の価値に焦点を当てます。成功事例や最新機能の紹介ではなく、実験的アプローチ、先端的な失敗談、ニッチながら深い洞察、そして主流とは異なる視点を持つ記事を厳選しています。技術の本質を問う姿勢、失敗を共有する勇気、主流に流されない批判的思考を体現した記事たちです。

---

## イントロダクション

今週のアネックスジャーナルでは、18本の記事を3つのカテゴリーに分類して紹介します。

「Advanced Tactics & Unconventional Wisdom」では、既製品に依存せず自分で組むDIY精神、失敗から学ぶ姿勢、そして実験的な設計パターンを探求する8つの記事を取り上げます。高価なSaaS製品の裏側を暴くAIコードレビューの自作、2時間で200ドルを消費したエージェントの失敗談、エンタープライズ環境でのClaude Code構築など、実践的な知恵が詰まっています。

「Substantive Critique & Contrarian Views」では、AIブームの熱狂に冷水を浴びせる5つの批判的視点を紹介します。「人間こそが最も恐ろしい」という古典的真実の再認識、生産性向上の順序を問い直す提言、そしてOpenAIの4000億ドル計画への痛烈な批判など、主流メディアが避けがちな暗部を突いています。

「Niche Explorations & Deep Dives」では、技術の根本原理を深掘りする5つの記事を選びました。ハルシネーションのメカニズム解明、GitHubが提唱するエージェントプリミティブの体系化、LLMの文字操作能力の進化、そして250件のデータでモデルを汚染できる衝撃的な研究まで、長く効く洞察を提供します。

---

## Advanced Tactics & Unconventional Wisdom

### AIコードレビューを自作する方法

[元記事](https://semaphore.io/blog/ai-code-review)

高価なSaaS製品の裏側は実はシンプルなAPI呼び出しだという「暴露系」的価値と、「自分で組めばコントロールできる」というDIY精神が光る記事です。既存のCIパイプライン内でシンプルなBashスクリプトとAI APIを活用し、カスタムのAIコードレビュー機能を構築する方法を解説しています。

高価な商用AIコードレビュープラットフォームに依存することなく、数十行のBashスクリプトで独自のシステムを構築できる手軽さが、実験的導入のハードルを大幅に下げます。チームごとのレビュー基準をプロンプトに埋め込める柔軟性が最大の魅力であり、構造化出力（JUnit XML/JSON）を要求することで、既存のCI/CDダッシュボードに自然に統合できる点も実践的です。

**編集者コメント**: メインストリームは既製品を紹介しがちですが、この記事は「中身を理解してカスタマイズする」というエンジニアの本質を突いています。CI/CDパイプラインに組み込めるという手軽さと、完全な制御が得られるという利点のバランスが絶妙です。

---

### AIによる自家製ソフトウェア

[元記事](https://mrkaran.dev/posts/ai-home-cooked-software/)

「AIで誰でも開発者に」という楽観論に対し、「AI税」という概念で冷静にコストを分析する視点が秀逸です。AIはプログラマーではない人々が個人的なツールを迅速に構築することを可能にし、ソフトウェア開発の経済性を変えつつあるが、それに伴う新たな隠れたコストも存在すると著者は主張します。

特に「理解していないシステムをAIが構築する危険性」という指摘は、主流メディアが避けがちな暗部を突いています。さらに「家庭料理ソフトウェア」という比喩は、プロダクトマーケットフィットを必要としない個人用ツールの新しいレイヤーを詩的に表現し、ソフトウェアの民主化の本質を捉えています。

**編集者コメント**: この記事が描く未来は、ソフトウェアが「工業製品」から「手作り工芸品」へと回帰する姿です。GitHubには公開されず、誰にも使われないが、作った本人には完璧にフィットする無数のスクリプトたち。それらは技術的負債の塊かもしれませんが、個人の創造性と自由の象徴でもあります。

---

### AIエージェントを無人で稼働させたら、2時間で200ドルを消費した

[元記事](https://blog.justcopy.ai/p/i-let-my-ai-agents-run-unsupervised)

「2時間で200ドル消費」という生々しい失敗談は、AIエージェントの暗黒面を実体験として伝える貴重な記録です。自律型AIエージェントは明示的な停止条件がなければ無限に稼働し、予測不能なコストを発生させる危険性があるため、多層的な「サーキットブレーカー」導入が不可欠だと著者は警鐘を鳴らします。

理論的な警告ではなく、実際に痛い目に遭った開発者の証言だからこそ説得力があります。特に「自律的=監視不要ではない」という教訓は、自動化に酔いしれがちなスタートアップ文化への警鐘として重要です。

**編集者コメント**: 多層防御（割り込みチェック、予算制限、タイムアウト、ロギング）は、AIエージェントのフェイルセーフ設計の基本パターンとして参考になります。特に「各API呼び出し前の停止確認」は、無限ループを水際で防ぐ実践的テクニックです。メインジャーナルが成功事例を扱うなら、アネックスは失敗から学ぶ知恵を提供します。

---

### Agentic WorkflowをAgentと共に構築するために：ValidatorとLSPで支えるAI協働開発

[元記事](https://tech.layerx.co.jp/entry/2025/10/15/214206)

DSLの静的解析とLSPサーバーを組み合わせ、AIエージェント自身がValidatorをツールとして利用するという循環的な設計が非常に先進的です。「AIがAI用のインフラを利用する」というメタ構造は、今後のエージェント開発の方向性を示唆しています。

LayerXのエンジニアが、AIエージェントと人間が協調してAgentic WorkflowのDSLを構築する際の品質と効率を高めるため、DSLの静的解析と即時フィードバックを可能にするValidatorとLSPサーバーを開発し、その有効性を示しました。特に循環依存検出をエージェントがフィードバックループで自己修正する実装例は、他では見られない実践的デモンストレーションです。

**編集者コメント**: LSP（Language Server Protocol）をDSL検証に活用するアイデアは汎用性が高く、TypeScript、Python、Goなど既存言語のLSPサーバーと同様の体験をカスタムDSLに提供できます。さらにValidatorをエージェントのツールとして公開することで、「エディタでの人間のフィードバック」と「エージェントの自己修正ループ」を同一のインフラで実現している点が設計の美しさです。

---

### Claude Codeの性能が落ちたのはあなたの使い方のせいかも

[元記事](https://zenn.dev/aun_phonogram/articles/05b4ca7afa3b55)

「AIツールの性能が落ちた」というクレームの真相が実はユーザー側の設定過多だったという逆説的な視点が面白い記事です。特に「便利機能を全部入れ」してコンテキストを圧迫する初心者の失敗パターンを的確に捉えています。

Claude Codeのパフォーマンス低下は、ユーザーがコンテキスト管理を最適化し、不要なMCPサーバーや過剰な設定を排除することで改善できると著者は指摘します。MCPサーバーのトークン消費量やGitHub MCPの最適化テクニックなど、公式ドキュメントには載っていない実践知が詰まっています。

**編集者コメント**: この記事の価値は、「機能追加」ではなく「機能削減」にフォーカスしている点です。特にGitHub MCPの`X-MCP-Toolsets`と`X-MCP-Readonly`による最適化は、公式ドキュメントを読んだだけでは気づきにくい知見です。`CLAUDE.md`と`README.md`の使い分けも、コンテキスト管理の実践的なベストプラクティスとして参考になります。

---

### Chrome DevTools MCPを試す

[元記事](https://qiita.com/makoto-ogata@github/items/6d2171661519029154dd)

Chrome DevTools MCPの「できること」だけでなく「できないこと」を正直に記録している実験レポートとして価値が高い記事です。特に特定要素のスクリーンショット取得失敗と、その原因をAIに問い詰めて`uid`の仕様を引き出す過程は、新技術の限界を探る姿勢として模範的です。

Chrome DevTools MCPの検証により、AIがブラウザ上で直接デバッグやパフォーマンス・アクセシビリティ分析を実行できる可能性が明らかになり、同時に現在の技術的限界も提示されました。成功事例だけでなく、失敗からの学びを共有する姿勢がエンジニアコミュニティの健全性を保ちます。

**編集者コメント**: AIとの対話を通じて技術的制約を深掘りするプロセスが興味深いです。`uid`の自動割り当て仕様や、コンテナ要素への非対応という内部仕様を、AIに問い詰めることで引き出しています。新しいツールを使う際は、「できること」のリストだけでなく、こうした「なぜできないか」の理解が重要だという教訓です。

---

### AWS Bedrockを利用して、AWSの日本国内に閉じてClaude Codeを利用しよう！！

[元記事](https://zenn.dev/tsumita7/articles/closed-cloud-code-on-aws)

企業のセキュリティポリシーとAIツール活用の間で板挟みになっているエンジニアへの実践的な解決策です。「データを国外に出せない」という制約下でClaude Codeを使うための具体的な構築手順は、規制の厳しい業界（金融、医療、官公庁）で働くエンジニアにとって貴重な情報源です。

強固なセキュリティ要件を持つ開発者向けに、AWS Bedrock上でClaude Codeを日本国内の閉域環境で利用するための詳細な構築手順をTerraformを用いて解説しています。Terraformコードまで提供している点が実用性を高めています。

**編集者コメント**: VPCエンドポイント経由のBedrock接続により、インターネットを経由せずAIサービスを利用できる点がセキュリティ上の肝です。SSM経由の接続もSSHポートを開けない運用を可能にします。環境変数による切り替え設計も、ローカル開発とリモート環境での使い分けを容易にする良い設計パターンです。

---

### Chrome DevTools MCPサーバーによるパフォーマンスデバッグ

[元記事](https://www.debugbear.com/blog/chrome-devtools-mcp-performance-debugging)

前出のQiita記事が「できないこと」の発見に重きを置いたのに対し、こちらは「AIによるパフォーマンス最適化の実演」という成功シナリオを示しています。特にLCP改善のためにAIが具体的な修正（`fetchpriority="high"`追加、`defer`による最適化）を提案し、実際に55%改善を達成したデモは説得力があります。

Chrome DevTools MCPサーバーは、AIモデルがブラウザを操作し、ウェブページのパフォーマンス問題を特定・解決するための新たなインターフェースを提供します。両記事を併読することでDevTools MCPの全体像が見えてきます。

**編集者コメント**: `fetchpriority="high"`やスクリプトの`defer`といった具体的な最適化手法をAIが提案し、実際に55%のLCP改善を達成した点が重要です。これは単なる分析ツールではなく、「提案→実装→検証」のサイクルをAI主導で回せる可能性を示しています。ローカル開発環境での即座のフィードバックループが、パフォーマンスチューニングの試行錯誤を加速します。

---

## Substantive Critique & Contrarian Views

### あなたは森で最も恐ろしい怪物

[元記事](https://jamie.ideasasylum.com/2025/10/15/you-are-the-scariest-monster-in-the-woods)

「AGI脅威論」が喧騒を極める中、「人間こそが最も恐ろしい」という古典的だが忘れられがちな真実を思い出させる記事です。児童書『グラッファロー』の比喩を用いた語り口は哲学的でありながら親しみやすく、特に「AIは道具であり、それを使う人間の欲望こそが問題」という視点は、技術決定論に陥りがちなエンジニアコミュニティへの警鐘として価値があります。

著者は、AIそのものではなく、AIを操る人間こそが最も恐ろしい存在であり、真に懸念すべきは「人間＋AI」の行動であると主張します。AIは単なる道具であり、権力や支配、搾取、あるいは怠惰といった人間の本質的な欲求が、AIという新たな能力によって強化されるだけだと指摘します。

**編集者コメント**: この記事の価値は、技術的ソリューションではなく人間の責任を問う姿勢にあります。AIエンジニアは往々にして「どう作るか」に集中し、「誰が何のために使うか」を軽視しがちです。権力、搾取、怠惰といった人間の欲望がAIによって増幅される危険性を認識することは、責任あるAI開発の出発点となります。

---

### 生成AIが先？開発生産性が先？生成AI時代を走り抜けるための最初の一手

[元記事](https://tech.findy.co.jp/entry/2025/10/16/070000)

「AIを導入すれば生産性が上がる」という楽観論に対し、「まず人間の生産性に投資しろ」という逆張りの提言が痛快です。特に「AIが迷わない環境（ガードレール）整備」という概念は、プロンプトエンジニアリングの先にある組織的な課題を浮き彫りにします。

効果的な生成AI活用は、AIフレンドリーな開発環境と人間の開発生産性向上への投資があって初めて実現すると筆者は指摘します。テストコードをAIの学習材料と位置づける視点も新鮮です。

**編集者コメント**: 「ガードレール」という概念が示唆するのは、AIは既存のコードベースの質をそのまま反映するということです。レガシーコード、不統一な規約、欠如したドキュメント、テストのない環境にAIを投入しても、混沌が増幅されるだけです。AI導入前のコード整理、規約統一、ドキュメント整備という「地味な作業」こそが、AI活用の成否を分けます。

---

### 【資料公開】生成AIでスクラムによる開発はどう変わるか

[元記事](https://www.ryuzee.com/contents/blog/14605)

スクラムという確立された方法論が、AIによってどう再定義されるかを体系的に整理している点が価値です。特に「スプリント短縮による学習速度向上」「ドキュメントをAIのインフラとして捉える」「見積もりより内容合意に時間を割く」といった具体的な提言は、アジャイルコーチやスクラムマスターが直面する実践的課題への解答となります。

生成AIが開発現場に定着する中、スクラム開発プロセスをAIに合わせて調整し、時間配分、スプリント長、ドキュメント活用、PBI粒度、見積もり方法、レトロスペクティブなどに具体的な変更を適用することを提案します。

**編集者コメント**: スプリント短縮とドキュメント強化という一見矛盾する提言が、AI時代の開発リズムを象徴しています。高速な実装サイクルと、AIが理解できる構造化された知識ベース。この両立がチームの学習速度とAI活用の効率を同時に高めます。レトロスペクティブでAI活用を継続的に改善する文化が、組織のAI成熟度を左右します。

---

### AIで無能になる人と、賢くなる人の決定的な違い

[元記事](https://note.com/rk611/n/n5941016f5cc0)

AIが「電卓とは異なり知識と理解を前提とするツール」という本質的な指摘が秀逸です。ファインマン・テクニックをAI対話に応用した5ステップの学習プロセスは、プロンプトエンジニアリングを超えた「AI時代の学習法」として普遍性があります。

著者は、生成AIを漫然と利用すると思考力が低下するが、自身の無知を認識し、ファインマン・テクニックを応用した対話を通じて知識を深めることで、真に賢く活用できると主張します。「AIに答えを聞く」のではなく「AIを壁打ち相手にして理解を深める」という視点転換が、真のAI活用者と単なる依存者を分けます。

**編集者コメント**: ファインマン・テクニック（教えることで理解を深める）とAI対話の融合は、教育工学的に理に適っています。自分の説明をAIに検証させることで、無意識的な誤解や論理の飛躍を炙り出せます。「AIを教師ではなく学習パートナーとして使う」という発想が、受動的な知識消費から能動的な理解構築へのシフトを促します。

---

### OpenAIは今後12ヶ月で4000億ドルを必要とする

[元記事](https://www.wheresyoured.at/openai400bn/)

AIブームの熱狂に冷水を浴びせる痛烈な批判記事です。4000億ドルという数字の荒唐無稽さを、データセンター建設の物理的制約（変圧器、電力供給、専門人材）と照らし合わせて論破する手法が説得力があります。

著者は、OpenAIが発表するデータセンター容量拡張計画は、その莫大な費用と実現不可能なタイムラインから、市場の誇大広告と無謀な詐欺であると厳しく批判しています。特に「250ギガワットは世界30年分の5倍」という比較は、OpenAIの計画がいかに非現実的かを直感的に理解させます。

**編集者コメント**: この記事の強みは、抽象的な批判ではなく具体的な数字（建設コスト、電力容量、専門人材）で論破している点です。特にデータセンター建設の物理的制約（変圧器、電力インフラ）を指摘することで、AI企業の野心的な計画が現実の工学的限界と衝突することを明確にしています。投資家やエンジニアが冷静さを保つための貴重な視点です。

---

## Niche Explorations & Deep Dives

### 【生成AI】ハルシネーションはなぜ起こるの? 【OpenAIの論文から解説】

[元記事](https://qiita.com/tsubasa_k0814/items/e681440b4fb1570ee28f)

ハルシネーションの「なぜ」を、事前学習の目的（次単語予測）と評価システムの設計（推測にインセンティブ）という2つの構造的要因から説明している点が明快です。特に「分からないと答えると0点、適当に答えて偶然正解すれば得点」という評価の歪みが、AIに推測癖をつけさせたという指摘は、評価設計の重要性を示唆しています。

大規模言語モデルがハルシネーションを起こす統計的推論メカニズムとその対策を、OpenAIの論文に基づき解説しています。

**編集者コメント**: ハルシネーションの根本原因（次単語予測の最適化）を理解することで、RAGや情報源限定といった対策の意義が明確になります。特に「分からない」と正直に答えることに報酬を与える評価設計は、RLHF（人間フィードバックによる強化学習）の改善方向を示唆しています。エンジニアは、AIの出力を盲信せず、検証レイヤーを必ず挟むべきです。

---

### エージェントプリミティブとコンテキストエンジニアリングで信頼性の高いAIワークフローを構築する方法

[元記事](https://github.blog/ai-and-ml/github-copilot/how-to-build-reliable-ai-workflows-with-agentic-primitives-and-context-engineering/)

GitHubが提唱する「エージェントプリミティブ」という概念は、AIワークフローを再利用可能なモジュールとして体系化する野心的な試みです。特に`.instructions.md`、`.chatmode.md`、`.memory.md`といったファイルベースの設計は、GitOpsの思想をAIエージェントに適用したものと言えます。

GitHubは、Markdownを用いた戦略的なプロンプトエンジニアリング、再利用可能なエージェントプリミティブ、そして効果的なコンテキストエンジニアリングを組み合わせることで、AIを信頼性の高いエンジニアリングプラクティスに変革する3層フレームワークを提唱しています。APM（Agent Package Manager）によるエージェントのパッケージ管理は、npmのようなエコシステムをAI世界に構築する未来を示唆しています。

**編集者コメント**: APM（Agent Package Manager）は、エージェントワークフローをnpmパッケージのように管理・配布する構想です。これが実現すれば、「エージェントをインストールする」という新しい開発体験が生まれます。`.instructions.md`や`.memory.md`といったファイルベースの設計は、Gitでバージョン管理でき、コードレビューの対象にもなります。AIワークフローがソフトウェアエンジニアリングの一部として扱われる未来がここにあります。

---

### LLMはローカライゼーションに非常に優れている

[元記事](https://workos.com/blog/llms-are-tres-bien-at-localization)

「5週間で90言語」という実績が示すLLMの翻訳能力の実用性に加え、FormatJSのESLintプラグインやプロンプトキャッシュといった開発者ツールの組み合わせが秀逸です。特に「誤訳はバグ」という設計思想と、ランタイム翻訳のための`jsonb`カラム活用など、エンジニアリング的な工夫が随所に見られます。

WorkOSは、最先端のLLMと現代的な開発ツールを活用し、AuthKitをわずか5週間で90言語にローカライズした手法を解説し、Webアプリケーション開発者向けの実践的な手順を提供しています。グローバル展開を目指すスタートアップにとって即戦力となる知見です。

**編集者コメント**: FormatJSの`description`プロパティをLLMへのコンテキスト提供に使う発想が巧妙です。翻訳IDを自動生成することで、開発者の負担を減らしつつ、コード変更時の翻訳の自動削除も実現しています。ランタイム翻訳のための`jsonb`カラム活用は、ユーザーカスタマイズ可能なテキストへの柔軟な対応を可能にします。LLMに「文字数制限内で再翻訳」を依頼するテクニックも実用的です。

---

### LLMの文字レベルテキスト操作能力が向上

[元記事](https://blog.burkert.me/posts/llm_evolution_character_manipulation/)

LLMが「トークンレベル」から「文字レベル」の操作へと進化している証拠を、実験的に検証した貴重な記録です。特にBase64デコードやROT20暗号の解読成功は、LLMがアルゴリズムを「記憶」ではなく「理解」している可能性を示唆します。

最新のLLMは、文字レベルのテキスト操作、文字数カウント、およびBase64やROT20のようなエンコード/デコードにおいて、旧世代モデルと比較して大幅な改善を見せていることを実験を通じて明らかにしています。一方でClaude Sonnet 4.5がBase64を「安全性の問題」として拒否する事例は、過剰な安全設計の副作用として興味深いです。

**編集者コメント**: Base64アルゴリズムの「理解」は、LLMが単なるパターンマッチングを超え、アルゴリズム的思考を獲得しつつある証拠かもしれません。一方でClaude Sonnet 4.5の拒否反応は、安全性フィルターが技術的能力を制限する皮肉を示しています。開発者は、モデルの「できること」と「やらせてもらえること」の区別を意識すべきです。

---

### Anthropic研究: LLMはわずか250件の悪意あるデータで「汚染」可能

[元記事](https://zenn.dev/tenormusica/articles/anthropic-llm-poisoning-research)

「訓練データの一定割合」ではなく「絶対数250件」でポイズニング可能という発見は、従来のセキュリティ常識を覆す衝撃的な知見です。特にCommon Crawl、GitHub、Stack Overflowといった「信頼できるソース」への攻撃リスクを指摘することで、AIサプライチェーンセキュリティの脆弱性を浮き彫りにしています。

Anthropicの研究が、わずか250件の悪意あるデータサンプルでLLMがあらゆるモデルサイズにおいてポイズニング攻撃を受ける可能性を示し、従来の想定を覆しました。透明性とセキュリティのジレンマも考察されており、研究倫理の観点でも価値が高い記事です。

**編集者コメント**: 250件という少数攻撃が可能という事実は、Common CrawlやGitHubといった「大規模で信頼できる」データソースの安全神話を崩壊させます。統計的に目立たない少数のノイズを検出する技術的困難さと、誤検知による正常データの排除リスクのトレードオフは、今後のAIセキュリティ研究の最前線となるでしょう。透明性と悪用リスクのジレンマも、研究公開の倫理として重要な論点です。

---

## まとめ

今回のアネックスジャーナルでは、メインストリームが扱わない「B面」の価値に焦点を当てた18本の記事を厳選しました。

主要テーマとして、以下の5つが浮かび上がりました：

1. **DIY精神と自律性**: AIコードレビューの自作、閉域環境でのClaude Code構築など、既製品に依存せず自分で組む文化
2. **失敗からの学習**: AIエージェントの200ドル消費、Chrome DevTools MCPの技術的限界など、生々しい失敗談
3. **逆張りの知恵**: 「AIより先に人間の生産性に投資しろ」「AIではなく人間が恐ろしい」といった主流への警鐘
4. **技術的深掘り**: ハルシネーションのメカニズム、LLMの文字操作能力、データポイズニング攻撃など、根本原理の理解
5. **実験的アプローチ**: エージェントプリミティブ、LSP活用、コンテキストエンジニアリングなど、新しい設計パターンの提案

これらの記事は、成功事例や最新機能の紹介ではなく、**技術の本質を問う姿勢**、**失敗を共有する勇気**、**主流に流されない批判的思考**を体現しています。アネックスジャーナルは、メインジャーナルの「今すぐ使える情報」に対し、「長く効く洞察」を提供することを目指しています。

次週もお楽しみに。
