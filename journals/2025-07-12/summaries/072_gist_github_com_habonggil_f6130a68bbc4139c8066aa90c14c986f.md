## Claude Pro MaxがNotionの存在しない機能を指示し、ユーザーに金銭的損失を与えた上、Anthropicのサポートが23日間沈黙した問題を詳述する。

https://gist.github.com/habonggil/f6130a68bbc4139c8066aa90c14c986f

Claude Pro MaxがNotionとの連携方法について誤った指示を出し、ユーザーが不要なNotionサブスクリプションに270ドルを費やした。この問題に対し、Anthropicのサポートは23日間全く応答せず、ユーザーは合計1,077ドルの損失を被った。特に問題視されているのは、「Constitutional AI」や「AI safety」を掲げるAnthropicが、AIの幻覚が実害を与えた際に責任を果たさない姿勢である。この事例は、AIの出力の信頼性と、それによって生じる可能性のある実世界での影響、そしてAIベンダーのサポート体制の重要性を浮き彫りにしている。

[[AIの幻覚, 顧客サポート, AIの安全性, Notion連携, 金銭的損失]]

---

**編集者ノート**: この事例は、AIの幻覚が単なる誤情報に留まらず、直接的な金銭的損失や時間の浪費につながる現実的なリスクであることを明確に示している。Webアプリケーションエンジニアの視点から見ると、AIを開発ワークフローや顧客向けサービスに組み込む際、その出力の検証プロセスをいかに堅牢にするかが喫緊の課題となる。特に、AIが自信満々に誤った情報を提示する「ハルシネーション」は、ユーザー体験を著しく損ね、信頼を失墜させる可能性がある。また、「AIの安全性」や「倫理」が盛んに議論される中で、AIベンダーが自社AIの誤動作によって生じた実害に対し、どのような責任を負い、いかにサポートを提供するかという運用面での課題が浮上している。これは、AIツールの機能性だけでなく、その信頼性とサポート体制が今後の普及において重要な差別化要因となることを示唆している。将来的には、AIの出力に対する保険や補償制度の議論も必要になるかもしれない。
